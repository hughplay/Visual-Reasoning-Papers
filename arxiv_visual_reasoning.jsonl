{"entry_id": "2208.00361", "title": "One for All: One-stage Referring Expression Comprehension with Dynamic Reasoning", "authors": ["Zhipeng Zhang", "Zhimin Wei", "Zhongzhen Huang", "Rui Niu", "Peng Wang"], "published": "2022-07-31 04:51:27", "updated": "2022-10-27 11:30:23", "summary": "Referring Expression Comprehension (REC) is one of the most important tasks\nin visual reasoning that requires a model to detect the target object referred\nby a natural language expression. Among the proposed pipelines, the one-stage\nReferring Expression Comprehension (OSREC) has become the dominant trend since\nit merges the region proposal and selection stages. Many state-of-the-art OSREC\nmodels adopt a multi-hop reasoning strategy because a sequence of objects is\nfrequently mentioned in a single expression which needs multi-hop reasoning to\nanalyze the semantic relation. However, one unsolved issue of these models is\nthat the number of reasoning steps needs to be pre-defined and fixed before\ninference, ignoring the varying complexity of expressions. In this paper, we\npropose a Dynamic Multi-step Reasoning Network, which allows the reasoning\nsteps to be dynamically adjusted based on the reasoning state and expression\ncomplexity. Specifically, we adopt a Transformer module to memorize & process\nthe reasoning state and a Reinforcement Learning strategy to dynamically infer\nthe reasoning steps. The work achieves the state-of-the-art performance or\nsignificant improvements on several REC datasets, ranging from RefCOCO (+, g)\nwith short expressions, to Ref-Reasoning, a dataset with long and complex\ncompositional expressions.", "comment": "27 pages, 6 figures", "links": []}
{"entry_id": "2206.11212", "title": "VisFIS: Visual Feature Importance Supervision with Right-for-the-Right-Reason Objectives", "authors": ["Zhuofan Ying", "Peter Hase", "Mohit Bansal"], "published": "2022-06-22 17:02:01", "updated": "2022-10-25 19:25:54", "summary": "Many past works aim to improve visual reasoning in models by supervising\nfeature importance (estimated by model explanation techniques) with human\nannotations such as highlights of important image regions. However, recent work\nhas shown that performance gains from feature importance (FI) supervision for\nVisual Question Answering (VQA) tasks persist even with random supervision,\nsuggesting that these methods do not meaningfully align model FI with human FI.\nIn this paper, we show that model FI supervision can meaningfully improve VQA\nmodel accuracy as well as performance on several Right-for-the-Right-Reason\n(RRR) metrics by optimizing for four key model objectives: (1) accurate\npredictions given limited but sufficient information (Sufficiency); (2)\nmax-entropy predictions given no important information (Uncertainty); (3)\ninvariance of predictions to changes in unimportant features (Invariance); and\n(4) alignment between model FI explanations and human FI explanations\n(Plausibility). Our best performing method, Visual Feature Importance\nSupervision (VisFIS), outperforms strong baselines on benchmark VQA datasets in\nterms of both in-distribution and out-of-distribution accuracy. While past work\nsuggests that the mechanism for improved accuracy is through improved\nexplanation plausibility, we show that this relationship depends crucially on\nexplanation faithfulness (whether explanations truly represent the model's\ninternal reasoning). Predictions are more accurate when explanations are\nplausible and faithful, and not when they are plausible but not faithful.\nLastly, we show that, surprisingly, RRR metrics are not predictive of\nout-of-distribution model accuracy when controlling for a model's\nin-distribution accuracy, which calls into question the value of these metrics\nfor evaluating model reasoning. All supporting code is available at\nhttps://github.com/zfying/visfis", "comment": "NeurIPS 2022 (first two authors contributed equally)", "links": []}
{"entry_id": "2112.08723", "title": "Distilled Dual-Encoder Model for Vision-Language Understanding", "authors": ["Zekun Wang", "Wenhui Wang", "Haichao Zhu", "Ming Liu", "Bing Qin", "Furu Wei"], "published": "2021-12-16 09:21:18", "updated": "2022-10-17 16:27:09", "summary": "We propose a cross-modal attention distillation framework to train a\ndual-encoder model for vision-language understanding tasks, such as visual\nreasoning and visual question answering. Dual-encoder models have a faster\ninference speed than fusion-encoder models and enable the pre-computation of\nimages and text during inference. However, the shallow interaction module used\nin dual-encoder models is insufficient to handle complex vision-language\nunderstanding tasks. In order to learn deep interactions of images and text, we\nintroduce cross-modal attention distillation, which uses the image-to-text and\ntext-to-image attention distributions of a fusion-encoder model to guide the\ntraining of our dual-encoder model. In addition, we show that applying the\ncross-modal attention distillation for both pre-training and fine-tuning stages\nachieves further improvements. Experimental results demonstrate that the\ndistilled dual-encoder model achieves competitive performance for visual\nreasoning, visual entailment and visual question answering tasks while enjoying\na much faster inference speed than fusion-encoder models. Our code and models\nwill be publicly available at https://github.com/kugwzk/Distilled-DualEncoder.", "comment": "EMNLP 2022", "links": []}
{"entry_id": "2210.05335", "title": "MAP: Modality-Agnostic Uncertainty-Aware Vision-Language Pre-training Model", "authors": ["Yatai Ji", "Junjie Wang", "Yuan Gong", "Lin Zhang", "Yanru Zhu", "Hongfa Wang", "Jiaxing Zhang", "Tetsuya Sakai", "Yujiu Yang"], "published": "2022-10-11 10:54:54", "updated": "2022-10-11 10:54:54", "summary": "Multimodal semantic understanding often has to deal with uncertainty, which\nmeans the obtained message tends to refer to multiple targets. Such uncertainty\nis problematic for our interpretation, including intra-modal and inter-modal\nuncertainty. Little effort studies the modeling of this uncertainty,\nparticularly in pre-training on unlabeled datasets and fine-tuning in\ntask-specific downstream tasks. To address this, we project the representations\nof all modalities as probabilistic distributions via a Probability Distribution\nEncoder (PDE) by utilizing rich multimodal semantic information. Furthermore,\nwe integrate uncertainty modeling with popular pre-training frameworks and\npropose suitable pre-training tasks: Distribution-based Vision-Language\nContrastive learning (D-VLC), Distribution-based Masked Language Modeling\n(D-MLM), and Distribution-based Image-Text Matching (D-ITM). The fine-tuned\nmodels are applied to challenging downstream tasks, including image-text\nretrieval, visual question answering, visual reasoning, and visual entailment,\nand achieve state-of-the-art results. Code is released at\nhttps://github.com/IIGROUP/MAP.", "comment": null, "links": []}
{"entry_id": "2210.04183", "title": "MAMO: Masked Multimodal Modeling for Fine-Grained Vision-Language Representation Learning", "authors": ["Zijia Zhao", "Longteng Guo", "Xingjian He", "Shuai Shao", "Zehuan Yuan", "Jing Liu"], "published": "2022-10-09 06:31:15", "updated": "2022-10-09 06:31:15", "summary": "Multimodal representation learning has shown promising improvements on\nvarious vision-language tasks. Most existing methods excel at building\nglobal-level alignment between vision and language while lacking effective\nfine-grained image-text interaction. In this paper, we propose a jointly masked\nmultimodal modeling method to learn fine-grained multimodal representations.\nOur method performs joint masking on image-text input and integrates both\nimplicit and explicit targets for the masked signals to recover. The implicit\ntarget provides a unified and debiased objective for vision and language, where\nthe model predicts latent multimodal representations of the unmasked input. The\nexplicit target further enriches the multimodal representations by recovering\nhigh-level and semantically meaningful information: momentum visual features of\nimage patches and concepts of word tokens. Through such a masked modeling\nprocess, our model not only learns fine-grained multimodal interaction, but\nalso avoids the semantic gap between high-level representations and low- or\nmid-level prediction targets (e.g. image pixels), thus producing semantically\nrich multimodal representations that perform well on both zero-shot and\nfine-tuned settings. Our pre-trained model (named MAMO) achieves\nstate-of-the-art performance on various downstream vision-language tasks,\nincluding image-text retrieval, visual question answering, visual reasoning,\nand weakly-supervised visual grounding.", "comment": null, "links": []}
{"entry_id": "2208.13628", "title": "Efficient Vision-Language Pretraining with Visual Concepts and Hierarchical Alignment", "authors": ["Mustafa Shukor", "Guillaume Couairon", "Matthieu Cord"], "published": "2022-08-29 14:24:08", "updated": "2022-10-05 11:35:46", "summary": "Vision and Language Pretraining has become the prevalent approach for\ntackling multimodal downstream tasks. The current trend is to move towards ever\nlarger models and pretraining datasets. This computational headlong rush does\nnot seem reasonable in the long term to move toward sustainable solutions, and\nde facto excludes academic laboratories with limited resources. In this work,\nwe propose a new framework, dubbed ViCHA, that efficiently exploits the input\ndata to boost the learning by: (a) a new hierarchical cross-modal alignment\nloss, (b) new self-supervised scheme based on masked image modeling, (c)\nleveraging image-level annotations, called Visual Concepts, obtained with\nexisting foundation models such as CLIP to boost the performance of the image\nencoder. Although pretrained on four times less data, our ViCHA strategy\noutperforms other approaches on several downstream tasks such as Image-Text\nRetrieval, VQA, Visual Reasoning, Visual Entailment and Visual Grounding. The\ncode will be made publicly available here: https://github.com/mshukor/ViCHA", "comment": "BMVC 2022", "links": []}
{"entry_id": "2210.01338", "title": "Learning to Collocate Visual-Linguistic Neural Modules for Image Captioning", "authors": ["Xu Yang", "Hanwang Zhang", "Chongyang Gao", "Jianfei Cai"], "published": "2022-10-04 03:09:50", "updated": "2022-10-04 03:09:50", "summary": "Humans tend to decompose a sentence into different parts like \\textsc{sth do\nsth at someplace} and then fill each part with certain content. Inspired by\nthis, we follow the \\textit{principle of modular design} to propose a novel\nimage captioner: learning to Collocate Visual-Linguistic Neural Modules\n(CVLNM). Unlike the \\re{widely used} neural module networks in VQA, where the\nlanguage (\\ie, question) is fully observable, \\re{the task of collocating\nvisual-linguistic modules is more challenging.} This is because the language is\nonly partially observable, for which we need to dynamically collocate the\nmodules during the process of image captioning. To sum up, we make the\nfollowing technical contributions to design and train our CVLNM: 1)\n\\textit{distinguishable module design} -- \\re{four modules in the encoder}\nincluding one linguistic module for function words and three visual modules for\ndifferent content words (\\ie, noun, adjective, and verb) and another linguistic\none in the decoder for commonsense reasoning, 2) a self-attention based\n\\textit{module controller} for robustifying the visual reasoning, 3) a\npart-of-speech based \\textit{syntax loss} imposed on the module controller for\nfurther regularizing the training of our CVLNM. Extensive experiments on the\nMS-COCO dataset show that our CVLNM is more effective, \\eg, achieving a new\nstate-of-the-art 129.5 CIDEr-D, and more robust, \\eg, being less likely to\noverfit to dataset bias and suffering less when fewer training samples are\navailable. Codes are available at \\url{https://github.com/GCYZSL/CVLMN}", "comment": "Accepted to IJCV. Codes are available at\n  https://github.com/GCYZSL/CVLMN", "links": []}
{"entry_id": "2210.00858", "title": "A Hybrid Compositional Reasoning Approach for Interactive Robot Manipulation", "authors": ["Georgios Tziafas", "Hamidreza Kasaei"], "published": "2022-10-03 12:21:45", "updated": "2022-10-03 12:21:45", "summary": "In this paper we present a neuro-symbolic (hybrid) compositional reasoning\nmodel for coupling language-guided visual reasoning with robot manipulation. A\nnon-expert human user can prompt the robot agent using natural language,\nproviding either a referring expression (REC), a question (VQA) or a grasp\naction instruction. The model can tackle all cases in a task-agnostic fashion\nthrough the utilization of a shared library of primitive skills. Each primitive\nhandles an independent sub-task, such as reasoning about visual attributes,\nspatial relation comprehension, logic and enumeration, as well as arm control.\nA language parser maps the input query to an executable program composed of\nsuch primitives depending on the context. While some primitives are purely\nsymbolic operations (e.g. counting), others are trainable neural functions\n(e.g. grounding words to images), therefore marrying the interpretability and\nsystematic generalization benefits of discrete symbolic approaches with the\nscalability and representational power of deep networks. We generate a\nsynthetic dataset of tabletop scenes to train our approach and perform several\nevaluation experiments for VQA in the synthetic and a real RGB-D dataset.\nResults show that the proposed method achieves very high accuracy while being\ntransferable to novel content with few-shot visual fine-tuning. Finally, we\nintegrate our method with a robot framework and demonstrate how it can serve as\nan interpretable solution for an interactive object picking task, both in\nsimulation and with a real robot.", "comment": "Submitted in RA-L, supplemetary video: https://youtu.be/Ry_rJ0F4cNk", "links": []}
{"entry_id": "2210.00220", "title": "A Dual-Attention Learning Network with Word and Sentence Embedding for Medical Visual Question Answering", "authors": ["Xiaofei Huang", "Hongfang Gong"], "published": "2022-10-01 08:32:40", "updated": "2022-10-01 08:32:40", "summary": "Research in medical visual question answering (MVQA) can contribute to the\ndevelopment of computeraided diagnosis. MVQA is a task that aims to predict\naccurate and convincing answers based on given medical images and associated\nnatural language questions. This task requires extracting medical\nknowledge-rich feature content and making fine-grained understandings of them.\nTherefore, constructing an effective feature extraction and understanding\nscheme are keys to modeling. Existing MVQA question extraction schemes mainly\nfocus on word information, ignoring medical information in the text. Meanwhile,\nsome visual and textual feature understanding schemes cannot effectively\ncapture the correlation between regions and keywords for reasonable visual\nreasoning. In this study, a dual-attention learning network with word and\nsentence embedding (WSDAN) is proposed. We design a module, transformer with\nsentence embedding (TSE), to extract a double embedding representation of\nquestions containing keywords and medical information. A dualattention learning\n(DAL) module consisting of self-attention and guided attention is proposed to\nmodel intensive intramodal and intermodal interactions. With multiple DAL\nmodules (DALs), learning visual and textual co-attention can increase the\ngranularity of understanding and improve visual reasoning. Experimental results\non the ImageCLEF 2019 VQA-MED (VQA-MED 2019) and VQA-RAD datasets demonstrate\nthat our proposed method outperforms previous state-of-the-art methods.\nAccording to the ablation studies and Grad-CAM maps, WSDAN can extract rich\ntextual information and has strong visual reasoning ability.", "comment": null, "links": []}
{"entry_id": "2209.15087", "title": "Zero-shot visual reasoning through probabilistic analogical mapping", "authors": ["Taylor W. Webb", "Shuhao Fu", "Trevor Bihl", "Keith J. Holyoak", "Hongjing Lu"], "published": "2022-09-29 20:29:26", "updated": "2022-09-29 20:29:26", "summary": "Human reasoning is grounded in an ability to identify highly abstract\ncommonalities governing superficially dissimilar visual inputs. Recent efforts\nto develop algorithms with this capacity have largely focused on approaches\nthat require extensive direct training on visual reasoning tasks, and yield\nlimited generalization to problems with novel content. In contrast, a long\ntradition of research in cognitive science has focused on elucidating the\ncomputational principles underlying human analogical reasoning; however, this\nwork has generally relied on manually constructed representations. Here we\npresent visiPAM (visual Probabilistic Analogical Mapping), a model of visual\nreasoning that synthesizes these two approaches. VisiPAM employs learned\nrepresentations derived directly from naturalistic visual inputs, coupled with\na similarity-based mapping operation derived from cognitive theories of human\nreasoning. We show that without any direct training, visiPAM outperforms a\nstate-of-the-art deep learning model on an analogical mapping task. In\naddition, visiPAM closely matches the pattern of human performance on a novel\ntask involving mapping of 3D objects across disparate categories.", "comment": null, "links": []}
{"entry_id": "2209.11990", "title": "Deep Neural Networks for Visual Reasoning", "authors": ["Thao Minh Le"], "published": "2022-09-24 12:11:00", "updated": "2022-09-24 12:11:00", "summary": "Visual perception and language understanding are - fundamental components of\nhuman intelligence, enabling them to understand and reason about objects and\ntheir interactions. It is crucial for machines to have this capacity to reason\nusing these two modalities to invent new robot-human collaborative systems.\nRecent advances in deep learning have built separate sophisticated\nrepresentations of both visual scenes and languages. However, understanding the\nassociations between the two modalities in a shared context for multimodal\nreasoning remains a challenge. Focusing on language and vision modalities, this\nthesis advances the understanding of how to exploit and use pivotal aspects of\nvision-and-language tasks with neural networks to support reasoning. We derive\nthese understandings from a series of works, making a two-fold contribution:\n(i) effective mechanisms for content selection and construction of temporal\nrelations from dynamic visual scenes in response to a linguistic query and\npreparing adequate knowledge for the reasoning process (ii) new frameworks to\nperform reasoning with neural networks by exploiting visual-linguistic\nassociations, deduced either directly from data or guided by external priors.", "comment": "PhD thesis", "links": []}
{"entry_id": "2206.04928", "title": "MAREO: Memory- and Attention- based visual REasOning", "authors": ["Mohit Vaishnav", "Thomas Serre"], "published": "2022-06-10 07:52:06", "updated": "2022-09-22 11:57:12", "summary": "Humans continue to outperform modern AI systems in their ability to parse and\nunderstand complex visual scenes flexibly. Attention and memory are two systems\nknown to play a critical role in our ability to selectively maintain and\nmanipulate behaviorally-relevant visual information to solve some of the most\nchallenging visual reasoning tasks. Here, we present a novel architecture for\nvisual reasoning inspired by the cognitive-science literature on visual\nreasoning, the Memory- and Attention-based (visual) REasOning (MAREO)\narchitecture. MAREO instantiates an active-vision theory, which posits that the\nbrain solves complex visual reasoning problems compositionally by learning to\ncombine previously-learned elementary visual operations to form more complex\nvisual routines. MAREO learns to solve visual reasoning tasks via sequences of\nattention shifts to route and maintain task-relevant visual information into a\nmemory bank via a multi-head transformer module. Visual routines are then\ndeployed by a dedicated reasoning module trained to judge various relations\nbetween objects in the scenes. Experiments on tasks containing complex visual\nrelations (SVRT challenge) and same-different differentiation, relation match\nto sample, Raven's and Identity rules from ART challenge demonstrate MAREO's\nability to learn visual routines in a robust and sample-efficient manner. We\nalso show the zero-shot generalization on unseen tasks and the compositionality\nnature of the architecture.", "comment": null, "links": []}
{"entry_id": "2209.09115", "title": "Compositional Law Parsing with Latent Random Functions", "authors": ["Fan Shi", "Bin Li", "Xiangyang Xue"], "published": "2022-09-15 06:57:23", "updated": "2022-09-15 06:57:23", "summary": "Human cognition has compositionality. We understand a scene by decomposing\nthe scene into different concepts (e.g. shape and position of an object) and\nlearning the respective laws of these concepts which may be either natural\n(e.g. laws of motion) or man-made (e.g. laws of a game). The automatic parsing\nof these laws indicates the model's ability to understand the scene, which\nmakes law parsing play a central role in many visual tasks. In this paper, we\npropose a deep latent variable model for Compositional LAw Parsing (CLAP). CLAP\nachieves the human-like compositionality ability through an encoding-decoding\narchitecture to represent concepts in the scene as latent variables, and\nfurther employ concept-specific random functions, instantiated with Neural\nProcesses, in the latent space to capture the law on each concept. Our\nexperimental results demonstrate that CLAP outperforms the compared baseline\nmethods in multiple visual tasks including intuitive physics, abstract visual\nreasoning, and scene representation. In addition, CLAP can learn\nconcept-specific laws in a scene without supervision and one can edit laws\nthrough modifying the corresponding latent random functions, validating its\ninterpretability and manipulability.", "comment": null, "links": []}
{"entry_id": "2209.07000", "title": "VIPHY: Probing \"Visible\" Physical Commonsense Knowledge", "authors": ["Shikhar Singh", "Ehsan Qasemi", "Muhao Chen"], "published": "2022-09-15 02:06:25", "updated": "2022-09-15 02:06:25", "summary": "In recent years, vision-language models (VLMs) have shown remarkable\nperformance on visual reasoning tasks (e.g. attributes, location). While such\ntasks measure the requisite knowledge to ground and reason over a given visual\ninstance, they do not, however, measure the ability of VLMs to retain and\ngeneralize such knowledge. In this work, we evaluate their ability to acquire\n\"visible\" physical knowledge -- the information that is easily accessible from\nimages of static scenes, particularly across the dimensions of object color,\nsize and space. We build an automatic pipeline to derive a comprehensive\nknowledge resource for calibrating and probing these models. Our results\nindicate a severe gap between model and human performance across all three\ntasks. Furthermore, our caption pretrained baseline (CapBERT) significantly\noutperforms VLMs on both size and spatial tasks -- highlighting that despite\nsufficient access to ground language with visual modality, they struggle to\nretain such knowledge. The dataset and code are available at\nhttps://github.com/Axe--/ViPhy .", "comment": "In Progress (under review)", "links": []}
{"entry_id": "2206.01127", "title": "VL-BEiT: Generative Vision-Language Pretraining", "authors": ["Hangbo Bao", "Wenhui Wang", "Li Dong", "Furu Wei"], "published": "2022-06-02 16:14:19", "updated": "2022-09-03 14:18:55", "summary": "We introduce a vision-language foundation model called VL-BEiT, which is a\nbidirectional multimodal Transformer learned by generative pretraining. Our\nminimalist solution conducts masked prediction on both monomodal and multimodal\ndata with a shared Transformer. Specifically, we perform masked vision-language\nmodeling on image-text pairs, masked language modeling on texts, and masked\nimage modeling on images. VL-BEiT is learned from scratch with one unified\npretraining task, one shared backbone, and one-stage training. Our method is\nconceptually simple and empirically effective. Experimental results show that\nVL-BEiT obtains strong results on various vision-language benchmarks, such as\nvisual question answering, visual reasoning, and image-text retrieval.\nMoreover, our method learns transferable visual features, achieving competitive\nperformance on image classification, and semantic segmentation.", "comment": null, "links": []}
{"entry_id": "2209.01319", "title": "Kinova Gemini: Interactive Robot Grasping with Visual Reasoning and Conversational AI", "authors": ["Hanxiao Chen", "Jiankun Wang", "Max Q. -H. Meng"], "published": "2022-09-03 03:52:07", "updated": "2022-09-03 03:52:07", "summary": "To facilitate recent advances in robotics and AI for delicate collaboration\nbetween humans and machines, we propose the Kinova Gemini, an original robotic\nsystem that integrates conversational AI dialogue and visual reasoning to make\nthe Kinova Gen3 lite robot help people retrieve objects or complete\nperception-based pick-and-place tasks. When a person walks up to Kinova Gen3\nlite, our Kinova Gemini is able to fulfill the user's requests in three\ndifferent applications: (1) It can start a natural dialogue with people to\ninteract and assist humans to retrieve objects and hand them to the user one by\none. (2) It detects diverse objects with YOLO v3 and recognize color attributes\nof the item to ask people if they want to grasp it via the dialogue or enable\nthe user to choose which specific one is required. (3) It applies YOLO v3 to\nrecognize multiple objects and let you choose two items for perception-based\npick-and-place tasks such as \"Put the banana into the bowl\" with visual\nreasoning and conversational interaction.", "comment": null, "links": []}
{"entry_id": "2208.10442", "title": "Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks", "authors": ["Wenhui Wang", "Hangbo Bao", "Li Dong", "Johan Bjorck", "Zhiliang Peng", "Qiang Liu", "Kriti Aggarwal", "Owais Khan Mohammed", "Saksham Singhal", "Subhojit Som", "Furu Wei"], "published": "2022-08-22 16:55:04", "updated": "2022-08-31 02:26:45", "summary": "A big convergence of language, vision, and multimodal pretraining is\nemerging. In this work, we introduce a general-purpose multimodal foundation\nmodel BEiT-3, which achieves state-of-the-art transfer performance on both\nvision and vision-language tasks. Specifically, we advance the big convergence\nfrom three aspects: backbone architecture, pretraining task, and model scaling\nup. We introduce Multiway Transformers for general-purpose modeling, where the\nmodular architecture enables both deep fusion and modality-specific encoding.\nBased on the shared backbone, we perform masked \"language\" modeling on images\n(Imglish), texts (English), and image-text pairs (\"parallel sentences\") in a\nunified manner. Experimental results show that BEiT-3 obtains state-of-the-art\nperformance on object detection (COCO), semantic segmentation (ADE20K), image\nclassification (ImageNet), visual reasoning (NLVR2), visual question answering\n(VQAv2), image captioning (COCO), and cross-modal retrieval (Flickr30K, COCO).", "comment": "18 pages", "links": []}
{"entry_id": "2202.04800", "title": "The Abduction of Sherlock Holmes: A Dataset for Visual Abductive Reasoning", "authors": ["Jack Hessel", "Jena D. Hwang", "Jae Sung Park", "Rowan Zellers", "Chandra Bhagavatula", "Anna Rohrbach", "Kate Saenko", "Yejin Choi"], "published": "2022-02-10 02:26:45", "updated": "2022-07-25 17:26:06", "summary": "Humans have remarkable capacity to reason abductively and hypothesize about\nwhat lies beyond the literal content of an image. By identifying concrete\nvisual clues scattered throughout a scene, we almost can't help but draw\nprobable inferences beyond the literal scene based on our everyday experience\nand knowledge about the world. For example, if we see a \"20 mph\" sign alongside\na road, we might assume the street sits in a residential area (rather than on a\nhighway), even if no houses are pictured. Can machines perform similar visual\nreasoning?\n  We present Sherlock, an annotated corpus of 103K images for testing machine\ncapacity for abductive reasoning beyond literal image contents. We adopt a\nfree-viewing paradigm: participants first observe and identify salient clues\nwithin images (e.g., objects, actions) and then provide a plausible inference\nabout the scene, given the clue. In total, we collect 363K (clue, inference)\npairs, which form a first-of-its-kind abductive visual reasoning dataset. Using\nour corpus, we test three complementary axes of abductive reasoning. We\nevaluate the capacity of models to: i) retrieve relevant inferences from a\nlarge candidate corpus; ii) localize evidence for inferences via bounding\nboxes, and iii) compare plausible inferences to match human judgments on a\nnewly-collected diagnostic corpus of 19K Likert-scale judgments. While we find\nthat fine-tuning CLIP-RN50x64 with a multitask objective outperforms strong\nbaselines, significant headroom exists between model performance and human\nagreement. Data, models, and leaderboard available at\nhttp://visualabduction.com/", "comment": "code, data, models at http://visualabduction.com/", "links": []}
{"entry_id": "2207.06403", "title": "3D Concept Grounding on Neural Fields", "authors": ["Yining Hong", "Yilun Du", "Chunru Lin", "Joshua B. Tenenbaum", "Chuang Gan"], "published": "2022-07-13 17:59:33", "updated": "2022-07-13 17:59:33", "summary": "In this paper, we address the challenging problem of 3D concept grounding\n(i.e. segmenting and learning visual concepts) by looking at RGBD images and\nreasoning about paired questions and answers. Existing visual reasoning\napproaches typically utilize supervised methods to extract 2D segmentation\nmasks on which concepts are grounded. In contrast, humans are capable of\ngrounding concepts on the underlying 3D representation of images. However,\ntraditionally inferred 3D representations (e.g., point clouds, voxelgrids, and\nmeshes) cannot capture continuous 3D features flexibly, thus making it\nchallenging to ground concepts to 3D regions based on the language description\nof the object being referred to. To address both issues, we propose to leverage\nthe continuous, differentiable nature of neural fields to segment and learn\nconcepts. Specifically, each 3D coordinate in a scene is represented as a\nhigh-dimensional descriptor. Concept grounding can then be performed by\ncomputing the similarity between the descriptor vector of a 3D coordinate and\nthe vector embedding of a language concept, which enables segmentations and\nconcept learning to be jointly learned on neural fields in a differentiable\nfashion. As a result, both 3D semantic and instance segmentations can emerge\ndirectly from question answering supervision using a set of defined neural\noperators on top of neural fields (e.g., filtering and counting). Experimental\nresults show that our proposed framework outperforms\nunsupervised/language-mediated segmentation models on semantic and instance\nsegmentation tasks, as well as outperforms existing models on the challenging\n3D aware visual reasoning tasks. Furthermore, our framework can generalize well\nto unseen shape categories and real scans.", "comment": "Project page: http://3d-cg.csail.mit.edu", "links": []}
{"entry_id": "2206.08358", "title": "MixGen: A New Multi-Modal Data Augmentation", "authors": ["Xiaoshuai Hao", "Yi Zhu", "Srikar Appalaraju", "Aston Zhang", "Wanqian Zhang", "Bo Li", "Mu Li"], "published": "2022-06-16 17:58:09", "updated": "2022-07-07 16:30:30", "summary": "Data augmentation is a necessity to enhance data efficiency in deep learning.\nFor vision-language pre-training, data is only augmented either for images or\nfor text in previous works. In this paper, we present MixGen: a joint data\naugmentation for vision-language representation learning to further improve\ndata efficiency. It generates new image-text pairs with semantic relationships\npreserved by interpolating images and concatenating text. It's simple, and can\nbe plug-and-played into existing pipelines. We evaluate MixGen on four\narchitectures, including CLIP, ViLT, ALBEF and TCL, across five downstream\nvision-language tasks to show its versatility and effectiveness. For example,\nadding MixGen in ALBEF pre-training leads to absolute performance improvements\non downstream tasks: image-text retrieval (+6.2% on COCO fine-tuned and +5.3%\non Flicker30K zero-shot), visual grounding (+0.9% on RefCOCO+), visual\nreasoning (+0.9% on NLVR$^{2}$), visual question answering (+0.3% on VQA2.0),\nand visual entailment (+0.4% on SNLI-VE).", "comment": "Technical report. First three authors contributed equally. Code are\n  available at https://github.com/amazon-research/mix-generation", "links": []}
{"entry_id": "2109.13156", "title": "DAReN: A Collaborative Approach Towards Reasoning And Disentangling", "authors": ["Pritish Sahu", "Kalliopi Basioti", "Vladimir Pavlovic"], "published": "2021-09-27 16:10:30", "updated": "2022-06-30 01:14:40", "summary": "Computational learning approaches to solving visual reasoning tests, such as\nRaven's Progressive Matrices (RPM), critically depend on the ability to\nidentify the visual concepts used in the test (i.e., the representation) as\nwell as the latent rules based on those concepts (i.e., the reasoning).\nHowever, learning of representation and reasoning is a challenging and\nill-posed task, often approached in a stage-wise manner (first representation,\nthen reasoning). In this work, we propose an end-to-end joint\nrepresentation-reasoning learning framework, which leverages a weak form of\ninductive bias to improve both tasks together. Specifically, we introduce a\ngeneral generative graphical model for RPMs, GM-RPM, and apply it to solve the\nreasoning test. We accomplish this using a novel learning framework\nDisentangling based Abstract Reasoning Network (DAReN) based on the principles\nof GM-RPM. We perform an empirical evaluation of DAReN over several benchmark\ndatasets. DAReN shows consistent improvement over state-of-the-art (SOTA)\nmodels on both the reasoning and the disentanglement tasks. This demonstrates\nthe strong correlation between disentangled latent representation and the\nability to solve abstract visual reasoning tasks.", "comment": null, "links": []}
{"entry_id": "2206.12533", "title": "From Shallow to Deep: Compositional Reasoning over Graphs for Visual Question Answering", "authors": ["Zihao Zhu"], "published": "2022-06-25 02:20:02", "updated": "2022-06-25 02:20:02", "summary": "In order to achieve a general visual question answering (VQA) system, it is\nessential to learn to answer deeper questions that require compositional\nreasoning on the image and external knowledge. Meanwhile, the reasoning process\nshould be explicit and explainable to understand the working mechanism of the\nmodel. It is effortless for human but challenging for machines. In this paper,\nwe propose a Hierarchical Graph Neural Module Network (HGNMN) that reasons over\nmulti-layer graphs with neural modules to address the above issues.\nSpecifically, we first encode the image by multi-layer graphs from the visual,\nsemantic and commonsense views since the clues that support the answer may\nexist in different modalities. Our model consists of several well-designed\nneural modules that perform specific functions over graphs, which can be used\nto conduct multi-step reasoning within and between different graphs. Compared\nto existing modular networks, we extend visual reasoning from one graph to more\ngraphs. We can explicitly trace the reasoning process according to module\nweights and graph attentions. Experiments show that our model not only achieves\nstate-of-the-art performance on the CRIC dataset but also obtains explicit and\nexplainable reasoning procedures.", "comment": null, "links": []}
{"entry_id": "2206.09265", "title": "SAViR-T: Spatially Attentive Visual Reasoning with Transformers", "authors": ["Pritish Sahu", "Kalliopi Basioti", "Vladimir Pavlovic"], "published": "2022-06-18 18:26:20", "updated": "2022-06-22 02:00:11", "summary": "We present a novel computational model, \"SAViR-T\", for the family of visual\nreasoning problems embodied in the Raven's Progressive Matrices (RPM). Our\nmodel considers explicit spatial semantics of visual elements within each image\nin the puzzle, encoded as spatio-visual tokens, and learns the intra-image as\nwell as the inter-image token dependencies, highly relevant for the visual\nreasoning task. Token-wise relationship, modeled through a transformer-based\nSAViR-T architecture, extract group (row or column) driven representations by\nleveraging the group-rule coherence and use this as the inductive bias to\nextract the underlying rule representations in the top two row (or column) per\ntoken in the RPM. We use this relation representations to locate the correct\nchoice image that completes the last row or column for the RPM. Extensive\nexperiments across both synthetic RPM benchmarks, including RAVEN, I-RAVEN,\nRAVEN-FAIR, and PGM, and the natural image-based \"V-PROM\" demonstrate that\nSAViR-T sets a new state-of-the-art for visual reasoning, exceeding prior\nmodels' performance by a considerable margin.", "comment": null, "links": []}
{"entry_id": "2204.11167", "title": "RelViT: Concept-guided Vision Transformer for Visual Relational Reasoning", "authors": ["Xiaojian Ma", "Weili Nie", "Zhiding Yu", "Huaizu Jiang", "Chaowei Xiao", "Yuke Zhu", "Song-Chun Zhu", "Anima Anandkumar"], "published": "2022-04-24 02:46:43", "updated": "2022-06-11 13:42:27", "summary": "Reasoning about visual relationships is central to how humans interpret the\nvisual world. This task remains challenging for current deep learning\nalgorithms since it requires addressing three key technical problems jointly:\n1) identifying object entities and their properties, 2) inferring semantic\nrelations between pairs of entities, and 3) generalizing to novel\nobject-relation combinations, i.e., systematic generalization. In this work, we\nuse vision transformers (ViTs) as our base model for visual reasoning and make\nbetter use of concepts defined as object entities and their relations to\nimprove the reasoning ability of ViTs. Specifically, we introduce a novel\nconcept-feature dictionary to allow flexible image feature retrieval at\ntraining time with concept keys. This dictionary enables two new concept-guided\nauxiliary tasks: 1) a global task for promoting relational reasoning, and 2) a\nlocal task for facilitating semantic object-centric correspondence learning. To\nexamine the systematic generalization of visual reasoning models, we introduce\nsystematic splits for the standard HICO and GQA benchmarks. We show the\nresulting model, Concept-guided Vision Transformer (or RelViT for short)\nsignificantly outperforms prior approaches on HICO and GQA by 16% and 13% in\nthe original split, and by 43% and 18% in the systematic split. Our ablation\nanalyses also reveal our model's compatibility with multiple ViT variants and\nrobustness to hyper-parameters.", "comment": "ICLR 2022; Code: https://github.com/NVlabs/RelViT", "links": []}
{"entry_id": "2206.05379", "title": "A Benchmark for Compositional Visual Reasoning", "authors": ["Aimen Zerroug", "Mohit Vaishnav", "Julien Colin", "Sebastian Musslick", "Thomas Serre"], "published": "2022-06-11 00:04:49", "updated": "2022-06-11 00:04:49", "summary": "A fundamental component of human vision is our ability to parse complex\nvisual scenes and judge the relations between their constituent objects. AI\nbenchmarks for visual reasoning have driven rapid progress in recent years with\nstate-of-the-art systems now reaching human accuracy on some of these\nbenchmarks. Yet, a major gap remains in terms of the sample efficiency with\nwhich humans and AI systems learn new visual reasoning tasks. Humans'\nremarkable efficiency at learning has been at least partially attributed to\ntheir ability to harness compositionality -- such that they can efficiently\ntake advantage of previously gained knowledge when learning new tasks. Here, we\nintroduce a novel visual reasoning benchmark, Compositional Visual Relations\n(CVR), to drive progress towards the development of more data-efficient\nlearning algorithms. We take inspiration from fluidic intelligence and\nnon-verbal reasoning tests and describe a novel method for creating\ncompositions of abstract rules and associated image datasets at scale. Our\nproposed benchmark includes measures of sample efficiency, generalization and\ntransfer across task rules, as well as the ability to leverage\ncompositionality. We systematically evaluate modern neural architectures and\nfind that, surprisingly, convolutional architectures surpass transformer-based\narchitectures across all performance measures in most data regimes. However,\nall computational models are a lot less data efficient compared to humans even\nafter learning informative visual representations using self-supervision.\nOverall, we hope that our challenge will spur interest in the development of\nneural architectures that can learn to harness compositionality toward more\nefficient learning.", "comment": null, "links": []}
{"entry_id": "2002.06838", "title": "Stratified Rule-Aware Network for Abstract Visual Reasoning", "authors": ["Sheng Hu", "Yuqing Ma", "Xianglong Liu", "Yanlu Wei", "Shihao Bai"], "published": "2020-02-17 08:44:05", "updated": "2022-06-07 11:49:44", "summary": "Abstract reasoning refers to the ability to analyze information, discover\nrules at an intangible level, and solve problems in innovative ways. Raven's\nProgressive Matrices (RPM) test is typically used to examine the capability of\nabstract reasoning. The subject is asked to identify the correct choice from\nthe answer set to fill the missing panel at the bottom right of RPM (e.g., a\n3$\\times$3 matrix), following the underlying rules inside the matrix. Recent\nstudies, taking advantage of Convolutional Neural Networks (CNNs), have\nachieved encouraging progress to accomplish the RPM test. However, they partly\nignore necessary inductive biases of RPM solver, such as order sensitivity\nwithin each row/column and incremental rule induction. To address this problem,\nin this paper we propose a Stratified Rule-Aware Network (SRAN) to generate the\nrule embeddings for two input sequences. Our SRAN learns multiple granularity\nrule embeddings at different levels, and incrementally integrates the\nstratified embedding flows through a gated fusion module. With the help of\nembeddings, a rule similarity metric is applied to guarantee that SRAN can not\nonly be trained using a tuplet loss but also infer the best answer efficiently.\nWe further point out the severe defects existing in the popular RAVEN dataset\nfor RPM test, which prevent from the fair evaluation of the abstract reasoning\nability. To fix the defects, we propose an answer set generation algorithm\ncalled Attribute Bisection Tree (ABT), forming an improved dataset named\nImpartial-RAVEN (I-RAVEN for short). Extensive experiments are conducted on\nboth PGM and I-RAVEN datasets, showing that our SRAN outperforms the\nstate-of-the-art models by a considerable margin.", "comment": "AAAI 2021 paper. Code: https://github.com/husheng12345/SRAN", "links": []}
{"entry_id": "2103.15022", "title": "'Just because you are right, doesn't mean I am wrong': Overcoming a Bottleneck in the Development and Evaluation of Open-Ended Visual Question Answering (VQA) Tasks", "authors": ["Man Luo", "Shailaja Keyur Sampat", "Riley Tallman", "Yankai Zeng", "Manuha Vancha", "Akarshan Sajja", "Chitta Baral"], "published": "2021-03-28 00:07:08", "updated": "2022-05-31 18:05:49", "summary": "GQA~\\citep{hudson2019gqa} is a dataset for real-world visual reasoning and\ncompositional question answering. We found that many answers predicted by the\nbest vision-language models on the GQA dataset do not match the ground-truth\nanswer but still are semantically meaningful and correct in the given context.\nIn fact, this is the case with most existing visual question answering (VQA)\ndatasets where they assume only one ground-truth answer for each question. We\npropose Alternative Answer Sets (AAS) of ground-truth answers to address this\nlimitation, which is created automatically using off-the-shelf NLP tools. We\nintroduce a semantic metric based on AAS and modify top VQA solvers to support\nmultiple plausible answers for a question. We implement this approach on the\nGQA dataset and show the performance improvements. Code and data are available\nin this link \\url{https://github.com/luomancs/alternative_answer_set.git}.", "comment": "accepted to EACL 2021", "links": []}
{"entry_id": "2205.14288", "title": "Few-shot Subgoal Planning with Language Models", "authors": ["Lajanugen Logeswaran", "Yao Fu", "Moontae Lee", "Honglak Lee"], "published": "2022-05-28 01:03:30", "updated": "2022-05-28 01:03:30", "summary": "Pre-trained large language models have shown successful progress in many\nlanguage understanding benchmarks. This work explores the capability of these\nmodels to predict actionable plans in real-world environments. Given a text\ninstruction, we show that language priors encoded in pre-trained language\nmodels allow us to infer fine-grained subgoal sequences. In contrast to recent\nmethods which make strong assumptions about subgoal supervision, our\nexperiments show that language models can infer detailed subgoal sequences from\nfew training sequences without any fine-tuning. We further propose a simple\nstrategy to re-rank language model predictions based on interaction and\nfeedback from the environment. Combined with pre-trained navigation and visual\nreasoning components, our approach demonstrates competitive performance on\nsubgoal prediction and task completion in the ALFRED benchmark compared to\nprior methods that assume more subgoal supervision.", "comment": "NAACL 2022", "links": []}
{"entry_id": "2205.13803", "title": "Bongard-HOI: Benchmarking Few-Shot Visual Reasoning for Human-Object Interactions", "authors": ["Huaizu Jiang", "Xiaojian Ma", "Weili Nie", "Zhiding Yu", "Yuke Zhu", "Anima Anandkumar"], "published": "2022-05-27 07:36:29", "updated": "2022-05-27 07:36:29", "summary": "A significant gap remains between today's visual pattern recognition models\nand human-level visual cognition especially when it comes to few-shot learning\nand compositional reasoning of novel concepts. We introduce Bongard-HOI, a new\nvisual reasoning benchmark that focuses on compositional learning of\nhuman-object interactions (HOIs) from natural images. It is inspired by two\ndesirable characteristics from the classical Bongard problems (BPs): 1)\nfew-shot concept learning, and 2) context-dependent reasoning. We carefully\ncurate the few-shot instances with hard negatives, where positive and negative\nimages only disagree on action labels, making mere recognition of object\ncategories insufficient to complete our benchmarks. We also design multiple\ntest sets to systematically study the generalization of visual learning models,\nwhere we vary the overlap of the HOI concepts between the training and test\nsets of few-shot instances, from partial to no overlaps. Bongard-HOI presents a\nsubstantial challenge to today's visual recognition models. The\nstate-of-the-art HOI detection model achieves only 62% accuracy on few-shot\nbinary prediction while even amateur human testers on MTurk have 91% accuracy.\nWith the Bongard-HOI benchmark, we hope to further advance research efforts in\nvisual reasoning, especially in holistic perception-reasoning systems and\nbetter representation learning.", "comment": "CVPR 2022 (oral); Code: https://github.com/NVlabs/Bongard-HOI", "links": []}
{"entry_id": "2205.12616", "title": "Guiding Visual Question Answering with Attention Priors", "authors": ["Thao Minh Le", "Vuong Le", "Sunil Gupta", "Svetha Venkatesh", "Truyen Tran"], "published": "2022-05-25 09:53:47", "updated": "2022-05-25 09:53:47", "summary": "The current success of modern visual reasoning systems is arguably attributed\nto cross-modality attention mechanisms. However, in deliberative reasoning such\nas in VQA, attention is unconstrained at each step, and thus may serve as a\nstatistical pooling mechanism rather than a semantic operation intended to\nselect information relevant to inference. This is because at training time,\nattention is only guided by a very sparse signal (i.e. the answer label) at the\nend of the inference chain. This causes the cross-modality attention weights to\ndeviate from the desired visual-language bindings. To rectify this deviation,\nwe propose to guide the attention mechanism using explicit linguistic-visual\ngrounding. This grounding is derived by connecting structured linguistic\nconcepts in the query to their referents among the visual objects. Here we\nlearn the grounding from the pairing of questions and images alone, without the\nneed for answer annotation or external grounding supervision. This grounding\nguides the attention mechanism inside VQA models through a duality of\nmechanisms: pre-training attention weight calculation and directly guiding the\nweights at inference time on a case-by-case basis. The resultant algorithm is\ncapable of probing attention-based reasoning models, injecting relevant\nassociative knowledge, and regulating the core reasoning process. This scalable\nenhancement improves the performance of VQA models, fortifies their robustness\nto limited access to supervised data, and increases interpretability.", "comment": "Preprint, 10 pages", "links": []}
{"entry_id": "2205.08013", "title": "Continual learning on 3D point clouds with random compressed rehearsal", "authors": ["Maciej Zamorski", "Michał Stypułkowski", "Konrad Karanowski", "Tomasz Trzciński", "Maciej Zięba"], "published": "2022-05-16 22:59:52", "updated": "2022-05-20 12:09:47", "summary": "Contemporary deep neural networks offer state-of-the-art results when applied\nto visual reasoning, e.g., in the context of 3D point cloud data. Point clouds\nare important datatype for precise modeling of three-dimensional environments,\nbut effective processing of this type of data proves to be challenging. In the\nworld of large, heavily-parameterized network architectures and\ncontinuously-streamed data, there is an increasing need for machine learning\nmodels that can be trained on additional data. Unfortunately, currently\navailable models cannot fully leverage training on additional data without\nlosing their past knowledge. Combating this phenomenon, called catastrophic\nforgetting, is one of the main objectives of continual learning. Continual\nlearning for deep neural networks has been an active field of research,\nprimarily in 2D computer vision, natural language processing, reinforcement\nlearning, and robotics. However, in 3D computer vision, there are hardly any\ncontinual learning solutions specifically designed to take advantage of point\ncloud structure. This work proposes a novel neural network architecture capable\nof continual learning on 3D point cloud data. We utilize point cloud structure\nproperties for preserving a heavily compressed set of past data. By using\nrehearsal and reconstruction as regularization methods of the learning process,\nour approach achieves a significant decrease of catastrophic forgetting\ncompared to the existing solutions on several most popular point cloud datasets\nconsidering two continual learning settings: when a task is known beforehand,\nand in the challenging scenario of when task information is unknown to the\nmodel.", "comment": "10 pages, 3 figures", "links": []}
{"entry_id": "2205.04061", "title": "Multilevel Hierarchical Network with Multiscale Sampling for Video Question Answering", "authors": ["Min Peng", "Chongyang Wang", "Yuan Gao", "Yu Shi", "Xiang-Dong Zhou"], "published": "2022-05-09 06:28:56", "updated": "2022-05-09 06:28:56", "summary": "Video question answering (VideoQA) is challenging given its multimodal\ncombination of visual understanding and natural language processing. While most\nexisting approaches ignore the visual appearance-motion information at\ndifferent temporal scales, it is unknown how to incorporate the multilevel\nprocessing capacity of a deep learning model with such multiscale information.\nTargeting these issues, this paper proposes a novel Multilevel Hierarchical\nNetwork (MHN) with multiscale sampling for VideoQA. MHN comprises two modules,\nnamely Recurrent Multimodal Interaction (RMI) and Parallel Visual Reasoning\n(PVR). With a multiscale sampling, RMI iterates the interaction of\nappearance-motion information at each scale and the question embeddings to\nbuild the multilevel question-guided visual representations. Thereon, with a\nshared transformer encoder, PVR infers the visual cues at each level in\nparallel to fit with answering different question types that may rely on the\nvisual information at relevant levels. Through extensive experiments on three\nVideoQA datasets, we demonstrate improved performances than previous\nstate-of-the-arts and justify the effectiveness of each part of our method.", "comment": "Accepted by IJCAI 2022. arXiv admin note: text overlap with\n  arXiv:2109.04735", "links": []}
{"entry_id": "2205.03854", "title": "Introduction to Soar", "authors": ["John E. Laird"], "published": "2022-05-08 12:44:51", "updated": "2022-05-08 12:44:51", "summary": "This paper is the recommended initial reading for a functional overview of\nSoar, version 9.6. It includes an abstract overview of the architectural\nstructure of Soar including its processing, memories, learning modules, their\ninterfaces, and the representations of knowledge used by those modules. From\nthere it describes the processing supported by those modules, including\ndecision making, impasses and substates, procedure learning via chunking,\nreinforcement learning, semantic memory, episodic memory, and spatial-visual\nreasoning. It then reviews the levels of decision making and variety of\nlearning in Soar, and analysis of Soar as an architecture supporting general\nhuman-level AI. Following the references is an appendix that contains short\ndescriptions of recent Soar agents and a glossary of the terminology we use in\ndescribing Soar.", "comment": "29 pages", "links": []}
{"entry_id": "2205.03075", "title": "QLEVR: A Diagnostic Dataset for Quantificational Language and Elementary Visual Reasoning", "authors": ["Zechen Li", "Anders Søgaard"], "published": "2022-05-06 08:51:13", "updated": "2022-05-06 08:51:13", "summary": "Synthetic datasets have successfully been used to probe visual\nquestion-answering datasets for their reasoning abilities. CLEVR\n(johnson2017clevr), for example, tests a range of visual reasoning abilities.\nThe questions in CLEVR focus on comparisons of shapes, colors, and sizes,\nnumerical reasoning, and existence claims. This paper introduces a minimally\nbiased, diagnostic visual question-answering dataset, QLEVR, that goes beyond\nexistential and numerical quantification and focus on more complex quantifiers\nand their combinations, e.g., asking whether there are more than two red balls\nthat are smaller than at least three blue balls in an image. We describe how\nthe dataset was created and present a first evaluation of state-of-the-art\nvisual question-answering models, showing that QLEVR presents a formidable\nchallenge to our current models. Code and Dataset are available at\nhttps://github.com/zechenli03/QLEVR", "comment": "To appear at Findings of NAACL 2022", "links": []}
{"entry_id": "2205.00949", "title": "Answer-Me: Multi-Task Open-Vocabulary Visual Question Answering", "authors": ["AJ Piergiovanni", "Wei Li", "Weicheng Kuo", "Mohammad Saffar", "Fred Bertsch", "Anelia Angelova"], "published": "2022-05-02 14:53:13", "updated": "2022-05-02 14:53:13", "summary": "We present Answer-Me, a task-aware multi-task framework which unifies a\nvariety of question answering tasks, such as, visual question answering, visual\nentailment, visual reasoning. In contrast to previous works using contrastive\nor generative captioning training, we propose a novel and simple recipe to\npre-train a vision-language joint model, which is multi-task as well. The\npre-training uses only noisy image captioning data, and is formulated to use\nthe entire architecture end-to-end with both a strong language encoder and\ndecoder. Our results show state-of-the-art performance, zero-shot\ngeneralization, robustness to forgetting, and competitive single-task results\nacross a variety of question answering tasks. Our multi-task mixture training\nlearns from tasks of various question intents and thus generalizes better,\nincluding on zero-shot vision-language tasks. We conduct experiments in the\nchallenging multi-task and open-vocabulary settings and across a variety of\ndatasets and tasks, such as VQA2.0, SNLI-VE, NLVR2, GQA, VizWiz. We observe\nthat the proposed approach is able to generalize to unseen tasks and that more\ndiverse mixtures lead to higher accuracy in both known and novel tasks.", "comment": null, "links": []}
{"entry_id": "2201.12382", "title": "Deep Learning Methods for Abstract Visual Reasoning: A Survey on Raven's Progressive Matrices", "authors": ["Mikołaj Małkiński", "Jacek Mańdziuk"], "published": "2022-01-28 19:24:30", "updated": "2022-04-19 18:41:45", "summary": "Abstract visual reasoning (AVR) domain encompasses problems solving which\nrequires the ability to reason about relations among entities present in a\ngiven scene. While humans, generally, solve AVR tasks in a \"natural\" way, even\nwithout prior experience, this type of problems has proven difficult for\ncurrent machine learning systems. The paper summarises recent progress in\napplying deep learning methods to solving AVR problems, as a proxy for studying\nmachine intelligence. We focus on the most common type of AVR tasks -- the\nRaven's Progressive Matrices (RPMs) -- and provide a comprehensive review of\nthe learning methods and deep neural models applied to solve RPMs, as well as,\nthe RPM benchmark sets. Performance analysis of the state-of-the-art approaches\nto solving RPMs leads to formulation of certain insights and remarks on the\ncurrent and future trends in this area. We conclude the paper by demonstrating\nhow real-world problems can benefit from the discoveries of RPM studies.", "comment": null, "links": []}
{"entry_id": "2204.05543", "title": "Towards Reliable Image Outpainting: Learning Structure-Aware Multimodal Fusion with Depth Guidance", "authors": ["Lei Zhang", "Kang Liao", "Chunyu Lin", "Yao Zhao"], "published": "2022-04-12 06:06:50", "updated": "2022-04-12 06:06:50", "summary": "Image outpainting technology generates visually reasonable content regardless\nof authenticity, making it unreliable to serve for practical applications even\nthough introducing additional modalities eg. the sketch. Since sparse depth\nmaps are widely captured in robotics and autonomous systems, together with RGB\nimages, we combine the sparse depth in the image outpainting task to provide\nmore reliable performance. Concretely, we propose a Depth-Guided Outpainting\nNetwork (DGONet) to model the feature representations of different modalities\ndifferentially and learn the structure-aware cross-modal fusion. To this end,\ntwo components are designed to implement: 1) The Multimodal Learning Module\nproduces unique depth and RGB feature representations from the perspectives of\ndifferent modal characteristics. 2) The Depth Guidance Fusion Module leverages\nthe complete depth modality to guide the establishment of RGB contents by\nprogressive multimodal feature fusion. Furthermore, we specially design an\nadditional constraint strategy consisting of Cross-modal Loss and Edge Loss to\nenhance ambiguous contours and expedite reliable content generation. Extensive\nexperiments on KITTI demonstrate our superiority over the state-of-the-art\nmethods with more reliable content generation.", "comment": null, "links": []}
{"entry_id": "2204.02380", "title": "CLEVR-X: A Visual Reasoning Dataset for Natural Language Explanations", "authors": ["Leonard Salewski", "A. Sophia Koepke", "Hendrik P. A. Lensch", "Zeynep Akata"], "published": "2022-04-05 17:38:04", "updated": "2022-04-05 17:38:04", "summary": "Providing explanations in the context of Visual Question Answering (VQA)\npresents a fundamental problem in machine learning. To obtain detailed insights\ninto the process of generating natural language explanations for VQA, we\nintroduce the large-scale CLEVR-X dataset that extends the CLEVR dataset with\nnatural language explanations. For each image-question pair in the CLEVR\ndataset, CLEVR-X contains multiple structured textual explanations which are\nderived from the original scene graphs. By construction, the CLEVR-X\nexplanations are correct and describe the reasoning and visual information that\nis necessary to answer a given question. We conducted a user study to confirm\nthat the ground-truth explanations in our proposed dataset are indeed complete\nand relevant. We present baseline results for generating natural language\nexplanations in the context of VQA using two state-of-the-art frameworks on the\nCLEVR-X dataset. Furthermore, we provide a detailed analysis of the explanation\ngeneration quality for different question and answer types. Additionally, we\nstudy the influence of using different numbers of ground-truth explanations on\nthe convergence of natural language generation (NLG) metrics. The CLEVR-X\ndataset is publicly available at\n\\url{https://explainableml.github.io/CLEVR-X/}.", "comment": null, "links": ["http://dx.doi.org/10.1007/978-3-031-04083-2_5"]}
{"entry_id": "2204.00879", "title": "Co-VQA : Answering by Interactive Sub Question Sequence", "authors": ["Ruonan Wang", "Yuxi Qian", "Fangxiang Feng", "Xiaojie Wang", "Huixing Jiang"], "published": "2022-04-02 15:09:16", "updated": "2022-04-02 15:09:16", "summary": "Most existing approaches to Visual Question Answering (VQA) answer questions\ndirectly, however, people usually decompose a complex question into a sequence\nof simple sub questions and finally obtain the answer to the original question\nafter answering the sub question sequence(SQS). By simulating the process, this\npaper proposes a conversation-based VQA (Co-VQA) framework, which consists of\nthree components: Questioner, Oracle, and Answerer. Questioner raises the sub\nquestions using an extending HRED model, and Oracle answers them one-by-one. An\nAdaptive Chain Visual Reasoning Model (ACVRM) for Answerer is also proposed,\nwhere the question-answer pair is used to update the visual representation\nsequentially. To perform supervised learning for each model, we introduce a\nwell-designed method to build a SQS for each question on VQA 2.0 and VQA-CP v2\ndatasets. Experimental results show that our method achieves state-of-the-art\non VQA-CP v2. Further analyses show that SQSs help build direct semantic\nconnections between questions and images, provide question-adaptive\nvariable-length reasoning chains, and with explicit interpretability as well as\nerror traceability.", "comment": "Accepted by Findings of ACL 2022", "links": []}
{"entry_id": "2203.06107", "title": "REX: Reasoning-aware and Grounded Explanation", "authors": ["Shi Chen", "Qi Zhao"], "published": "2022-03-11 17:28:42", "updated": "2022-03-11 17:28:42", "summary": "Effectiveness and interpretability are two essential properties for\ntrustworthy AI systems. Most recent studies in visual reasoning are dedicated\nto improving the accuracy of predicted answers, and less attention is paid to\nexplaining the rationales behind the decisions. As a result, they commonly take\nadvantage of spurious biases instead of actually reasoning on the\nvisual-textual data, and have yet developed the capability to explain their\ndecision making by considering key information from both modalities. This paper\naims to close the gap from three distinct perspectives: first, we define a new\ntype of multi-modal explanations that explain the decisions by progressively\ntraversing the reasoning process and grounding keywords in the images. We\ndevelop a functional program to sequentially execute different reasoning steps\nand construct a new dataset with 1,040,830 multi-modal explanations. Second, we\nidentify the critical need to tightly couple important components across the\nvisual and textual modalities for explaining the decisions, and propose a novel\nexplanation generation method that explicitly models the pairwise\ncorrespondence between words and regions of interest. It improves the visual\ngrounding capability by a considerable margin, resulting in enhanced\ninterpretability and reasoning performance. Finally, with our new data and\nmethod, we perform extensive analyses to study the effectiveness of our\nexplanation under different settings, including multi-task learning and\ntransfer learning. Our code and data are available at\nhttps://github.com/szzexpoi/rex.", "comment": "To appear in CVPR2022", "links": []}
{"entry_id": "2202.10284", "title": "A Review of Emerging Research Directions in Abstract Visual Reasoning", "authors": ["Mikołaj Małkiński", "Jacek Mańdziuk"], "published": "2022-02-21 14:58:02", "updated": "2022-03-07 09:56:09", "summary": "Abstract Visual Reasoning (AVR) problems are commonly used to approximate\nhuman intelligence. They test the ability of applying previously gained\nknowledge, experience and skills in a completely new setting, which makes them\nparticularly well-suited for this task. Recently, the AVR problems have become\npopular as a proxy to study machine intelligence, which has led to emergence of\nnew distinct types of problems and multiple benchmark sets. In this work we\nreview this emerging AVR research and propose a taxonomy to categorise the AVR\ntasks along 5 dimensions: input shapes, hidden rules, target task, cognitive\nfunction, and main challenge. The perspective taken in this survey allows to\ncharacterise AVR problems with respect to their shared and distinct properties,\nprovides a unified view on the existing approaches for solving AVR tasks, shows\nhow the AVR problems relate to practical applications, and outlines promising\ndirections for future work. One of them refers to the observation that in the\nmachine learning literature different tasks are considered in isolation, which\nis in the stark contrast with the way the AVR tasks are used to measure human\nintelligence, where multiple types of problems are combined within a single IQ\ntest.", "comment": null, "links": []}
{"entry_id": "2202.12162", "title": "Measuring CLEVRness: Blackbox testing of Visual Reasoning Models", "authors": ["Spyridon Mouselinos", "Henryk Michalewski", "Mateusz Malinowski"], "published": "2022-02-24 15:59:29", "updated": "2022-02-28 14:02:08", "summary": "How can we measure the reasoning capabilities of intelligence systems? Visual\nquestion answering provides a convenient framework for testing the model's\nabilities by interrogating the model through questions about the scene.\nHowever, despite scores of various visual QA datasets and architectures, which\nsometimes yield even a super-human performance, the question of whether those\narchitectures can actually reason remains open to debate. To answer this, we\nextend the visual question answering framework and propose the following\nbehavioral test in the form of a two-player game. We consider black-box neural\nmodels of CLEVR. These models are trained on a diagnostic dataset benchmarking\nreasoning. Next, we train an adversarial player that re-configures the scene to\nfool the CLEVR model. We show that CLEVR models, which otherwise could perform\nat a human level, can easily be fooled by our agent. Our results put in doubt\nwhether data-driven approaches can do reasoning without exploiting the numerous\nbiases that are often present in those datasets. Finally, we also propose a\ncontrolled experiment measuring the efficiency of such models to learn and\nperform reasoning.", "comment": "ICLR 2022", "links": []}
{"entry_id": "2202.13115", "title": "Analysis of Visual Reasoning on One-Stage Object Detection", "authors": ["Tolga Aksoy", "Ugur Halici"], "published": "2022-02-26 11:11:59", "updated": "2022-02-26 11:11:59", "summary": "Current state-of-the-art one-stage object detectors are limited by treating\neach image region separately without considering possible relations of the\nobjects. This causes dependency solely on high-quality convolutional feature\nrepresentations for detecting objects successfully. However, this may not be\npossible sometimes due to some challenging conditions. In this paper, the usage\nof reasoning features on one-stage object detection is analyzed. We attempted\ndifferent architectures that reason the relations of the image regions by using\nself-attention. YOLOv3-Reasoner2 model spatially and semantically enhances\nfeatures in the reasoning layer and fuses them with the original convolutional\nfeatures to improve performance. The YOLOv3-Reasoner2 model achieves around\n2.5% absolute improvement with respect to baseline YOLOv3 on COCO in terms of\nmAP while still running in real-time.", "comment": "Submitted to IEEE International Conference on Image Processing (ICIP)\n  2022", "links": []}
{"entry_id": "2202.12626", "title": "Joint Answering and Explanation for Visual Commonsense Reasoning", "authors": ["Zhenyang Li", "Yangyang Guo", "Kejie Wang", "Yinwei Wei", "Liqiang Nie", "Mohan Kankanhalli"], "published": "2022-02-25 11:26:52", "updated": "2022-02-25 11:26:52", "summary": "Visual Commonsense Reasoning (VCR), deemed as one challenging extension of\nthe Visual Question Answering (VQA), endeavors to pursue a more high-level\nvisual comprehension. It is composed of two indispensable processes: question\nanswering over a given image and rationale inference for answer explanation.\nOver the years, a variety of methods tackling VCR have advanced the performance\non the benchmark dataset. Despite significant as these methods are, they often\ntreat the two processes in a separate manner and hence decompose the VCR into\ntwo irrelevant VQA instances. As a result, the pivotal connection between\nquestion answering and rationale inference is interrupted, rendering existing\nefforts less faithful on visual reasoning. To empirically study this issue, we\nperform some in-depth explorations in terms of both language shortcuts and\ngeneralization capability to verify the pitfalls of this treatment. Based on\nour findings, in this paper, we present a plug-and-play knowledge distillation\nenhanced framework to couple the question answering and rationale inference\nprocesses. The key contribution is the introduction of a novel branch, which\nserves as the bridge to conduct processes connecting. Given that our framework\nis model-agnostic, we apply it to the existing popular baselines and validate\nits effectiveness on the benchmark dataset. As detailed in the experimental\nresults, when equipped with our framework, these baselines achieve consistent\nand significant performance improvements, demonstrating the viability of\nprocesses coupling, as well as the superiority of the proposed framework.", "comment": null, "links": []}
{"entry_id": "2202.08806", "title": "Grammar-Based Grounded Lexicon Learning", "authors": ["Jiayuan Mao", "Haoyue Shi", "Jiajun Wu", "Roger P. Levy", "Joshua B. Tenenbaum"], "published": "2022-02-17 18:19:53", "updated": "2022-02-17 18:19:53", "summary": "We present Grammar-Based Grounded Lexicon Learning (G2L2), a lexicalist\napproach toward learning a compositional and grounded meaning representation of\nlanguage from grounded data, such as paired images and texts. At the core of\nG2L2 is a collection of lexicon entries, which map each word to a tuple of a\nsyntactic type and a neuro-symbolic semantic program. For example, the word\nshiny has a syntactic type of adjective; its neuro-symbolic semantic program\nhas the symbolic form {\\lambda}x. filter(x, SHINY), where the concept SHINY is\nassociated with a neural network embedding, which will be used to classify\nshiny objects. Given an input sentence, G2L2 first looks up the lexicon entries\nassociated with each token. It then derives the meaning of the sentence as an\nexecutable neuro-symbolic program by composing lexical meanings based on\nsyntax. The recovered meaning programs can be executed on grounded inputs. To\nfacilitate learning in an exponentially-growing compositional space, we\nintroduce a joint parsing and expected execution algorithm, which does local\nmarginalization over derivations to reduce the training time. We evaluate G2L2\non two domains: visual reasoning and language-driven navigation. Results show\nthat G2L2 can generalize from small amounts of data to novel compositions of\nwords.", "comment": "NeurIPS 2021. Project page: https://g2l2.csail.mit.edu/", "links": []}
{"entry_id": "2202.04053", "title": "DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generative Transformers", "authors": ["Jaemin Cho", "Abhay Zala", "Mohit Bansal"], "published": "2022-02-08 18:36:52", "updated": "2022-02-08 18:36:52", "summary": "Generating images from textual descriptions has gained a lot of attention.\nRecently, DALL-E, a multimodal transformer language model, and its variants\nhave shown high-quality text-to-image generation capabilities with a simple\narchitecture and training objective, powered by large-scale training data and\ncomputation. However, despite the interesting image generation results, there\nhas not been a detailed analysis on how to evaluate such models. In this work,\nwe investigate the reasoning capabilities and social biases of such\ntext-to-image generative transformers in detail. First, we measure four visual\nreasoning skills: object recognition, object counting, color recognition, and\nspatial relation understanding. For this, we propose PaintSkills, a diagnostic\ndataset and evaluation toolkit that measures these four visual reasoning\nskills. Second, we measure the text alignment and quality of the generated\nimages based on pretrained image captioning, image-text retrieval, and image\nclassification models. Third, we assess social biases in the models. For this,\nwe suggest evaluation of gender and racial biases of text-to-image generation\nmodels based on a pretrained image-text retrieval model and human evaluation.\nIn our experiments, we show that recent text-to-image models perform better in\nrecognizing and counting objects than recognizing colors and understanding\nspatial relations, while there exists a large gap between model performances\nand oracle accuracy on all skills. Next, we demonstrate that recent\ntext-to-image models learn specific gender/racial biases from web image-text\npairs. We also show that our automatic evaluations of visual reasoning skills\nand gender bias are highly correlated with human judgments. We hope our work\nwill help guide future progress in improving text-to-image models on visual\nreasoning skills and social biases. Code and data at:\nhttps://github.com/j-min/DallEval", "comment": "20 pages, 10 figures, 13 tables", "links": []}
{"entry_id": "2202.01334", "title": "Adaptive Discrete Communication Bottlenecks with Dynamic Vector Quantization", "authors": ["Dianbo Liu", "Alex Lamb", "Xu Ji", "Pascal Notsawo", "Mike Mozer", "Yoshua Bengio", "Kenji Kawaguchi"], "published": "2022-02-02 23:54:26", "updated": "2022-02-02 23:54:26", "summary": "Vector Quantization (VQ) is a method for discretizing latent representations\nand has become a major part of the deep learning toolkit. It has been\ntheoretically and empirically shown that discretization of representations\nleads to improved generalization, including in reinforcement learning where\ndiscretization can be used to bottleneck multi-agent communication to promote\nagent specialization and robustness. The discretization tightness of most\nVQ-based methods is defined by the number of discrete codes in the\nrepresentation vector and the codebook size, which are fixed as\nhyperparameters. In this work, we propose learning to dynamically select\ndiscretization tightness conditioned on inputs, based on the hypothesis that\ndata naturally contains variations in complexity that call for different levels\nof representational coarseness. We show that dynamically varying tightness in\ncommunication bottlenecks can improve model performance on visual reasoning and\nreinforcement learning tasks.", "comment": null, "links": []}
{"entry_id": "2101.01169", "title": "Transformers in Vision: A Survey", "authors": ["Salman Khan", "Muzammal Naseer", "Munawar Hayat", "Syed Waqas Zamir", "Fahad Shahbaz Khan", "Mubarak Shah"], "published": "2021-01-04 18:57:24", "updated": "2022-01-19 05:49:50", "summary": "Astounding results from Transformer models on natural language tasks have\nintrigued the vision community to study their application to computer vision\nproblems. Among their salient benefits, Transformers enable modeling long\ndependencies between input sequence elements and support parallel processing of\nsequence as compared to recurrent networks e.g., Long short-term memory (LSTM).\nDifferent from convolutional networks, Transformers require minimal inductive\nbiases for their design and are naturally suited as set-functions. Furthermore,\nthe straightforward design of Transformers allows processing multiple\nmodalities (e.g., images, videos, text and speech) using similar processing\nblocks and demonstrates excellent scalability to very large capacity networks\nand huge datasets. These strengths have led to exciting progress on a number of\nvision tasks using Transformer networks. This survey aims to provide a\ncomprehensive overview of the Transformer models in the computer vision\ndiscipline. We start with an introduction to fundamental concepts behind the\nsuccess of Transformers i.e., self-attention, large-scale pre-training, and\nbidirectional encoding. We then cover extensive applications of transformers in\nvision including popular recognition tasks (e.g., image classification, object\ndetection, action recognition, and segmentation), generative modeling,\nmulti-modal tasks (e.g., visual-question answering, visual reasoning, and\nvisual grounding), video processing (e.g., activity recognition, video\nforecasting), low-level vision (e.g., image super-resolution, image\nenhancement, and colorization) and 3D analysis (e.g., point cloud\nclassification and segmentation). We compare the respective advantages and\nlimitations of popular techniques both in terms of architectural design and\ntheir experimental value. Finally, we provide an analysis on open research\ndirections and possible future works.", "comment": "30 pages (Accepted in ACM Computing Surveys December 2021)", "links": ["http://dx.doi.org/10.1145/3505244"]}
{"entry_id": "2111.12301", "title": "Two-stage Rule-induction Visual Reasoning on RPMs with an Application to Video Prediction", "authors": ["Wentao He", "Jianfeng Ren", "Ruibin Bai", "Xudong Jiang"], "published": "2021-11-24 06:51:38", "updated": "2022-01-05 04:40:43", "summary": "Raven's Progressive Matrices (RPMs) are frequently used in evaluating human's\nvisual reasoning ability. Researchers have made considerable efforts in\ndeveloping systems to automatically solve the RPM problem, often through a\nblack-box end-to-end convolutional neural network for both visual recognition\nand logical reasoning tasks. Based on the two intrinsic natures of RPM problem,\nvisual recognition and logical reasoning, we propose a Two-stage Rule-Induction\nVisual Reasoner (TRIVR), which consists of a perception module and a reasoning\nmodule, to tackle the challenges of real-world visual recognition and\nsubsequent logical reasoning tasks, respectively. For the reasoning module, we\nfurther propose a \"2+1\" formulation that models human's thinking in solving\nRPMs and significantly reduces the model complexity. It derives a reasoning\nrule from each RPM sample, which is not feasible for existing methods. As a\nresult, the proposed reasoning module is capable of yielding a set of reasoning\nrules modeling human in solving the RPM problems. To validate the proposed\nmethod on real-world applications, an RPM-like Video Prediction (RVP) dataset\nis constructed, where visual reasoning is conducted on RPMs constructed using\nreal-world video frames. Experimental results on various RPM-like datasets\ndemonstrate that the proposed TRIVR achieves a significant and consistent\nperformance gain compared with the state-of-the-art models.", "comment": "Under review", "links": []}
{"entry_id": "2112.11691", "title": "CLEVR3D: Compositional Language and Elementary Visual Reasoning for Question Answering in 3D Real-World Scenes", "authors": ["Xu Yan", "Zhihao Yuan", "Yuhao Du", "Yinghong Liao", "Yao Guo", "Zhen Li", "Shuguang Cui"], "published": "2021-12-22 06:43:21", "updated": "2021-12-31 09:13:52", "summary": "3D scene understanding is a relatively emerging research field. In this\npaper, we introduce the Visual Question Answering task in 3D real-world scenes\n(VQA-3D), which aims to answer all possible questions given a 3D scene. To\ntackle this problem, the first VQA-3D dataset, namely CLEVR3D, is proposed,\nwhich contains 60K questions in 1,129 real-world scenes. Specifically, we\ndevelop a question engine leveraging 3D scene graph structures to generate\ndiverse reasoning questions, covering the questions of objects' attributes\n(i.e., size, color, and material) and their spatial relationships. Built upon\nthis dataset, we further design the first VQA-3D baseline model, TransVQA3D.\nThe TransVQA3D model adopts well-designed Transformer architectures to achieve\nsuperior VQA-3D performance, compared with the pure language baseline and\nprevious 3D reasoning methods directly applied to 3D scenarios. Experimental\nresults verify that taking VQA-3D as an auxiliary task can boost the\nperformance of 3D scene understanding, including scene graph analysis for the\nnode-wise classification and whole-graph recognition.", "comment": null, "links": []}
{"entry_id": "2112.15324", "title": "Deconfounded Visual Grounding", "authors": ["Jianqiang Huang", "Yu Qin", "Jiaxin Qi", "Qianru Sun", "Hanwang Zhang"], "published": "2021-12-31 07:14:59", "updated": "2021-12-31 07:14:59", "summary": "We focus on the confounding bias between language and location in the visual\ngrounding pipeline, where we find that the bias is the major visual reasoning\nbottleneck. For example, the grounding process is usually a trivial\nlanguage-location association without visual reasoning, e.g., grounding any\nlanguage query containing sheep to the nearly central regions, due to that most\nqueries about sheep have ground-truth locations at the image center. First, we\nframe the visual grounding pipeline into a causal graph, which shows the\ncausalities among image, query, target location and underlying confounder.\nThrough the causal graph, we know how to break the grounding bottleneck:\ndeconfounded visual grounding. Second, to tackle the challenge that the\nconfounder is unobserved in general, we propose a confounder-agnostic approach\ncalled: Referring Expression Deconfounder (RED), to remove the confounding\nbias. Third, we implement RED as a simple language attention, which can be\napplied in any grounding method. On popular benchmarks, RED improves various\nstate-of-the-art grounding methods by a significant margin. Code will soon be\navailable at: https://github.com/JianqiangH/Deconfounded_VG.", "comment": "AAAI 2022 Accepted", "links": []}
{"entry_id": "2112.05136", "title": "PTR: A Benchmark for Part-based Conceptual, Relational, and Physical Reasoning", "authors": ["Yining Hong", "Li Yi", "Joshua B. Tenenbaum", "Antonio Torralba", "Chuang Gan"], "published": "2021-12-09 18:59:34", "updated": "2021-12-09 18:59:34", "summary": "A critical aspect of human visual perception is the ability to parse visual\nscenes into individual objects and further into object parts, forming\npart-whole hierarchies. Such composite structures could induce a rich set of\nsemantic concepts and relations, thus playing an important role in the\ninterpretation and organization of visual signals as well as for the\ngeneralization of visual perception and reasoning. However, existing visual\nreasoning benchmarks mostly focus on objects rather than parts. Visual\nreasoning based on the full part-whole hierarchy is much more challenging than\nobject-centric reasoning due to finer-grained concepts, richer geometry\nrelations, and more complex physics. Therefore, to better serve for part-based\nconceptual, relational and physical reasoning, we introduce a new large-scale\ndiagnostic visual reasoning dataset named PTR. PTR contains around 70k RGBD\nsynthetic images with ground truth object and part level annotations regarding\nsemantic instance segmentation, color attributes, spatial and geometric\nrelationships, and certain physical properties such as stability. These images\nare paired with 700k machine-generated questions covering various types of\nreasoning types, making them a good testbed for visual reasoning models. We\nexamine several state-of-the-art visual reasoning models on this dataset and\nobserve that they still make many surprising mistakes in situations where\nhumans can easily infer the correct answer. We believe this dataset will open\nup new opportunities for part-based reasoning.", "comment": "NeurIPS 2021. Project page: http://ptr.csail.mit.edu/", "links": []}
{"entry_id": "2108.03603", "title": "Understanding the computational demands underlying visual reasoning", "authors": ["Mohit Vaishnav", "Remi Cadene", "Andrea Alamia", "Drew Linsley", "Rufin VanRullen", "Thomas Serre"], "published": "2021-08-08 10:46:53", "updated": "2021-12-09 04:57:02", "summary": "Visual understanding requires comprehending complex visual relations between\nobjects within a scene. Here, we seek to characterize the computational demands\nfor abstract visual reasoning. We do this by systematically assessing the\nability of modern deep convolutional neural networks (CNNs) to learn to solve\nthe \"Synthetic Visual Reasoning Test\" (SVRT) challenge, a collection of\ntwenty-three visual reasoning problems. Our analysis reveals a novel taxonomy\nof visual reasoning tasks, which can be primarily explained by both the type of\nrelations (same-different vs. spatial-relation judgments) and the number of\nrelations used to compose the underlying rules. Prior cognitive neuroscience\nwork suggests that attention plays a key role in humans' visual reasoning\nability. To test this hypothesis, we extended the CNNs with spatial and\nfeature-based attention mechanisms. In a second series of experiments, we\nevaluated the ability of these attention networks to learn to solve the SVRT\nchallenge and found the resulting architectures to be much more efficient at\nsolving the hardest of these visual reasoning tasks. Most importantly, the\ncorresponding improvements on individual tasks partially explained our novel\ntaxonomy. Overall, this work provides an granular computational account of\nvisual reasoning and yields testable neuroscience predictions regarding the\ndifferential need for feature-based vs. spatial attention depending on the type\nof visual reasoning problem.", "comment": "26 pages, 16 figures", "links": ["http://dx.doi.org/10.1162/neco_a_01485"]}
{"entry_id": "2111.14666", "title": "An in-depth experimental study of sensor usage and visual reasoning of robots navigating in real environments", "authors": ["Assem Sadek", "Guillaume Bono", "Boris Chidlovskii", "Christian Wolf"], "published": "2021-11-29 16:27:29", "updated": "2021-11-29 16:27:29", "summary": "Visual navigation by mobile robots is classically tackled through SLAM plus\noptimal planning, and more recently through end-to-end training of policies\nimplemented as deep networks. While the former are often limited to waypoint\nplanning, but have proven their efficiency even on real physical environments,\nthe latter solutions are most frequently employed in simulation, but have been\nshown to be able learn more complex visual reasoning, involving complex\nsemantical regularities. Navigation by real robots in physical environments is\nstill an open problem. End-to-end training approaches have been thoroughly\ntested in simulation only, with experiments involving real robots being\nrestricted to rare performance evaluations in simplified laboratory conditions.\nIn this work we present an in-depth study of the performance and reasoning\ncapacities of real physical agents, trained in simulation and deployed to two\ndifferent physical environments. Beyond benchmarking, we provide insights into\nthe generalization capabilities of different agents training in different\nconditions. We visualize sensor usage and the importance of the different types\nof signals. We show, that for the PointGoal task, an agent pre-trained on wide\nvariety of tasks and fine-tuned on a simulated version of the target\nenvironment can reach competitive performance without modelling any sim2real\ntransfer, i.e. by deploying the trained agent directly from simulation to a\nreal physical robot.", "comment": null, "links": []}
{"entry_id": "2111.14576", "title": "Recurrent Vision Transformer for Solving Visual Reasoning Problems", "authors": ["Nicola Messina", "Giuseppe Amato", "Fabio Carrara", "Claudio Gennaro", "Fabrizio Falchi"], "published": "2021-11-29 15:01:09", "updated": "2021-11-29 15:01:09", "summary": "Although convolutional neural networks (CNNs) showed remarkable results in\nmany vision tasks, they are still strained by simple yet challenging visual\nreasoning problems. Inspired by the recent success of the Transformer network\nin computer vision, in this paper, we introduce the Recurrent Vision\nTransformer (RViT) model. Thanks to the impact of recurrent connections and\nspatial attention in reasoning tasks, this network achieves competitive results\non the same-different visual reasoning problems from the SVRT dataset. The\nweight-sharing both in spatial and depth dimensions regularizes the model,\nallowing it to learn using far fewer free parameters, using only 28k training\nsamples. A comprehensive ablation study confirms the importance of a hybrid CNN\n+ Transformer architecture and the role of the feedback connections, which\niteratively refine the internal representation until a stable prediction is\nobtained. In the end, this study can lay the basis for a deeper understanding\nof the role of attention and recurrent connections for solving visual abstract\nreasoning tasks.", "comment": null, "links": []}
{"entry_id": "2006.00753", "title": "Structured Multimodal Attentions for TextVQA", "authors": ["Chenyu Gao", "Qi Zhu", "Peng Wang", "Hui Li", "Yuliang Liu", "Anton van den Hengel", "Qi Wu"], "published": "2020-06-01 07:07:36", "updated": "2021-11-26 03:00:58", "summary": "In this paper, we propose an end-to-end structured multimodal attention (SMA)\nneural network to mainly solve the first two issues above. SMA first uses a\nstructural graph representation to encode the object-object, object-text and\ntext-text relationships appearing in the image, and then designs a multimodal\ngraph attention network to reason over it. Finally, the outputs from the above\nmodules are processed by a global-local attentional answering module to produce\nan answer splicing together tokens from both OCR and general vocabulary\niteratively by following M4C. Our proposed model outperforms the SoTA models on\nTextVQA dataset and two tasks of ST-VQA dataset among all models except\npre-training based TAP. Demonstrating strong reasoning ability, it also won\nfirst place in TextVQA Challenge 2020. We extensively test different OCR\nmethods on several reasoning models and investigate the impact of gradually\nincreased OCR performance on TextVQA benchmark. With better OCR results,\ndifferent models share dramatic improvement over the VQA accuracy, but our\nmodel benefits most blessed by strong textual-visual reasoning ability. To\ngrant our method an upper bound and make a fair testing base available for\nfurther works, we also provide human-annotated ground-truth OCR annotations for\nthe TextVQA dataset, which were not given in the original release. The code and\nground-truth OCR annotations for the TextVQA dataset are available at\nhttps://github.com/ChenyuGAO-CS/SMA", "comment": "winner of TextVQA Challenge 2020, Accepted by IEEE Transactions on\n  Pattern Analysis and Machine Intelligence", "links": []}
{"entry_id": "2103.05222", "title": "Data augmentation by morphological mixup for solving Raven's Progressive Matrices", "authors": ["Wentao He", "Jianfeng Ren", "Ruibin Bai"], "published": "2021-03-09 04:50:32", "updated": "2021-11-19 07:37:38", "summary": "Raven's Progressive Matrices (RPMs) are frequently used in testing human's\nvisual reasoning ability. Recent advances of RPM-like datasets and solution\nmodels partially address the challenges of visually understanding the RPM\nquestions and logically reasoning the missing answers. In view of the poor\ngeneralization performance due to insufficient samples in RPM datasets, we\npropose an effective scheme, namely Candidate Answer Morphological Mixup\n(CAM-Mix). CAM-Mix serves as a data augmentation strategy by gray-scale image\nmorphological mixup, which regularizes various solution methods and overcomes\nthe model overfitting problem. By creating new negative candidate answers\nsemantically similar to the correct answers, a more accurate decision boundary\ncould be defined. By applying the proposed data augmentation method, a\nsignificant and consistent performance improvement is achieved on various\nRPM-like datasets compared with the state-of-the-art models.", "comment": "Under review", "links": []}
{"entry_id": "2009.03979", "title": "A Distance-preserving Matrix Sketch", "authors": ["Leland Wilkinson", "Hengrui Luo"], "published": "2020-09-08 20:15:14", "updated": "2021-11-19 06:39:11", "summary": "Visualizing very large matrices involves many formidable problems. Various\npopular solutions to these problems involve sampling, clustering, projection,\nor feature selection to reduce the size and complexity of the original task. An\nimportant aspect of these methods is how to preserve relative distances between\npoints in the higher-dimensional space after reducing rows and columns to fit\nin a lower dimensional space. This aspect is important because conclusions\nbased on faulty visual reasoning can be harmful. Judging dissimilar points as\nsimilar or similar points as dissimilar on the basis of a visualization can\nlead to false conclusions. To ameliorate this bias and to make visualizations\nof very large datasets feasible, we introduce two new algorithms that\nrespectively select a subset of rows and columns of a rectangular matrix. This\nselection is designed to preserve relative distances as closely as possible. We\ncompare our matrix sketch to more traditional alternatives on a variety of\nartificial and real datasets.", "comment": "38 pages, 13 figures", "links": ["http://dx.doi.org/10.1080/10618600.2022.2050246"]}
{"entry_id": "2106.13488", "title": "Probing Inter-modality: Visual Parsing with Self-Attention for Vision-Language Pre-training", "authors": ["Hongwei Xue", "Yupan Huang", "Bei Liu", "Houwen Peng", "Jianlong Fu", "Houqiang Li", "Jiebo Luo"], "published": "2021-06-25 08:04:25", "updated": "2021-11-09 06:27:44", "summary": "Vision-Language Pre-training (VLP) aims to learn multi-modal representations\nfrom image-text pairs and serves for downstream vision-language tasks in a\nfine-tuning fashion. The dominant VLP models adopt a CNN-Transformer\narchitecture, which embeds images with a CNN, and then aligns images and text\nwith a Transformer. Visual relationship between visual contents plays an\nimportant role in image understanding and is the basic for inter-modal\nalignment learning. However, CNNs have limitations in visual relation learning\ndue to local receptive field's weakness in modeling long-range dependencies.\nThus the two objectives of learning visual relation and inter-modal alignment\nare encapsulated in the same Transformer network. Such design might restrict\nthe inter-modal alignment learning in the Transformer by ignoring the\nspecialized characteristic of each objective. To tackle this, we propose a\nfully Transformer visual embedding for VLP to better learn visual relation and\nfurther promote inter-modal alignment. Specifically, we propose a metric named\nInter-Modality Flow (IMF) to measure the interaction between vision and\nlanguage modalities (i.e., inter-modality). We also design a novel masking\noptimization mechanism named Masked Feature Regression (MFR) in Transformer to\nfurther promote the inter-modality learning. To the best of our knowledge, this\nis the first study to explore the benefit of Transformer for visual feature\nlearning in VLP. We verify our method on a wide range of vision-language tasks,\nincluding Image-Text Retrieval, Visual Question Answering (VQA), Visual\nEntailment and Visual Reasoning. Our approach not only outperforms the\nstate-of-the-art VLP performance, but also shows benefits on the IMF metric.", "comment": "Accepted by NeurIPS 2021", "links": []}
{"entry_id": "2102.01916", "title": "Answer Questions with Right Image Regions: A Visual Attention Regularization Approach", "authors": ["Yibing Liu", "Yangyang Guo", "Jianhua Yin", "Xuemeng Song", "Weifeng Liu", "Liqiang Nie"], "published": "2021-02-03 07:33:30", "updated": "2021-11-08 08:28:36", "summary": "Visual attention in Visual Question Answering (VQA) targets at locating the\nright image regions regarding the answer prediction, offering a powerful\ntechnique to promote multi-modal understanding. However, recent studies have\npointed out that the highlighted image regions from the visual attention are\noften irrelevant to the given question and answer, leading to model confusion\nfor correct visual reasoning. To tackle this problem, existing methods mostly\nresort to aligning the visual attention weights with human attentions.\nNevertheless, gathering such human data is laborious and expensive, making it\nburdensome to adapt well-developed models across datasets. To address this\nissue, in this paper, we devise a novel visual attention regularization\napproach, namely AttReg, for better visual grounding in VQA. Specifically,\nAttReg firstly identifies the image regions which are essential for question\nanswering yet unexpectedly ignored (i.e., assigned with low attention weights)\nby the backbone model. And then a mask-guided learning scheme is leveraged to\nregularize the visual attention to focus more on these ignored key regions. The\nproposed method is very flexible and model-agnostic, which can be integrated\ninto most visual attention-based VQA models and require no human attention\nsupervision. Extensive experiments over three benchmark datasets, i.e., VQA-CP\nv2, VQA-CP v1, and VQA v2, have been conducted to evaluate the effectiveness of\nAttReg. As a by-product, when incorporating AttReg into the strong baseline\nLMH, our approach can achieve a new state-of-the-art accuracy of 60.00% with an\nabsolute performance gain of 7.01% on the VQA-CP v2 benchmark dataset...", "comment": "ACM TOMM 2021", "links": []}
{"entry_id": "2110.15358", "title": "Dynamic Visual Reasoning by Learning Differentiable Physics Models from Video and Language", "authors": ["Mingyu Ding", "Zhenfang Chen", "Tao Du", "Ping Luo", "Joshua B. Tenenbaum", "Chuang Gan"], "published": "2021-10-28 17:59:13", "updated": "2021-10-28 17:59:13", "summary": "In this work, we propose a unified framework, called Visual Reasoning with\nDiffer-entiable Physics (VRDP), that can jointly learn visual concepts and\ninfer physics models of objects and their interactions from videos and\nlanguage. This is achieved by seamlessly integrating three components: a visual\nperception module, a concept learner, and a differentiable physics engine. The\nvisual perception module parses each video frame into object-centric\ntrajectories and represents them as latent scene representations. The concept\nlearner grounds visual concepts (e.g., color, shape, and material) from these\nobject-centric representations based on the language, thus providing prior\nknowledge for the physics engine. The differentiable physics model, implemented\nas an impulse-based differentiable rigid-body simulator, performs\ndifferentiable physical simulation based on the grounded concepts to infer\nphysical properties, such as mass, restitution, and velocity, by fitting the\nsimulated trajectories into the video observations. Consequently, these learned\nconcepts and physical models can explain what we have seen and imagine what is\nabout to happen in future and counterfactual scenarios. Integrating\ndifferentiable physics into the dynamic reasoning framework offers several\nappealing benefits. More accurate dynamics prediction in learned physics models\nenables state-of-the-art performance on both synthetic and real-world\nbenchmarks while still maintaining high transparency and interpretability; most\nnotably, VRDP improves the accuracy of predictive and counterfactual questions\nby 4.5% and 11.5% compared to its best counterpart. VRDP is also highly\ndata-efficient: physical parameters can be optimized from very few videos, and\neven a single video can be sufficient. Finally, with all physical parameters\ninferred, VRDP can quickly learn new concepts from a few examples.", "comment": "NeurIPS 2021. Project page: http://vrdp.csail.mit.edu/", "links": []}
{"entry_id": "2012.08508", "title": "Attention over learned object embeddings enables complex visual reasoning", "authors": ["David Ding", "Felix Hill", "Adam Santoro", "Malcolm Reynolds", "Matt Botvinick"], "published": "2020-12-15 18:57:40", "updated": "2021-10-26 15:55:56", "summary": "Neural networks have achieved success in a wide array of perceptual tasks but\noften fail at tasks involving both perception and higher-level reasoning. On\nthese more challenging tasks, bespoke approaches (such as modular symbolic\ncomponents, independent dynamics models or semantic parsers) targeted towards\nthat specific type of task have typically performed better. The downside to\nthese targeted approaches, however, is that they can be more brittle than\ngeneral-purpose neural networks, requiring significant modification or even\nredesign according to the particular task at hand. Here, we propose a more\ngeneral neural-network-based approach to dynamic visual reasoning problems that\nobtains state-of-the-art performance on three different domains, in each case\noutperforming bespoke modular approaches tailored specifically to the task. Our\nmethod relies on learned object-centric representations, self-attention and\nself-supervised dynamics learning, and all three elements together are required\nfor strong performance to emerge. The success of this combination suggests that\nthere may be no need to trade off flexibility for performance on problems\ninvolving spatio-temporal or causal-style reasoning. With the right soft biases\nand learning objectives in a neural network we may be able to attain the best\nof both worlds.", "comment": "22 pages, 5 figures", "links": []}
{"entry_id": "2110.11536", "title": "Neural-guided, Bidirectional Program Search for Abstraction and Reasoning", "authors": ["Simon Alford", "Anshula Gandhi", "Akshay Rangamani", "Andrzej Banburski", "Tony Wang", "Sylee Dandekar", "John Chin", "Tomaso Poggio", "Peter Chin"], "published": "2021-10-22 00:41:47", "updated": "2021-10-26 15:26:31", "summary": "One of the challenges facing artificial intelligence research today is\ndesigning systems capable of utilizing systematic reasoning to generalize to\nnew tasks. The Abstraction and Reasoning Corpus (ARC) measures such a\ncapability through a set of visual reasoning tasks. In this paper we report\nincremental progress on ARC and lay the foundations for two approaches to\nabstraction and reasoning not based in brute-force search. We first apply an\nexisting program synthesis system called DreamCoder to create symbolic\nabstractions out of tasks solved so far, and show how it enables solving of\nprogressively more challenging ARC tasks. Second, we design a reasoning\nalgorithm motivated by the way humans approach ARC. Our algorithm constructs a\nsearch graph and reasons over this graph structure to discover task solutions.\nMore specifically, we extend existing execution-guided program synthesis\napproaches with deductive reasoning based on function inverse semantics to\nenable a neural-guided bidirectional search algorithm. We demonstrate the\neffectiveness of the algorithm on three domains: ARC, 24-Game tasks, and a\n'double-and-add' arithmetic puzzle.", "comment": "Published as a conference paper at Complex Networks 2021", "links": []}
{"entry_id": "2110.00804", "title": "ProTo: Program-Guided Transformer for Program-Guided Tasks", "authors": ["Zelin Zhao", "Karan Samel", "Binghong Chen", "Le Song"], "published": "2021-10-02 13:46:32", "updated": "2021-10-16 02:14:06", "summary": "Programs, consisting of semantic and structural information, play an\nimportant role in the communication between humans and agents. Towards learning\ngeneral program executors to unify perception, reasoning, and decision making,\nwe formulate program-guided tasks which require learning to execute a given\nprogram on the observed task specification. Furthermore, we propose the\nProgram-guided Transformer (ProTo), which integrates both semantic and\nstructural guidance of a program by leveraging cross-attention and masked\nself-attention to pass messages between the specification and routines in the\nprogram. ProTo executes a program in a learned latent space and enjoys stronger\nrepresentation ability than previous neural-symbolic approaches. We demonstrate\nthat ProTo significantly outperforms the previous state-of-the-art methods on\nGQA visual reasoning and 2D Minecraft policy learning datasets. Additionally,\nProTo demonstrates better generalization to unseen, complex, and human-written\nprograms.", "comment": "Accepted in NeurIPS 2021", "links": []}
{"entry_id": "2012.04932", "title": "Semantically Robust Unpaired Image Translation for Data with Unmatched Semantics Statistics", "authors": ["Zhiwei Jia", "Bodi Yuan", "Kangkang Wang", "Hong Wu", "David Clifford", "Zhiqiang Yuan", "Hao Su"], "published": "2020-12-09 09:28:53", "updated": "2021-10-06 05:27:10", "summary": "Many applications of unpaired image-to-image translation require the input\ncontents to be preserved semantically during translations. Unaware of the\ninherently unmatched semantics distributions between source and target domains,\nexisting distribution matching methods (i.e., GAN-based) can give undesired\nsolutions. In particular, although producing visually reasonable outputs, the\nlearned models usually flip the semantics of the inputs. To tackle this without\nusing extra supervision, we propose to enforce the translated outputs to be\nsemantically invariant w.r.t. small perceptual variations of the inputs, a\nproperty we call \"semantic robustness\". By optimizing a robustness loss w.r.t.\nmulti-scale feature space perturbations of the inputs, our method effectively\nreduces semantics flipping and produces translations that outperform existing\nmethods both quantitatively and qualitatively.", "comment": "Accepted to ICCV 2021", "links": []}
{"entry_id": "2109.01934", "title": "Weakly Supervised Relative Spatial Reasoning for Visual Question Answering", "authors": ["Pratyay Banerjee", "Tejas Gokhale", "Yezhou Yang", "Chitta Baral"], "published": "2021-09-04 21:29:06", "updated": "2021-09-04 21:29:06", "summary": "Vision-and-language (V\\&L) reasoning necessitates perception of visual\nconcepts such as objects and actions, understanding semantics and language\ngrounding, and reasoning about the interplay between the two modalities. One\ncrucial aspect of visual reasoning is spatial understanding, which involves\nunderstanding relative locations of objects, i.e.\\ implicitly learning the\ngeometry of the scene. In this work, we evaluate the faithfulness of V\\&L\nmodels to such geometric understanding, by formulating the prediction of\npair-wise relative locations of objects as a classification as well as a\nregression task. Our findings suggest that state-of-the-art transformer-based\nV\\&L models lack sufficient abilities to excel at this task. Motivated by this,\nwe design two objectives as proxies for 3D spatial reasoning (SR) -- object\ncentroid estimation, and relative position estimation, and train V\\&L with weak\nsupervision from off-the-shelf depth estimators. This leads to considerable\nimprovements in accuracy for the \"GQA\" visual question answering challenge (in\nfully supervised, few-shot, and O.O.D settings) as well as improvements in\nrelative spatial reasoning. Code and data will be released\n\\href{https://github.com/pratyay-banerjee/weak_sup_vqa}{here}.", "comment": "Accepted to ICCV 2021. PaperId : ICCV2021-10857 Copyright transferred\n  to IEEE ICCV. DOI will be updated later", "links": []}
{"entry_id": "2106.08503", "title": "Understanding and Evaluating Racial Biases in Image Captioning", "authors": ["Dora Zhao", "Angelina Wang", "Olga Russakovsky"], "published": "2021-06-16 01:07:24", "updated": "2021-08-30 15:07:38", "summary": "Image captioning is an important task for benchmarking visual reasoning and\nfor enabling accessibility for people with vision impairments. However, as in\nmany machine learning settings, social biases can influence image captioning in\nundesirable ways. In this work, we study bias propagation pathways within image\ncaptioning, focusing specifically on the COCO dataset. Prior work has analyzed\ngender bias in captions using automatically-derived gender labels; here we\nexamine racial and intersectional biases using manual annotations. Our first\ncontribution is in annotating the perceived gender and skin color of 28,315 of\nthe depicted people after obtaining IRB approval. Using these annotations, we\ncompare racial biases present in both manual and automatically-generated image\ncaptions. We demonstrate differences in caption performance, sentiment, and\nword choice between images of lighter versus darker-skinned people. Further, we\nfind the magnitude of these differences to be greater in modern captioning\nsystems compared to older ones, thus leading to concerns that without proper\nconsideration and mitigation these differences will only become increasingly\nprevalent. Code and data is available at\nhttps://princetonvisualai.github.io/imagecaptioning-bias .", "comment": "ICCV 2021", "links": []}
{"entry_id": "2011.11603", "title": "Interpretable Visual Reasoning via Induced Symbolic Space", "authors": ["Zhonghao Wang", "Kai Wang", "Mo Yu", "Jinjun Xiong", "Wen-mei Hwu", "Mark Hasegawa-Johnson", "Humphrey Shi"], "published": "2020-11-23 18:21:49", "updated": "2021-08-24 13:55:14", "summary": "We study the problem of concept induction in visual reasoning, i.e.,\nidentifying concepts and their hierarchical relationships from question-answer\npairs associated with images; and achieve an interpretable model via working on\nthe induced symbolic concept space. To this end, we first design a new\nframework named object-centric compositional attention model (OCCAM) to perform\nthe visual reasoning task with object-level visual features. Then, we come up\nwith a method to induce concepts of objects and relations using clues from the\nattention patterns between objects' visual features and question words.\nFinally, we achieve a higher level of interpretability by imposing OCCAM on the\nobjects represented in the induced symbolic concept space. Our model design\nmakes this an easy adaption via first predicting the concepts of objects and\nrelations and then projecting the predicted concepts back to the visual feature\nspace so the compositional reasoning module can process normally. Experiments\non the CLEVR and GQA datasets demonstrate: 1) our OCCAM achieves a new state of\nthe art without human-annotated functional programs; 2) our induced concepts\nare both accurate and sufficient as OCCAM achieves an on-par performance on\nobjects represented either in visual features or in the induced symbolic\nconcept space.", "comment": "ICCV 2021", "links": []}
{"entry_id": "2108.04024", "title": "Image Retrieval on Real-life Images with Pre-trained Vision-and-Language Models", "authors": ["Zheyuan Liu", "Cristian Rodriguez-Opazo", "Damien Teney", "Stephen Gould"], "published": "2021-08-09 13:25:06", "updated": "2021-08-09 13:25:06", "summary": "We extend the task of composed image retrieval, where an input query consists\nof an image and short textual description of how to modify the image. Existing\nmethods have only been applied to non-complex images within narrow domains,\nsuch as fashion products, thereby limiting the scope of study on in-depth\nvisual reasoning in rich image and language contexts. To address this issue, we\ncollect the Compose Image Retrieval on Real-life images (CIRR) dataset, which\nconsists of over 36,000 pairs of crowd-sourced, open-domain images with\nhuman-generated modifying text. To extend current methods to the open-domain,\nwe propose CIRPLANT, a transformer based model that leverages rich pre-trained\nvision-and-language (V&L) knowledge for modifying visual features conditioned\non natural language. Retrieval is then done by nearest neighbor lookup on the\nmodified features. We demonstrate that with a relatively simple architecture,\nCIRPLANT outperforms existing methods on open-domain images, while matching\nstate-of-the-art accuracy on the existing narrow datasets, such as fashion.\nTogether with the release of CIRR, we believe this work will inspire further\nresearch on composed image retrieval.", "comment": "ICCV 2021. Dataset, code, and pre-trained models are released at\n  https://cuberick-orion.github.io/CIRR/", "links": []}
{"entry_id": "2107.02334", "title": "Effect of uncertainty visualizations on myopic loss aversion and equity premium puzzle in retirement investment decisions", "authors": ["Ryan Wesslen", "Alireza Karduni", "Douglas Markant", "Wenwen Dou"], "published": "2021-07-06 01:06:27", "updated": "2021-07-27 13:37:46", "summary": "For many households, investing for retirement is one of the most significant\ndecisions and is fraught with uncertainty. In a classic study in behavioral\neconomics, Benartzi and Thaler (1999) found evidence using bar charts that\ninvestors exhibit myopic loss aversion in retirement decisions: Investors\noverly focus on the potential for short-term losses, leading them to invest\nless in riskier assets and miss out on higher long-term returns. Recently,\nadvances in uncertainty visualizations have shown improvements in\ndecision-making under uncertainty in a variety of tasks. In this paper, we\nconduct a controlled and incentivized crowdsourced experiment replicating\nBenartzi and Thaler (1999) and extending it to measure the effect of different\nuncertainty representations on myopic loss aversion. Consistent with the\noriginal study, we find evidence of myopic loss aversion with bar charts and\nfind that participants make better investment decisions with longer evaluation\nperiods. We also find that common uncertainty representations such as interval\nplots and bar charts achieve the highest mean expected returns while other\nuncertainty visualizations lead to poorer long-term performance and strong\neffects on the equity premium. Qualitative feedback further suggests that\ndifferent uncertainty representations lead to visual reasoning heuristics that\ncan either mitigate or encourage a focus on potential short-term losses. We\ndiscuss implications of our results on using uncertainty visualizations for\nretirement decisions in practice and possible extensions for future work.", "comment": "To be published in TVCG Special Issue on the 2021 IEEE Visualization\n  Conference (VIS)", "links": []}
{"entry_id": "2106.03089", "title": "Referring Transformer: A One-step Approach to Multi-task Visual Grounding", "authors": ["Muchen Li", "Leonid Sigal"], "published": "2021-06-06 10:53:39", "updated": "2021-07-14 12:22:08", "summary": "As an important step towards visual reasoning, visual grounding (e.g., phrase\nlocalization, referring expression comprehension/segmentation) has been widely\nexplored Previous approaches to referring expression comprehension (REC) or\nsegmentation (RES) either suffer from limited performance, due to a two-stage\nsetup, or require the designing of complex task-specific one-stage\narchitectures. In this paper, we propose a simple one-stage multi-task\nframework for visual grounding tasks. Specifically, we leverage a transformer\narchitecture, where two modalities are fused in a visual-lingual encoder. In\nthe decoder, the model learns to generate contextualized lingual queries which\nare then decoded and used to directly regress the bounding box and produce a\nsegmentation mask for the corresponding referred regions. With this simple but\nhighly contextualized model, we outperform state-of-the-arts methods by a large\nmargin on both REC and RES tasks. We also show that a simple pre-training\nschedule (on an external dataset) further improves the performance. Extensive\nexperiments and ablations illustrate that our model benefits greatly from\ncontextualized information and multi-task training.", "comment": null, "links": []}
{"entry_id": "2107.05833", "title": "Enforcing Consistency in Weakly Supervised Semantic Parsing", "authors": ["Nitish Gupta", "Sameer Singh", "Matt Gardner"], "published": "2021-07-13 03:48:04", "updated": "2021-07-13 03:48:04", "summary": "The predominant challenge in weakly supervised semantic parsing is that of\nspurious programs that evaluate to correct answers for the wrong reasons. Prior\nwork uses elaborate search strategies to mitigate the prevalence of spurious\nprograms; however, they typically consider only one input at a time. In this\nwork we explore the use of consistency between the output programs for related\ninputs to reduce the impact of spurious programs. We bias the program search\n(and thus the model's training signal) towards programs that map the same\nphrase in related inputs to the same sub-parts in their respective programs.\nAdditionally, we study the importance of designing logical formalisms that\nfacilitate this kind of consAistency-based training. We find that a more\nconsistent formalism leads to improved model performance even without\nconsistency-based training. When combined together, these two insights lead to\na 10% absolute improvement over the best prior result on the Natural Language\nVisual Reasoning dataset.", "comment": "Published in ACL 2021", "links": []}
{"entry_id": "2106.11072", "title": "Techniques for Symbol Grounding with SATNet", "authors": ["Sever Topan", "David Rolnick", "Xujie Si"], "published": "2021-06-16 18:42:12", "updated": "2021-06-16 18:42:12", "summary": "Many experts argue that the future of artificial intelligence is limited by\nthe field's ability to integrate symbolic logical reasoning into deep learning\narchitectures. The recently proposed differentiable MAXSAT solver, SATNet, was\na breakthrough in its capacity to integrate with a traditional neural network\nand solve visual reasoning problems. For instance, it can learn the rules of\nSudoku purely from image examples. Despite its success, SATNet was shown to\nsuccumb to a key challenge in neurosymbolic systems known as the Symbol\nGrounding Problem: the inability to map visual inputs to symbolic variables\nwithout explicit supervision (\"label leakage\"). In this work, we present a\nself-supervised pre-training pipeline that enables SATNet to overcome this\nlimitation, thus broadening the class of problems that SATNet architectures can\nsolve to include datasets where no intermediary labels are available at all. We\ndemonstrate that our method allows SATNet to attain full accuracy even with a\nharder problem setup that prevents any label leakage. We additionally introduce\na proofreading method that further improves the performance of SATNet\narchitectures, beating the state-of-the-art on Visual Sudoku.", "comment": "Code available at https://github.com/SeverTopan/SATNet", "links": []}
{"entry_id": "2105.02061", "title": "Proposal-free One-stage Referring Expression via Grid-Word Cross-Attention", "authors": ["Wei Suo", "Mengyang Sun", "Peng Wang", "Qi Wu"], "published": "2021-05-05 13:53:53", "updated": "2021-05-05 13:53:53", "summary": "Referring Expression Comprehension (REC) has become one of the most important\ntasks in visual reasoning, since it is an essential step for many\nvision-and-language tasks such as visual question answering. However, it has\nnot been widely used in many downstream tasks because it suffers 1) two-stage\nmethods exist heavy computation cost and inevitable error accumulation, and 2)\none-stage methods have to depend on lots of hyper-parameters (such as anchors)\nto generate bounding box. In this paper, we present a proposal-free one-stage\n(PFOS) model that is able to regress the region-of-interest from the image,\nbased on a textual query, in an end-to-end manner. Instead of using the\ndominant anchor proposal fashion, we directly take the dense-grid of an image\nas input for a cross-attention transformer that learns grid-word\ncorrespondences. The final bounding box is predicted directly from the image\nwithout the time-consuming anchor selection process that previous methods\nsuffer. Our model achieves the state-of-the-art performance on four referring\nexpression datasets with higher efficiency, comparing to previous best\none-stage and two-stage methods.", "comment": "To be published in the 30th International Joint Conference on\n  Artificial Intelligence (IJCAI-2021)", "links": []}
{"entry_id": "2104.14741", "title": "Chop Chop BERT: Visual Question Answering by Chopping VisualBERT's Heads", "authors": ["Chenyu Gao", "Qi Zhu", "Peng Wang", "Qi Wu"], "published": "2021-04-30 03:32:02", "updated": "2021-04-30 03:32:02", "summary": "Vision-and-Language (VL) pre-training has shown great potential on many\nrelated downstream tasks, such as Visual Question Answering (VQA), one of the\nmost popular problems in the VL field. All of these pre-trained models (such as\nVisualBERT, ViLBERT, LXMERT and UNITER) are built with Transformer, which\nextends the classical attention mechanism to multiple layers and heads. To\ninvestigate why and how these models work on VQA so well, in this paper we\nexplore the roles of individual heads and layers in Transformer models when\nhandling $12$ different types of questions. Specifically, we manually remove\n(chop) heads (or layers) from a pre-trained VisualBERT model at a time, and\ntest it on different levels of questions to record its performance. As shown in\nthe interesting echelon shape of the result matrices, experiments reveal\ndifferent heads and layers are responsible for different question types, with\nhigher-level layers activated by higher-level visual reasoning questions. Based\non this observation, we design a dynamic chopping module that can automatically\nremove heads and layers of the VisualBERT at an instance level when dealing\nwith different questions. Our dynamic chopping module can effectively reduce\nthe parameters of the original model by 50%, while only damaging the accuracy\nby less than 1% on the VQA task.", "comment": "14 pages", "links": []}
{"entry_id": "2104.14102", "title": "Comparing Visual Reasoning in Humans and AI", "authors": ["Shravan Murlidaran", "William Yang Wang", "Miguel P. Eckstein"], "published": "2021-04-29 04:44:13", "updated": "2021-04-29 04:44:13", "summary": "Recent advances in natural language processing and computer vision have led\nto AI models that interpret simple scenes at human levels. Yet, we do not have\na complete understanding of how humans and AI models differ in their\ninterpretation of more complex scenes. We created a dataset of complex scenes\nthat contained human behaviors and social interactions. AI and humans had to\ndescribe the scenes with a sentence. We used a quantitative metric of\nsimilarity between scene descriptions of the AI/human and ground truth of five\nother human descriptions of each scene. Results show that the machine/human\nagreement scene descriptions are much lower than human/human agreement for our\ncomplex scenes. Using an experimental manipulation that occludes different\nspatial regions of the scenes, we assessed how machines and humans vary in\nutilizing regions of images to understand the scenes. Together, our results are\na first step toward understanding how machines fall short of human visual\nreasoning with complex scenes depicting human behaviors.", "comment": null, "links": []}
{"entry_id": "2004.09406", "title": "Five Points to Check when Comparing Visual Perception in Humans and Machines", "authors": ["Christina M. Funke", "Judy Borowski", "Karolina Stosio", "Wieland Brendel", "Thomas S. A. Wallis", "Matthias Bethge"], "published": "2020-04-20 16:05:36", "updated": "2021-04-13 16:03:20", "summary": "With the rise of machines to human-level performance in complex recognition\ntasks, a growing amount of work is directed towards comparing information\nprocessing in humans and machines. These studies are an exciting chance to\nlearn about one system by studying the other. Here, we propose ideas on how to\ndesign, conduct and interpret experiments such that they adequately support the\ninvestigation of mechanisms when comparing human and machine perception. We\ndemonstrate and apply these ideas through three case studies. The first case\nstudy shows how human bias can affect how we interpret results, and that\nseveral analytic tools can help to overcome this human reference point. In the\nsecond case study, we highlight the difference between necessary and sufficient\nmechanisms in visual reasoning tasks. Thereby, we show that contrary to\nprevious suggestions, feedback mechanisms might not be necessary for the tasks\nin question. The third case study highlights the importance of aligning\nexperimental conditions. We find that a previously-observed difference in\nobject recognition does not hold when adapting the experiment to make\nconditions more equitable between humans and machines. In presenting a\nchecklist for comparative studies of visual reasoning in humans and machines,\nwe hope to highlight how to overcome potential pitfalls in design or inference.", "comment": "V3: minor changes like in published JOV version\n  (https://doi.org/10.1167/jov.21.3.16) V2: New title; added general section\n  (checklist); manuscript restructured such that each case study is one\n  chapter; adversarial examples in first study replaced by different analysis", "links": ["http://dx.doi.org/10.1167/jov.21.3.16"]}
{"entry_id": "2011.13160", "title": "Transformation Driven Visual Reasoning", "authors": ["Xin Hong", "Yanyan Lan", "Liang Pang", "Jiafeng Guo", "Xueqi Cheng"], "published": "2020-11-26 07:11:31", "updated": "2021-04-02 06:25:46", "summary": "This paper defines a new visual reasoning paradigm by introducing an\nimportant factor, i.e.~transformation. The motivation comes from the fact that\nmost existing visual reasoning tasks, such as CLEVR in VQA, are solely defined\nto test how well the machine understands the concepts and relations within\nstatic settings, like one image. We argue that this kind of \\textbf{state\ndriven visual reasoning} approach has limitations in reflecting whether the\nmachine has the ability to infer the dynamics between different states, which\nhas been shown as important as state-level reasoning for human cognition in\nPiaget's theory. To tackle this problem, we propose a novel\n\\textbf{transformation driven visual reasoning} task. Given both the initial\nand final states, the target is to infer the corresponding single-step or\nmulti-step transformation, represented as a triplet (object, attribute, value)\nor a sequence of triplets, respectively. Following this definition, a new\ndataset namely TRANCE is constructed on the basis of CLEVR, including three\nlevels of settings, i.e.~Basic (single-step transformation), Event (multi-step\ntransformation), and View (multi-step transformation with variant views).\nExperimental results show that the state-of-the-art visual reasoning models\nperform well on Basic, but are still far from human-level intelligence on Event\nand View. We believe the proposed new paradigm will boost the development of\nmachine visual reasoning. More advanced methods and real data need to be\ninvestigated in this direction. The resource of TVR is available at\nhttps://hongxin2019.github.io/TVR.", "comment": "Accepted to CVPR 2021. Resources including the TRANCE dataset and the\n  code can be found at our homepage https://hongxin2019.github.io/TVR", "links": []}
{"entry_id": "2103.16564", "title": "Grounding Physical Concepts of Objects and Events Through Dynamic Visual Reasoning", "authors": ["Zhenfang Chen", "Jiayuan Mao", "Jiajun Wu", "Kwan-Yee Kenneth Wong", "Joshua B. Tenenbaum", "Chuang Gan"], "published": "2021-03-30 17:59:48", "updated": "2021-03-30 17:59:48", "summary": "We study the problem of dynamic visual reasoning on raw videos. This is a\nchallenging problem; currently, state-of-the-art models often require dense\nsupervision on physical object properties and events from simulation, which are\nimpractical to obtain in real life. In this paper, we present the Dynamic\nConcept Learner (DCL), a unified framework that grounds physical objects and\nevents from video and language. DCL first adopts a trajectory extractor to\ntrack each object over time and to represent it as a latent, object-centric\nfeature vector. Building upon this object-centric representation, DCL learns to\napproximate the dynamic interaction among objects using graph networks. DCL\nfurther incorporates a semantic parser to parse questions into semantic\nprograms and, finally, a program executor to run the program to answer the\nquestion, levering the learned dynamics model. After training, DCL can detect\nand associate objects across the frames, ground visual properties, and physical\nevents, understand the causal relationship between events, make future and\ncounterfactual predictions, and leverage these extracted presentations for\nanswering queries. DCL achieves state-of-the-art performance on CLEVRER, a\nchallenging causal video reasoning dataset, even without using ground-truth\nattributes and collision labels from simulations for training. We further test\nDCL on a newly proposed video-retrieval and event localization dataset derived\nfrom CLEVRER, showing its strong generalization capacity.", "comment": "ICLR 2021. Project page: http://dcl.csail.mit.edu/", "links": []}
{"entry_id": "2103.16002", "title": "AGQA: A Benchmark for Compositional Spatio-Temporal Reasoning", "authors": ["Madeleine Grunde-McLaughlin", "Ranjay Krishna", "Maneesh Agrawala"], "published": "2021-03-30 00:24:01", "updated": "2021-03-30 00:24:01", "summary": "Visual events are a composition of temporal actions involving actors\nspatially interacting with objects. When developing computer vision models that\ncan reason about compositional spatio-temporal events, we need benchmarks that\ncan analyze progress and uncover shortcomings. Existing video question\nanswering benchmarks are useful, but they often conflate multiple sources of\nerror into one accuracy metric and have strong biases that models can exploit,\nmaking it difficult to pinpoint model weaknesses. We present Action Genome\nQuestion Answering (AGQA), a new benchmark for compositional spatio-temporal\nreasoning. AGQA contains $192M$ unbalanced question answer pairs for $9.6K$\nvideos. We also provide a balanced subset of $3.9M$ question answer pairs, $3$\norders of magnitude larger than existing benchmarks, that minimizes bias by\nbalancing the answer distributions and types of question structures. Although\nhuman evaluators marked $86.02\\%$ of our question-answer pairs as correct, the\nbest model achieves only $47.74\\%$ accuracy. In addition, AGQA introduces\nmultiple training/test splits to test for various reasoning abilities,\nincluding generalization to novel compositions, to indirect references, and to\nmore compositional steps. Using AGQA, we evaluate modern visual reasoning\nsystems, demonstrating that the best models barely perform better than\nnon-visual baselines exploiting linguistic biases and that none of the existing\nmodels generalize to novel compositions unseen during training.", "comment": "8 pages, 15 pages supplementary, 12 figures. To be published in CVPR\n  2021", "links": []}
{"entry_id": "2103.14232", "title": "ACRE: Abstract Causal REasoning Beyond Covariation", "authors": ["Chi Zhang", "Baoxiong Jia", "Mark Edmonds", "Song-Chun Zhu", "Yixin Zhu"], "published": "2021-03-26 02:42:38", "updated": "2021-03-26 02:42:38", "summary": "Causal induction, i.e., identifying unobservable mechanisms that lead to the\nobservable relations among variables, has played a pivotal role in modern\nscientific discovery, especially in scenarios with only sparse and limited\ndata. Humans, even young toddlers, can induce causal relationships surprisingly\nwell in various settings despite its notorious difficulty. However, in contrast\nto the commonplace trait of human cognition is the lack of a diagnostic\nbenchmark to measure causal induction for modern Artificial Intelligence (AI)\nsystems. Therefore, in this work, we introduce the Abstract Causal REasoning\n(ACRE) dataset for systematic evaluation of current vision systems in causal\ninduction. Motivated by the stream of research on causal discovery in Blicket\nexperiments, we query a visual reasoning system with the following four types\nof questions in either an independent scenario or an interventional scenario:\ndirect, indirect, screening-off, and backward-blocking, intentionally going\nbeyond the simple strategy of inducing causal relationships by covariation. By\nanalyzing visual reasoning architectures on this testbed, we notice that pure\nneural models tend towards an associative strategy under their chance-level\nperformance, whereas neuro-symbolic combinations struggle in backward-blocking\nreasoning. These deficiencies call for future research in models with a more\ncomprehensive capability of causal induction.", "comment": "CVPR 2021 paper. Supplementary:\n  http://wellyzhang.github.io/attach/cvpr21zhang_acre_supp.pdf Project:\n  http://wellyzhang.github.io/project/acre.html", "links": []}
{"entry_id": "2103.12045", "title": "Raven's Progressive Matrices Completion with Latent Gaussian Process Priors", "authors": ["Fan Shi", "Bin Li", "Xiangyang Xue"], "published": "2021-03-22 17:48:44", "updated": "2021-03-22 17:48:44", "summary": "Abstract reasoning ability is fundamental to human intelligence. It enables\nhumans to uncover relations among abstract concepts and further deduce implicit\nrules from the relations. As a well-known abstract visual reasoning task,\nRaven's Progressive Matrices (RPM) are widely used in human IQ tests. Although\nextensive research has been conducted on RPM solvers with machine intelligence,\nfew studies have considered further advancing the standard answer-selection\n(classification) problem to a more challenging answer-painting (generating)\nproblem, which can verify whether the model has indeed understood the implicit\nrules. In this paper we aim to solve the latter one by proposing a deep latent\nvariable model, in which multiple Gaussian processes are employed as priors of\nlatent variables to separately learn underlying abstract concepts from RPMs;\nthus the proposed model is interpretable in terms of concept-specific latent\nvariables. The latent Gaussian process also provides an effective way of\nextrapolation for answer painting based on the learned concept-changing rules.\nWe evaluate the proposed model on RPM-like datasets with multiple\ncontinuously-changing visual concepts. Experimental results demonstrate that\nour model requires only few training samples to paint high-quality answers,\ngenerate novel RPM panels, and achieve interpretability through\nconcept-specific latent variables.", "comment": null, "links": []}
{"entry_id": "2004.14603", "title": "Dynamic Language Binding in Relational Visual Reasoning", "authors": ["Thao Minh Le", "Vuong Le", "Svetha Venkatesh", "Truyen Tran"], "published": "2020-04-30 06:26:20", "updated": "2021-02-18 03:35:24", "summary": "We present Language-binding Object Graph Network, the first neural reasoning\nmethod with dynamic relational structures across both visual and textual\ndomains with applications in visual question answering. Relaxing the common\nassumption made by current models that the object predicates pre-exist and stay\nstatic, passive to the reasoning process, we propose that these dynamic\npredicates expand across the domain borders to include pair-wise\nvisual-linguistic object binding. In our method, these contextualized object\nlinks are actively found within each recurrent reasoning step without relying\non external predicative priors. These dynamic structures reflect the\nconditional dual-domain object dependency given the evolving context of the\nreasoning through co-attention. Such discovered dynamic graphs facilitate\nmulti-step knowledge combination and refinements that iteratively deduce the\ncompact representation of the final answer. The effectiveness of this model is\ndemonstrated on image question answering demonstrating favorable performance on\nmajor VQA datasets. Our method outperforms other methods in sophisticated\nquestion-answering tasks wherein multiple object relations are involved. The\ngraph structure effectively assists the progress of training, and therefore the\nnetwork learns efficiently compared to other reasoning models.", "comment": "Early version accepted by IJCAI20, Code available at\n  https://github.com/thaolmk54/LOGNet-VQA", "links": []}
{"entry_id": "2101.06013", "title": "Reasoning over Vision and Language: Exploring the Benefits of Supplemental Knowledge", "authors": ["Violetta Shevchenko", "Damien Teney", "Anthony Dick", "Anton van den Hengel"], "published": "2021-01-15 08:37:55", "updated": "2021-01-15 08:37:55", "summary": "The limits of applicability of vision-and-language models are defined by the\ncoverage of their training data. Tasks like vision question answering (VQA)\noften require commonsense and factual information beyond what can be learned\nfrom task-specific datasets. This paper investigates the injection of knowledge\nfrom general-purpose knowledge bases (KBs) into vision-and-language\ntransformers. We use an auxiliary training objective that encourages the\nlearned representations to align with graph embeddings of matching entities in\na KB. We empirically study the relevance of various KBs to multiple tasks and\nbenchmarks. The technique brings clear benefits to knowledge-demanding question\nanswering tasks (OK-VQA, FVQA) by capturing semantic and relational knowledge\nabsent from existing models. More surprisingly, the technique also benefits\nvisual reasoning tasks (NLVR2, SNLI-VE). We perform probing experiments and\nshow that the injection of additional knowledge regularizes the space of\nembeddings, which improves the representation of lexical and semantic\nsimilarities. The technique is model-agnostic and can expand the applicability\nof any vision-and-language transformer with minimal computational overhead.", "comment": null, "links": []}
{"entry_id": "2010.00763", "title": "Bongard-LOGO: A New Benchmark for Human-Level Concept Learning and Reasoning", "authors": ["Weili Nie", "Zhiding Yu", "Lei Mao", "Ankit B. Patel", "Yuke Zhu", "Animashree Anandkumar"], "published": "2020-10-02 03:19:46", "updated": "2021-01-04 21:50:06", "summary": "Humans have an inherent ability to learn novel concepts from only a few\nsamples and generalize these concepts to different situations. Even though\ntoday's machine learning models excel with a plethora of training data on\nstandard recognition tasks, a considerable gap exists between machine-level\npattern recognition and human-level concept learning. To narrow this gap, the\nBongard problems (BPs) were introduced as an inspirational challenge for visual\ncognition in intelligent systems. Despite new advances in representation\nlearning and learning to learn, BPs remain a daunting challenge for modern AI.\nInspired by the original one hundred BPs, we propose a new benchmark\nBongard-LOGO for human-level concept learning and reasoning. We develop a\nprogram-guided generation technique to produce a large set of\nhuman-interpretable visual cognition problems in action-oriented LOGO language.\nOur benchmark captures three core properties of human cognition: 1)\ncontext-dependent perception, in which the same object may have disparate\ninterpretations given different contexts; 2) analogy-making perception, in\nwhich some meaningful concepts are traded off for other meaningful concepts;\nand 3) perception with a few samples but infinite vocabulary. In experiments,\nwe show that the state-of-the-art deep learning methods perform substantially\nworse than human subjects, implying that they fail to capture core human\ncognition properties. Finally, we discuss research directions towards a general\narchitecture for visual reasoning to tackle this benchmark.", "comment": "22 pages, NeurIPS 2020", "links": []}
{"entry_id": "2011.13406", "title": "Learning from Lexical Perturbations for Consistent Visual Question Answering", "authors": ["Spencer Whitehead", "Hui Wu", "Yi Ren Fung", "Heng Ji", "Rogerio Feris", "Kate Saenko"], "published": "2020-11-26 17:38:03", "updated": "2020-12-23 00:29:27", "summary": "Existing Visual Question Answering (VQA) models are often fragile and\nsensitive to input variations. In this paper, we propose a novel approach to\naddress this issue based on modular networks, which creates two questions\nrelated by linguistic perturbations and regularizes the visual reasoning\nprocess between them to be consistent during training. We show that our\nframework markedly improves consistency and generalization ability,\ndemonstrating the value of controlled linguistic perturbations as a useful and\ncurrently underutilized training and regularization tool for VQA models. We\nalso present VQA Perturbed Pairings (VQA P2), a new, low-cost benchmark and\naugmentation pipeline to create controllable linguistic variations of VQA\nquestions. Our benchmark uniquely draws from large-scale linguistic resources,\navoiding human annotation effort while maintaining data quality compared to\ngenerative approaches. We benchmark existing VQA models using VQA P2 and\nprovide robustness analysis on each type of linguistic variation.", "comment": "14 pages, 8 figures", "links": []}
{"entry_id": "2012.11587", "title": "Object-Centric Diagnosis of Visual Reasoning", "authors": ["Jianwei Yang", "Jiayuan Mao", "Jiajun Wu", "Devi Parikh", "David D. Cox", "Joshua B. Tenenbaum", "Chuang Gan"], "published": "2020-12-21 18:59:28", "updated": "2020-12-21 18:59:28", "summary": "When answering questions about an image, it not only needs knowing what --\nunderstanding the fine-grained contents (e.g., objects, relationships) in the\nimage, but also telling why -- reasoning over grounding visual cues to derive\nthe answer for a question. Over the last few years, we have seen significant\nprogress on visual question answering. Though impressive as the accuracy grows,\nit still lags behind to get knowing whether these models are undertaking\ngrounding visual reasoning or just leveraging spurious correlations in the\ntraining data. Recently, a number of works have attempted to answer this\nquestion from perspectives such as grounding and robustness. However, most of\nthem are either focusing on the language side or coarsely studying the\npixel-level attention maps. In this paper, by leveraging the step-wise object\ngrounding annotations provided in the GQA dataset, we first present a\nsystematical object-centric diagnosis of visual reasoning on grounding and\nrobustness, particularly on the vision side. According to the extensive\ncomparisons across different models, we find that even models with high\naccuracy are not good at grounding objects precisely, nor robust to visual\ncontent perturbations. In contrast, symbolic and modular models have a\nrelatively better grounding and robustness, though at the cost of accuracy. To\nreconcile these different aspects, we further develop a diagnostic model,\nnamely Graph Reasoning Machine. Our model replaces purely symbolic visual\nrepresentation with probabilistic scene graph and then applies teacher-forcing\ntraining for the visual reasoning module. The designed model improves the\nperformance on all three metrics over the vanilla neural-symbolic model while\ninheriting the transparency. Further ablation studies suggest that this\nimprovement is mainly due to more accurate image understanding and proper\nintermediate reasoning supervisions.", "comment": null, "links": []}
{"entry_id": "2012.07966", "title": "Odd-One-Out Representation Learning", "authors": ["Salman Mohammadi", "Anders Kirk Uhrenholt", "Bjørn Sand Jensen"], "published": "2020-12-14 22:01:15", "updated": "2020-12-14 22:01:15", "summary": "The effective application of representation learning to real-world problems\nrequires both techniques for learning useful representations, and also robust\nways to evaluate properties of representations. Recent work in disentangled\nrepresentation learning has shown that unsupervised representation learning\napproaches rely on fully supervised disentanglement metrics, which assume\naccess to labels for ground-truth factors of variation. In many real-world\ncases ground-truth factors are expensive to collect, or difficult to model,\nsuch as for perception. Here we empirically show that a weakly-supervised\ndownstream task based on odd-one-out observations is suitable for model\nselection by observing high correlation on a difficult downstream abstract\nvisual reasoning task. We also show that a bespoke metric-learning VAE model\nwhich performs highly on this task also out-performs other standard\nunsupervised and a weakly-supervised disentanglement model across several\nmetrics.", "comment": null, "links": []}
{"entry_id": "2012.01944", "title": "Multi-Label Contrastive Learning for Abstract Visual Reasoning", "authors": ["Mikołaj Małkiński", "Jacek Mańdziuk"], "published": "2020-12-03 14:18:15", "updated": "2020-12-03 14:18:15", "summary": "For a long time the ability to solve abstract reasoning tasks was considered\none of the hallmarks of human intelligence. Recent advances in application of\ndeep learning (DL) methods led, as in many other domains, to surpassing human\nabstract reasoning performance, specifically in the most popular type of such\nproblems - the Raven's Progressive Matrices (RPMs). While the efficacy of DL\nsystems is indeed impressive, the way they approach the RPMs is very different\nfrom that of humans. State-of-the-art systems solving RPMs rely on massive\npattern-based training and sometimes on exploiting biases in the dataset,\nwhereas humans concentrate on identification of the rules / concepts underlying\nthe RPM (or generally a visual reasoning task) to be solved. Motivated by this\ncognitive difference, this work aims at combining DL with human way of solving\nRPMs and getting the best of both worlds. Specifically, we cast the problem of\nsolving RPMs into multi-label classification framework where each RPM is viewed\nas a multi-label data point, with labels determined by the set of abstract\nrules underlying the RPM. For efficient training of the system we introduce a\ngeneralisation of the Noise Contrastive Estimation algorithm to the case of\nmulti-label samples. Furthermore, we propose a new sparse rule encoding scheme\nfor RPMs which, besides the new training algorithm, is the key factor\ncontributing to the state-of-the-art performance. The proposed approach is\nevaluated on two most popular benchmark datasets (Balanced-RAVEN and PGM) and\non both of them demonstrates an advantage over the current state-of-the-art\nresults. Contrary to applications of contrastive learning methods reported in\nother domains, the state-of-the-art performance reported in the paper is\nachieved with no need for large batch sizes or strong data augmentation.", "comment": null, "links": ["http://dx.doi.org/10.1109/TNNLS.2022.3185949"]}
{"entry_id": "1910.03230", "title": "Meta Module Network for Compositional Visual Reasoning", "authors": ["Wenhu Chen", "Zhe Gan", "Linjie Li", "Yu Cheng", "William Wang", "Jingjing Liu"], "published": "2019-10-08 06:28:24", "updated": "2020-11-08 02:52:51", "summary": "Neural Module Network (NMN) exhibits strong interpretability and\ncompositionality thanks to its handcrafted neural modules with explicit\nmulti-hop reasoning capability. However, most NMNs suffer from two critical\ndrawbacks: 1) scalability: customized module for specific function renders it\nimpractical when scaling up to a larger set of functions in complex tasks; 2)\ngeneralizability: rigid pre-defined module inventory makes it difficult to\ngeneralize to unseen functions in new tasks/domains. To design a more powerful\nNMN architecture for practical use, we propose Meta Module Network (MMN)\ncentered on a novel meta module, which can take in function recipes and morph\ninto diverse instance modules dynamically. The instance modules are then woven\ninto an execution graph for complex visual reasoning, inheriting the strong\nexplainability and compositionality of NMN. With such a flexible instantiation\nmechanism, the parameters of instance modules are inherited from the central\nmeta module, retaining the same model complexity as the function set grows,\nwhich promises better scalability. Meanwhile, as functions are encoded into the\nembedding space, unseen functions can be readily represented based on its\nstructural similarity with previously observed ones, which ensures better\ngeneralizability. Experiments on GQA and CLEVR datasets validate the\nsuperiority of MMN over state-of-the-art NMN designs. Synthetic experiments on\nheld-out unseen functions from GQA dataset also demonstrate the strong\ngeneralizability of MMN. Our code and model are released in Github\nhttps://github.com/wenhuchen/Meta-Module-Network.", "comment": "Accepted to WACV 21 (Oral)", "links": []}
{"entry_id": "2010.07526", "title": "Natural Language Rationales with Full-Stack Visual Reasoning: From Pixels to Semantic Frames to Commonsense Graphs", "authors": ["Ana Marasović", "Chandra Bhagavatula", "Jae Sung Park", "Ronan Le Bras", "Noah A. Smith", "Yejin Choi"], "published": "2020-10-15 05:08:56", "updated": "2020-10-15 05:08:56", "summary": "Natural language rationales could provide intuitive, higher-level\nexplanations that are easily understandable by humans, complementing the more\nbroadly studied lower-level explanations based on gradients or attention\nweights. We present the first study focused on generating natural language\nrationales across several complex visual reasoning tasks: visual commonsense\nreasoning, visual-textual entailment, and visual question answering. The key\nchallenge of accurate rationalization is comprehensive image understanding at\nall levels: not just their explicit content at the pixel level, but their\ncontextual contents at the semantic and pragmatic levels. We present\nRationale^VT Transformer, an integrated model that learns to generate free-text\nrationales by combining pretrained language models with object recognition,\ngrounded visual semantic frames, and visual commonsense graphs. Our experiments\nshow that the base pretrained language model benefits from visual adaptation\nand that free-text rationalization is a promising research direction to\ncomplement model interpretability for complex visual-textual reasoning tasks.", "comment": "Accepted to Findings of EMNLP", "links": []}
{"entry_id": "2010.05633", "title": "Contextual Modulation for Relation-Level Metaphor Identification", "authors": ["Omnia Zayed", "John P. McCrae", "Paul Buitelaar"], "published": "2020-10-12 12:07:02", "updated": "2020-10-12 12:07:02", "summary": "Identifying metaphors in text is very challenging and requires comprehending\nthe underlying comparison. The automation of this cognitive process has gained\nwide attention lately. However, the majority of existing approaches concentrate\non word-level identification by treating the task as either single-word\nclassification or sequential labelling without explicitly modelling the\ninteraction between the metaphor components. On the other hand, while existing\nrelation-level approaches implicitly model this interaction, they ignore the\ncontext where the metaphor occurs. In this work, we address these limitations\nby introducing a novel architecture for identifying relation-level metaphoric\nexpressions of certain grammatical relations based on contextual modulation. In\na methodology inspired by works in visual reasoning, our approach is based on\nconditioning the neural network computation on the deep contextualised features\nof the candidate expressions using feature-wise linear modulation. We\ndemonstrate that the proposed architecture achieves state-of-the-art results on\nbenchmark datasets. The proposed methodology is generic and could be applied to\nother textual classification problems that benefit from contextual interaction.", "comment": "accepted at Findings of EMNLP 2020", "links": []}
{"entry_id": "2009.09154", "title": "CLEVR Parser: A Graph Parser Library for Geometric Learning on Language Grounded Image Scenes", "authors": ["Raeid Saqur", "Ameet Deshpande"], "published": "2020-09-19 03:32:37", "updated": "2020-10-01 22:56:35", "summary": "The CLEVR dataset has been used extensively in language grounded visual\nreasoning in Machine Learning (ML) and Natural Language Processing (NLP)\ndomains. We present a graph parser library for CLEVR, that provides\nfunctionalities for object-centric attributes and relationships extraction, and\nconstruction of structural graph representations for dual modalities.\nStructural order-invariant representations enable geometric learning and can\naid in downstream tasks like language grounding to vision, robotics,\ncompositionality, interpretability, and computational grammar construction. We\nprovide three extensible main components - parser, embedder, and visualizer\nthat can be tailored to suit specific learning setups. We also provide\nout-of-the-box functionality for seamless integration with popular deep graph\nneural network (GNN) libraries. Additionally, we discuss downstream usage and\napplications of the library, and how it accelerates research for the NLP\nresearch community.", "comment": "Accepted at NLP-OSS, EMNLP 2020 (2nd Workshop for Natural Language\n  Processing Open Source Software)", "links": []}
{"entry_id": "2007.14516", "title": "Visual Reasoning Strategies for Effect Size Judgments and Decisions", "authors": ["Alex Kale", "Matthew Kay", "Jessica Hullman"], "published": "2020-07-28 22:56:32", "updated": "2020-09-12 20:21:47", "summary": "Uncertainty visualizations often emphasize point estimates to support\nmagnitude estimates or decisions through visual comparison. However, when\ndesign choices emphasize means, users may overlook uncertainty information and\nmisinterpret visual distance as a proxy for effect size. We present findings\nfrom a mixed design experiment on Mechanical Turk which tests eight uncertainty\nvisualization designs: 95% containment intervals, hypothetical outcome plots,\ndensities, and quantile dotplots, each with and without means added. We find\nthat adding means to uncertainty visualizations has small biasing effects on\nboth magnitude estimation and decision-making, consistent with discounting\nuncertainty. We also see that visualization designs that support the least\nbiased effect size estimation do not support the best decision-making,\nsuggesting that a chart user's sense of effect size may not necessarily be\nidentical when they use the same information for different tasks. In a\nqualitative analysis of users' strategy descriptions, we find that many users\nswitch strategies and do not employ an optimal strategy when one exists.\nUncertainty visualizations which are optimally designed in theory may not be\nthe most effective in practice because of the ways that users satisfice with\nheuristics, suggesting opportunities to better understand visualization\neffectiveness by modeling sets of potential strategies.", "comment": "Accepted for publication at IEEE VIS 2020", "links": []}
{"entry_id": "1904.08324", "title": "Question Guided Modular Routing Networks for Visual Question Answering", "authors": ["Yanze Wu", "Qiang Sun", "Jianqi Ma", "Bin Li", "Yanwei Fu", "Yao Peng", "Xiangyang Xue"], "published": "2019-04-17 15:45:13", "updated": "2020-09-04 17:21:28", "summary": "This paper studies the task of Visual Question Answering (VQA), which is\ntopical in Multimedia community recently. Particularly, we explore two critical\nresearch problems existed in VQA: (1) efficiently fusing the visual and textual\nmodalities; (2) enabling the visual reasoning ability of VQA models in\nanswering complex questions. To address these challenging problems, a novel\nQuestion Guided Modular Routing Networks (QGMRN) has been proposed in this\npaper. Particularly, The QGMRN is composed of visual, textual and routing\nnetwork. The visual and textual network serve as the backbones for the generic\nfeature extractors of visual and textual modalities. QGMRN can fuse the visual\nand textual modalities at multiple semantic levels. Typically, the visual\nreasoning is facilitated by the routing network in a discrete and stochastic\nway by using Gumbel-Softmax trick for module selection. When the input reaches\na certain modular layer, routing network newly proposed in this paper,\ndynamically selects a portion of modules from that layer to process the input\ndepending on the question features generated by the textual network. It can\nalso learn to reason by routing between the generic modules without additional\nsupervision information or expert knowledge. Benefiting from the dynamic\nrouting mechanism, QGMRN can outperform the previous classical VQA methods by a\nlarge margin and achieve the competitive results against the state-of-the-art\nmethods. Furthermore, attention mechanism is integrated into our QGMRN model\nand thus can further boost the model performance. Empirically, extensive\nexperiments on the CLEVR and CLEVR-Humans datasets validate the effectiveness\nof our proposed model, and the state-of-the-art performance has been achieved.", "comment": null, "links": []}
{"entry_id": "2009.01067", "title": "Video Captioning Using Weak Annotation", "authors": ["Jingyi Hou", "Yunde Jia", "Xinxiao wu", "Yayun Qi"], "published": "2020-09-02 13:45:01", "updated": "2020-09-02 13:45:01", "summary": "Video captioning has shown impressive progress in recent years. One key\nreason of the performance improvements made by existing methods lie in massive\npaired video-sentence data, but collecting such strong annotation, i.e.,\nhigh-quality sentences, is time-consuming and laborious. It is the fact that\nthere now exist an amazing number of videos with weak annotation that only\ncontains semantic concepts such as actions and objects. In this paper, we\ninvestigate using weak annotation instead of strong annotation to train a video\ncaptioning model. To this end, we propose a progressive visual reasoning method\nthat progressively generates fine sentences from weak annotations by inferring\nmore semantic concepts and their dependency relationships for video captioning.\nTo model concept relationships, we use dependency trees that are spanned by\nexploiting external knowledge from large sentence corpora. Through traversing\nthe dependency trees, the sentences are generated to train the captioning\nmodel. Accordingly, we develop an iterative refinement algorithm that refines\nsentences via spanning dependency trees and fine-tunes the captioning model\nusing the refined sentences in an alternative training manner. Experimental\nresults demonstrate that our method using weak annotation is very competitive\nto the state-of-the-art methods using strong annotation.", "comment": null, "links": []}
{"entry_id": "2006.11524", "title": "Neuro-Symbolic Visual Reasoning: Disentangling \"Visual\" from \"Reasoning\"", "authors": ["Saeed Amizadeh", "Hamid Palangi", "Oleksandr Polozov", "Yichen Huang", "Kazuhito Koishida"], "published": "2020-06-20 08:48:29", "updated": "2020-08-25 23:30:57", "summary": "Visual reasoning tasks such as visual question answering (VQA) require an\ninterplay of visual perception with reasoning about the question semantics\ngrounded in perception. However, recent advances in this area are still\nprimarily driven by perception improvements (e.g. scene graph generation)\nrather than reasoning. Neuro-symbolic models such as Neural Module Networks\nbring the benefits of compositional reasoning to VQA, but they are still\nentangled with visual representation learning, and thus neural reasoning is\nhard to improve and assess on its own. To address this, we propose (1) a\nframework to isolate and evaluate the reasoning aspect of VQA separately from\nits perception, and (2) a novel top-down calibration technique that allows the\nmodel to answer reasoning questions even with imperfect perception. To this\nend, we introduce a differentiable first-order logic formalism for VQA that\nexplicitly decouples question answering from visual perception. On the\nchallenging GQA dataset, this framework is used to perform in-depth,\ndisentangled comparisons between well-known VQA models leading to informative\ninsights regarding the participating models as well as the task.", "comment": "Published in Proceedings of the 37th International Conference on\n  Machine Learning (ICML), Online, PMLR 119, 2020", "links": []}
{"entry_id": "2003.12462", "title": "TextCaps: a Dataset for Image Captioning with Reading Comprehension", "authors": ["Oleksii Sidorov", "Ronghang Hu", "Marcus Rohrbach", "Amanpreet Singh"], "published": "2020-03-24 02:38:35", "updated": "2020-08-04 04:08:02", "summary": "Image descriptions can help visually impaired people to quickly understand\nthe image content. While we made significant progress in automatically\ndescribing images and optical character recognition, current approaches are\nunable to include written text in their descriptions, although text is\nomnipresent in human environments and frequently critical to understand our\nsurroundings. To study how to comprehend text in the context of an image we\ncollect a novel dataset, TextCaps, with 145k captions for 28k images. Our\ndataset challenges a model to recognize text, relate it to its visual context,\nand decide what part of the text to copy or paraphrase, requiring spatial,\nsemantic, and visual reasoning between multiple text tokens and visual\nentities, such as objects. We study baselines and adapt existing approaches to\nthis new task, which we refer to as image captioning with reading\ncomprehension. Our analysis with automatic and human studies shows that our new\nTextCaps dataset provides many new technical challenges over previous datasets.", "comment": "To appear in ECCV 2020 (oral) Project page:\n  https://textvqa.org/textcaps", "links": []}
{"entry_id": "2007.12020", "title": "Few-shot Visual Reasoning with Meta-analogical Contrastive Learning", "authors": ["Youngsung Kim", "Jinwoo Shin", "Eunho Yang", "Sung Ju Hwang"], "published": "2020-07-23 14:00:34", "updated": "2020-07-23 14:00:34", "summary": "While humans can solve a visual puzzle that requires logical reasoning by\nobserving only few samples, it would require training over large amount of data\nfor state-of-the-art deep reasoning models to obtain similar performance on the\nsame task. In this work, we propose to solve such a few-shot (or low-shot)\nvisual reasoning problem, by resorting to analogical reasoning, which is a\nunique human ability to identify structural or relational similarity between\ntwo sets. Specifically, given training and test sets that contain the same type\nof visual reasoning problems, we extract the structural relationships between\nelements in both domains, and enforce them to be as similar as possible with\nanalogical learning. We repeatedly apply this process with slightly modified\nqueries of the same problem under the assumption that it does not affect the\nrelationship between a training and a test sample. This allows to learn the\nrelational similarity between the two samples in an effective manner even with\na single pair of samples. We validate our method on RAVEN dataset, on which it\noutperforms state-of-the-art method, with larger gains when the training data\nis scarce. We further meta-learn our analogical contrastive learning model over\nthe same tasks with diverse attributes, and show that it generalizes to the\nsame visual reasoning problem with unseen attributes.", "comment": null, "links": []}
{"entry_id": "2007.09049", "title": "Learning to Discretely Compose Reasoning Module Networks for Video Captioning", "authors": ["Ganchao Tan", "Daqing Liu", "Meng Wang", "Zheng-Jun Zha"], "published": "2020-07-17 15:27:37", "updated": "2020-07-17 15:27:37", "summary": "Generating natural language descriptions for videos, i.e., video captioning,\nessentially requires step-by-step reasoning along the generation process. For\nexample, to generate the sentence \"a man is shooting a basketball\", we need to\nfirst locate and describe the subject \"man\", next reason out the man is\n\"shooting\", then describe the object \"basketball\" of shooting. However,\nexisting visual reasoning methods designed for visual question answering are\nnot appropriate to video captioning, for it requires more complex visual\nreasoning on videos over both space and time, and dynamic module composition\nalong the generation process. In this paper, we propose a novel visual\nreasoning approach for video captioning, named Reasoning Module Networks (RMN),\nto equip the existing encoder-decoder framework with the above reasoning\ncapacity. Specifically, our RMN employs 1) three sophisticated spatio-temporal\nreasoning modules, and 2) a dynamic and discrete module selector trained by a\nlinguistic loss with a Gumbel approximation. Extensive experiments on MSVD and\nMSR-VTT datasets demonstrate the proposed RMN outperforms the state-of-the-art\nmethods while providing an explicit and explainable generation process. Our\ncode is available at https://github.com/tgc1997/RMN.", "comment": "Accepted at IJCAI 2020 Main Track. Sole copyright holder is IJCAI.\n  Code is available at https://github.com/tgc1997/RMN", "links": []}
{"entry_id": "2007.04670", "title": "Multi-Granularity Modularized Network for Abstract Visual Reasoning", "authors": ["Xiangru Tang", "Haoyuan Wang", "Xiang Pan", "Jiyang Qi"], "published": "2020-07-09 09:54:05", "updated": "2020-07-10 02:32:25", "summary": "Abstract visual reasoning connects mental abilities to the physical world,\nwhich is a crucial factor in cognitive development. Most toddlers display\nsensitivity to this skill, but it is not easy for machines. Aimed at it, we\nfocus on the Raven Progressive Matrices Test, designed to measure cognitive\nreasoning. Recent work designed some black-boxes to solve it in an end-to-end\nfashion, but they are incredibly complicated and difficult to explain. Inspired\nby cognitive studies, we propose a Multi-Granularity Modularized Network (MMoN)\nto bridge the gap between the processing of raw sensory information and\nsymbolic reasoning. Specifically, it learns modularized reasoning functions to\nmodel the semantic rule from the visual grounding in a neuro-symbolic and\nsemi-supervision way. To comprehensively evaluate MMoN, our experiments are\nconducted on the dataset of both seen and unseen reasoning rules. The result\nshows that MMoN is well suited for abstract visual reasoning and also\nexplainable on the generalization test.", "comment": null, "links": []}
{"entry_id": "2006.14264", "title": "Self-Segregating and Coordinated-Segregating Transformer for Focused Deep Multi-Modular Network for Visual Question Answering", "authors": ["Chiranjib Sur"], "published": "2020-06-25 09:17:03", "updated": "2020-06-25 09:17:03", "summary": "Attention mechanism has gained huge popularity due to its effectiveness in\nachieving high accuracy in different domains. But attention is opportunistic\nand is not justified by the content or usability of the content. Transformer\nlike structure creates all/any possible attention(s). We define segregating\nstrategies that can prioritize the contents for the applications for\nenhancement of performance. We defined two strategies: Self-Segregating\nTransformer (SST) and Coordinated-Segregating Transformer (CST) and used it to\nsolve visual question answering application. Self-segregation strategy for\nattention contributes in better understanding and filtering the information\nthat can be most helpful for answering the question and create diversity of\nvisual-reasoning for attention. This work can easily be used in many other\napplications that involve repetition and multiple frames of features and would\nreduce the commonality of the attentions to a great extent. Visual Question\nAnswering (VQA) requires understanding and coordination of both images and\ntextual interpretations. Experiments demonstrate that segregation strategies\nfor cascaded multi-head transformer attention outperforms many previous works\nand achieved considerable improvement for VQA-v2 dataset benchmark.", "comment": null, "links": []}
{"entry_id": "2004.00849", "title": "Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers", "authors": ["Zhicheng Huang", "Zhaoyang Zeng", "Bei Liu", "Dongmei Fu", "Jianlong Fu"], "published": "2020-04-02 07:39:28", "updated": "2020-06-22 09:09:22", "summary": "We propose Pixel-BERT to align image pixels with text by deep multi-modal\ntransformers that jointly learn visual and language embedding in a unified\nend-to-end framework. We aim to build a more accurate and thorough connection\nbetween image pixels and language semantics directly from image and sentence\npairs instead of using region-based image features as the most recent vision\nand language tasks. Our Pixel-BERT which aligns semantic connection in pixel\nand text level solves the limitation of task-specific visual representation for\nvision and language tasks. It also relieves the cost of bounding box\nannotations and overcomes the unbalance between semantic labels in visual task\nand language semantic. To provide a better representation for down-stream\ntasks, we pre-train a universal end-to-end model with image and sentence pairs\nfrom Visual Genome dataset and MS-COCO dataset. We propose to use a random\npixel sampling mechanism to enhance the robustness of visual representation and\nto apply the Masked Language Model and Image-Text Matching as pre-training\ntasks. Extensive experiments on downstream tasks with our pre-trained model\nshow that our approach makes the most state-of-the-arts in downstream tasks,\nincluding Visual Question Answering (VQA), image-text retrieval, Natural\nLanguage for Visual Reasoning for Real (NLVR). Particularly, we boost the\nperformance of a single model in VQA task by 2.17 points compared with SOTA\nunder fair comparison.", "comment": null, "links": []}
{"entry_id": "2006.11197", "title": "Abstract Diagrammatic Reasoning with Multiplex Graph Networks", "authors": ["Duo Wang", "Mateja Jamnik", "Pietro Lio"], "published": "2020-06-19 15:50:25", "updated": "2020-06-19 15:50:25", "summary": "Abstract reasoning, particularly in the visual domain, is a complex human\nability, but it remains a challenging problem for artificial neural learning\nsystems. In this work we propose MXGNet, a multilayer graph neural network for\nmulti-panel diagrammatic reasoning tasks. MXGNet combines three powerful\nconcepts, namely, object-level representation, graph neural networks and\nmultiplex graphs, for solving visual reasoning tasks. MXGNet first extracts\nobject-level representations for each element in all panels of the diagrams,\nand then forms a multi-layer multiplex graph capturing multiple relations\nbetween objects across different diagram panels. MXGNet summarises the multiple\ngraphs extracted from the diagrams of the task, and uses this summarisation to\npick the most probable answer from the given candidates. We have tested MXGNet\non two types of diagrammatic reasoning tasks, namely Diagram Syllogisms and\nRaven Progressive Matrices (RPM). For an Euler Diagram Syllogism task MXGNet\nachieves state-of-the-art accuracy of 99.8%. For PGM and RAVEN, two\ncomprehensive datasets for RPM reasoning, MXGNet outperforms the\nstate-of-the-art models by a considerable margin.", "comment": null, "links": []}
{"entry_id": "2006.05398", "title": "Deep Visual Reasoning: Learning to Predict Action Sequences for Task and Motion Planning from an Initial Scene Image", "authors": ["Danny Driess", "Jung-Su Ha", "Marc Toussaint"], "published": "2020-06-09 16:52:02", "updated": "2020-06-09 16:52:02", "summary": "In this paper, we propose a deep convolutional recurrent neural network that\npredicts action sequences for task and motion planning (TAMP) from an initial\nscene image. Typical TAMP problems are formalized by combining reasoning on a\nsymbolic, discrete level (e.g. first-order logic) with continuous motion\nplanning such as nonlinear trajectory optimization. Due to the great\ncombinatorial complexity of possible discrete action sequences, a large number\nof optimization/motion planning problems have to be solved to find a solution,\nwhich limits the scalability of these approaches.\n  To circumvent this combinatorial complexity, we develop a neural network\nwhich, based on an initial image of the scene, directly predicts promising\ndiscrete action sequences such that ideally only one motion planning problem\nhas to be solved to find a solution to the overall TAMP problem. A key aspect\nis that our method generalizes to scenes with many and varying number of\nobjects, although being trained on only two objects at a time. This is possible\nby encoding the objects of the scene in images as input to the neural network,\ninstead of a fixed feature vector. Results show runtime improvements of several\nmagnitudes. Video: https://youtu.be/i8yyEbbvoEk", "comment": "Robotics: Science and Systems (R:SS) 2020", "links": []}
{"entry_id": "2004.12770", "title": "Differentiable Adaptive Computation Time for Visual Reasoning", "authors": ["Cristobal Eyzaguirre", "Alvaro Soto"], "published": "2020-04-27 13:20:23", "updated": "2020-05-22 16:57:14", "summary": "This paper presents a novel attention-based algorithm for achieving adaptive\ncomputation called DACT, which, unlike existing ones, is end-to-end\ndifferentiable. Our method can be used in conjunction with many networks; in\nparticular, we study its application to the widely known MAC architecture,\nobtaining a significant reduction in the number of recurrent steps needed to\nachieve similar accuracies, therefore improving its performance to computation\nratio. Furthermore, we show that by increasing the maximum number of steps\nused, we surpass the accuracy of even our best non-adaptive MAC in the CLEVR\ndataset, demonstrating that our approach is able to control the number of steps\nwithout significant loss of performance. Additional advantages provided by our\napproach include considerably improving interpretability by discarding useless\nsteps and providing more insights into the underlying reasoning process.\nFinally, we present adaptive computation as an equivalent to an ensemble of\nmodels, similar to a mixture of expert formulation. Both the code and the\nconfiguration files for our experiments are made available to support further\nresearch in this area.", "comment": "CVPR 2020", "links": []}
{"entry_id": "2005.09183", "title": "Retrieving and Highlighting Action with Spatiotemporal Reference", "authors": ["Seito Kasai", "Yuchi Ishikawa", "Masaki Hayashi", "Yoshimitsu Aoki", "Kensho Hara", "Hirokatsu Kataoka"], "published": "2020-05-19 03:12:31", "updated": "2020-05-19 03:12:31", "summary": "In this paper, we present a framework that jointly retrieves and\nspatiotemporally highlights actions in videos by enhancing current deep\ncross-modal retrieval methods. Our work takes on the novel task of action\nhighlighting, which visualizes where and when actions occur in an untrimmed\nvideo setting. Action highlighting is a fine-grained task, compared to\nconventional action recognition tasks which focus on classification or\nwindow-based localization. Leveraging weak supervision from annotated captions,\nour framework acquires spatiotemporal relevance maps and generates local\nembeddings which relate to the nouns and verbs in captions. Through\nexperiments, we show that our model generates various maps conditioned on\ndifferent actions, in which conventional visual reasoning methods only go as\nfar as to show a single deterministic saliency map. Also, our model improves\nretrieval recall over our baseline without alignment by 2-3% on the MSR-VTT\ndataset.", "comment": "Accepted to ICIP 2020", "links": []}
{"entry_id": "2005.06035", "title": "Cross-Modality Relevance for Reasoning on Language and Vision", "authors": ["Chen Zheng", "Quan Guo", "Parisa Kordjamshidi"], "published": "2020-05-12 20:17:25", "updated": "2020-05-12 20:17:25", "summary": "This work deals with the challenge of learning and reasoning over language\nand vision data for the related downstream tasks such as visual question\nanswering (VQA) and natural language for visual reasoning (NLVR). We design a\nnovel cross-modality relevance module that is used in an end-to-end framework\nto learn the relevance representation between components of various input\nmodalities under the supervision of a target task, which is more generalizable\nto unobserved data compared to merely reshaping the original representation\nspace. In addition to modeling the relevance between the textual entities and\nvisual entities, we model the higher-order relevance between entity relations\nin the text and object relations in the image. Our proposed approach shows\ncompetitive performance on two different language and vision tasks using public\nbenchmarks and improves the state-of-the-art published results. The learned\nalignments of input spaces and their relevance representations by NLVR task\nboost the training efficiency of VQA task.", "comment": "Accepted by ACL 2020", "links": []}
{"entry_id": "2004.12193", "title": "Machine Number Sense: A Dataset of Visual Arithmetic Problems for Abstract and Relational Reasoning", "authors": ["Wenhe Zhang", "Chi Zhang", "Yixin Zhu", "Song-Chun Zhu"], "published": "2020-04-25 17:14:58", "updated": "2020-04-25 17:14:58", "summary": "As a comprehensive indicator of mathematical thinking and intelligence, the\nnumber sense (Dehaene 2011) bridges the induction of symbolic concepts and the\ncompetence of problem-solving. To endow such a crucial cognitive ability to\nmachine intelligence, we propose a dataset, Machine Number Sense (MNS),\nconsisting of visual arithmetic problems automatically generated using a\ngrammar model--And-Or Graph (AOG). These visual arithmetic problems are in the\nform of geometric figures: each problem has a set of geometric shapes as its\ncontext and embedded number symbols. Solving such problems is not trivial; the\nmachine not only has to recognize the number, but also to interpret the number\nwith its contexts, shapes, and relations (e.g., symmetry) together with proper\noperations. We benchmark the MNS dataset using four predominant neural network\nmodels as baselines in this visual reasoning task. Comprehensive experiments\nshow that current neural-network-based models still struggle to understand\nnumber concepts and relational operations. We show that a simple brute-force\nsearch algorithm could work out some of the problems without context\ninformation. Crucially, taking geometric context into account by an additional\nperception module would provide a sharp performance gain with fewer search\nsteps. Altogether, we call for attention in fusing the classic search-based\nalgorithms with modern neural networks to discover the essential number\nconcepts in future research.", "comment": "AAAI 2020 Oral. Project page:\n  https://sites.google.com/view/number-sense/home Code:\n  https://github.com/zwh1999anne/Machine-Number-Sense-Dataset Dataset:\n  https://drive.google.com/file/d/17KuL8KOIDAeRL-lD418oiDEm8bE6TEFb/view", "links": []}
{"entry_id": "2004.02673", "title": "SHOP-VRB: A Visual Reasoning Benchmark for Object Perception", "authors": ["Michal Nazarczuk", "Krystian Mikolajczyk"], "published": "2020-04-06 13:46:54", "updated": "2020-04-06 13:46:54", "summary": "In this paper we present an approach and a benchmark for visual reasoning in\nrobotics applications, in particular small object grasping and manipulation.\nThe approach and benchmark are focused on inferring object properties from\nvisual and text data. It concerns small household objects with their\nproperties, functionality, natural language descriptions as well as\nquestion-answer pairs for visual reasoning queries along with their\ncorresponding scene semantic representations. We also present a method for\ngenerating synthetic data which allows to extend the benchmark to other objects\nor scenes and propose an evaluation protocol that is more challenging than in\nthe existing datasets. We propose a reasoning system based on symbolic program\nexecution. A disentangled representation of the visual and textual inputs is\nobtained and used to execute symbolic programs that represent a 'reasoning\nprocess' of the algorithm. We perform a set of experiments on the proposed\nbenchmark and compare to results for the state of the art methods. These\nresults expose the shortcomings of the existing benchmarks that may lead to\nmisleading conclusions on the actual performance of the visual reasoning\nsystems.", "comment": "International Conference on Robotics and Automation (ICRA) 2020", "links": []}
{"entry_id": "2001.02359", "title": "Weakly Supervised Visual Semantic Parsing", "authors": ["Alireza Zareian", "Svebor Karaman", "Shih-Fu Chang"], "published": "2020-01-08 03:46:13", "updated": "2020-03-31 18:54:06", "summary": "Scene Graph Generation (SGG) aims to extract entities, predicates and their\nsemantic structure from images, enabling deep understanding of visual content,\nwith many applications such as visual reasoning and image retrieval.\nNevertheless, existing SGG methods require millions of manually annotated\nbounding boxes for training, and are computationally inefficient, as they\nexhaustively process all pairs of object proposals to detect predicates. In\nthis paper, we address those two limitations by first proposing a generalized\nformulation of SGG, namely Visual Semantic Parsing, which disentangles entity\nand predicate recognition, and enables sub-quadratic performance. Then we\npropose the Visual Semantic Parsing Network, VSPNet, based on a dynamic,\nattention-based, bipartite message passing framework that jointly infers graph\nnodes and edges through an iterative process. Additionally, we propose the\nfirst graph-based weakly supervised learning framework, based on a novel graph\nalignment algorithm, which enables training without bounding box annotations.\nThrough extensive experiments, we show that VSPNet outperforms weakly\nsupervised baselines significantly and approaches fully supervised performance,\nwhile being several times faster. We publicly release the source code of our\nmethod.", "comment": "To be presented at CVPR 2020 (oral paper)", "links": []}
{"entry_id": "1902.10200", "title": "Differentiable Scene Graphs", "authors": ["Moshiko Raboh", "Roei Herzig", "Gal Chechik", "Jonathan Berant", "Amir Globerson"], "published": "2019-02-26 20:22:33", "updated": "2020-03-14 16:25:32", "summary": "Reasoning about complex visual scenes involves perception of entities and\ntheir relations. Scene graphs provide a natural representation for reasoning\ntasks, by assigning labels to both entities (nodes) and relations (edges).\nUnfortunately, reasoning systems based on SGs are typically trained in a\ntwo-step procedure: First, training a model to predict SGs from images; Then, a\nseparate model is created to reason based on predicted SGs. In many domains, it\nis preferable to train systems jointly in an end-to-end manner, but SGs are not\ncommonly used as intermediate components in visual reasoning systems because\nbeing discrete and sparse, scene-graph representations are non-differentiable\nand difficult to optimize. Here we propose Differentiable Scene Graphs (DSGs),\nan image representation that is amenable to differentiable end-to-end\noptimization, and requires supervision only from the downstream tasks. DSGs\nprovide a dense representation for all regions and pairs of regions, and do not\nspend modelling capacity on areas of the images that do not contain objects or\nrelations of interest. We evaluate our model on the challenging task of\nidentifying referring relationships (RR) in three benchmark datasets, Visual\nGenome, VRD and CLEVR. We describe a multi-task objective, and train in an\nend-to-end manner supervised by the downstream RR task. Using DSGs as an\nintermediate representation leads to new state-of-the-art performance.", "comment": "Winter Conference on Applications of Computer Vision (WACV), 2020", "links": []}
{"entry_id": "1910.01442", "title": "CLEVRER: CoLlision Events for Video REpresentation and Reasoning", "authors": ["Kexin Yi", "Chuang Gan", "Yunzhu Li", "Pushmeet Kohli", "Jiajun Wu", "Antonio Torralba", "Joshua B. Tenenbaum"], "published": "2019-10-03 13:16:36", "updated": "2020-03-08 00:09:07", "summary": "The ability to reason about temporal and causal events from videos lies at\nthe core of human intelligence. Most video reasoning benchmarks, however, focus\non pattern recognition from complex visual and language input, instead of on\ncausal structure. We study the complementary problem, exploring the temporal\nand causal structures behind videos of objects with simple visual appearance.\nTo this end, we introduce the CoLlision Events for Video REpresentation and\nReasoning (CLEVRER), a diagnostic video dataset for systematic evaluation of\ncomputational models on a wide range of reasoning tasks. Motivated by the\ntheory of human casual judgment, CLEVRER includes four types of questions:\ndescriptive (e.g., \"what color\"), explanatory (\"what is responsible for\"),\npredictive (\"what will happen next\"), and counterfactual (\"what if\"). We\nevaluate various state-of-the-art models for visual reasoning on our benchmark.\nWhile these models thrive on the perception-based task (descriptive), they\nperform poorly on the causal tasks (explanatory, predictive and\ncounterfactual), suggesting that a principled approach for causal reasoning\nshould incorporate the capability of both perceiving complex visual and\nlanguage inputs, and understanding the underlying dynamics and causal\nrelations. We also study an oracle model that explicitly combines these\ncomponents via symbolic representations.", "comment": "The first two authors contributed equally to this work. Accepted as\n  Oral Spotlight as ICLR 2020. Project page: http://clevrer.csail.mit.edu/", "links": []}
{"entry_id": "2003.01835", "title": "Learning Rope Manipulation Policies Using Dense Object Descriptors Trained on Synthetic Depth Data", "authors": ["Priya Sundaresan", "Jennifer Grannen", "Brijen Thananjeyan", "Ashwin Balakrishna", "Michael Laskey", "Kevin Stone", "Joseph E. Gonzalez", "Ken Goldberg"], "published": "2020-03-03 23:43:05", "updated": "2020-03-03 23:43:05", "summary": "Robotic manipulation of deformable 1D objects such as ropes, cables, and\nhoses is challenging due to the lack of high-fidelity analytic models and large\nconfiguration spaces. Furthermore, learning end-to-end manipulation policies\ndirectly from images and physical interaction requires significant time on a\nrobot and can fail to generalize across tasks. We address these challenges\nusing interpretable deep visual representations for rope, extending recent work\non dense object descriptors for robot manipulation. This facilitates the design\nof interpretable and transferable geometric policies built on top of the\nlearned representations, decoupling visual reasoning and control. We present an\napproach that learns point-pair correspondences between initial and goal rope\nconfigurations, which implicitly encodes geometric structure, entirely in\nsimulation from synthetic depth images. We demonstrate that the learned\nrepresentation -- dense depth object descriptors (DDODs) -- can be used to\nmanipulate a real rope into a variety of different arrangements either by\nlearning from demonstrations or using interpretable geometric policies. In 50\ntrials of a knot-tying task with the ABB YuMi Robot, the system achieves a 66%\nknot-tying success rate from previously unseen configurations. See\nhttps://tinyurl.com/rope-learning for supplementary material and videos.", "comment": null, "links": []}
{"entry_id": "2003.00403", "title": "Cops-Ref: A new Dataset and Task on Compositional Referring Expression Comprehension", "authors": ["Zhenfang Chen", "Peng Wang", "Lin Ma", "Kwan-Yee K. Wong", "Qi Wu"], "published": "2020-03-01 04:59:38", "updated": "2020-03-01 04:59:38", "summary": "Referring expression comprehension (REF) aims at identifying a particular\nobject in a scene by a natural language expression. It requires joint reasoning\nover the textual and visual domains to solve the problem. Some popular\nreferring expression datasets, however, fail to provide an ideal test bed for\nevaluating the reasoning ability of the models, mainly because 1) their\nexpressions typically describe only some simple distinctive properties of the\nobject and 2) their images contain limited distracting information. To bridge\nthe gap, we propose a new dataset for visual reasoning in context of referring\nexpression comprehension with two main features. First, we design a novel\nexpression engine rendering various reasoning logics that can be flexibly\ncombined with rich visual properties to generate expressions with varying\ncompositionality. Second, to better exploit the full reasoning chain embodied\nin an expression, we propose a new test setting by adding additional\ndistracting images containing objects sharing similar properties with the\nreferent, thus minimising the success rate of reasoning-free cross-domain\nalignment. We evaluate several state-of-the-art REF models, but find none of\nthem can achieve promising performance. A proposed modular hard mining strategy\nperforms the best but still leaves substantial room for improvement. We hope\nthis new dataset and task can serve as a benchmark for deeper visual reasoning\nanalysis and foster the research on referring expression comprehension.", "comment": "To appear in CVPR2020", "links": []}
{"entry_id": "1911.11938", "title": "Transfer Learning in Visual and Relational Reasoning", "authors": ["T. S. Jayram", "Vincent Marois", "Tomasz Kornuta", "Vincent Albouy", "Emre Sevgen", "Ahmet S. Ozcan"], "published": "2019-11-27 03:54:15", "updated": "2020-02-15 04:26:42", "summary": "Transfer learning has become the de facto standard in computer vision and\nnatural language processing, especially where labeled data is scarce. Accuracy\ncan be significantly improved by using pre-trained models and subsequent\nfine-tuning. In visual reasoning tasks, such as image question answering,\ntransfer learning is more complex. In addition to transferring the capability\nto recognize visual features, we also expect to transfer the system's ability\nto reason. Moreover, for video data, temporal reasoning adds another dimension.\nIn this work, we formalize these unique aspects of transfer learning and\npropose a theoretical framework for visual reasoning, exemplified by the\nwell-established CLEVR and COG datasets. Furthermore, we introduce a new,\nend-to-end differentiable recurrent model (SAMNet), which shows\nstate-of-the-art accuracy and better performance in transfer learning on both\ndatasets. The improved performance of SAMNet stems from its capability to\ndecouple the abstract multi-step reasoning from the length of the sequence and\nits selective attention enabling to store only the question-relevant objects in\nthe external memory.", "comment": "18 pages; more baseline comparisons; additional clarifications", "links": []}
{"entry_id": "1905.12506", "title": "Are Disentangled Representations Helpful for Abstract Visual Reasoning?", "authors": ["Sjoerd van Steenkiste", "Francesco Locatello", "Jürgen Schmidhuber", "Olivier Bachem"], "published": "2019-05-29 14:52:32", "updated": "2020-01-07 14:36:07", "summary": "A disentangled representation encodes information about the salient factors\nof variation in the data independently. Although it is often argued that this\nrepresentational format is useful in learning to solve many real-world\ndown-stream tasks, there is little empirical evidence that supports this claim.\nIn this paper, we conduct a large-scale study that investigates whether\ndisentangled representations are more suitable for abstract reasoning tasks.\nUsing two new tasks similar to Raven's Progressive Matrices, we evaluate the\nusefulness of the representations learned by 360 state-of-the-art unsupervised\ndisentanglement models. Based on these representations, we train 3600 abstract\nreasoning models and observe that disentangled representations do in fact lead\nto better down-stream performance. In particular, they enable quicker learning\nusing fewer samples.", "comment": "Accepted to NeurIPS 2019", "links": []}
{"entry_id": "1905.11666", "title": "Learning Dynamics of Attention: Human Prior for Interpretable Machine Reasoning", "authors": ["Wonjae Kim", "Yoonho Lee"], "published": "2019-05-28 08:13:37", "updated": "2019-12-23 05:37:28", "summary": "Without relevant human priors, neural networks may learn uninterpretable\nfeatures. We propose Dynamics of Attention for Focus Transition (DAFT) as a\nhuman prior for machine reasoning. DAFT is a novel method that regularizes\nattention-based reasoning by modelling it as a continuous dynamical system\nusing neural ordinary differential equations. As a proof of concept, we augment\na state-of-the-art visual reasoning model with DAFT. Our experiments reveal\nthat applying DAFT yields similar performance to the original model while using\nfewer reasoning steps, showing that it implicitly learns to skip unnecessary\nsteps. We also propose a new metric, Total Length of Transition (TLT), which\nrepresents the effective reasoning step size by quantifying how much a given\nmodel's focus drifts while reasoning about a question. We show that adding DAFT\nresults in lower TLT, demonstrating that our method indeed obeys the human\nprior towards shorter reasoning paths in addition to producing more\ninterpretable attention maps. Our code is available at\nhttps://github.com/kakao/DAFT.", "comment": "20 pages, 18 figures, 2 tables", "links": []}
{"entry_id": "1912.09589", "title": "Smart Home Appliances: Chat with Your Fridge", "authors": ["Denis Gudovskiy", "Gyuri Han", "Takuya Yamaguchi", "Sotaro Tsukizawa"], "published": "2019-12-19 23:12:25", "updated": "2019-12-19 23:12:25", "summary": "Current home appliances are capable to execute a limited number of voice\ncommands such as turning devices on or off, adjusting music volume or light\nconditions. Recent progress in machine reasoning gives an opportunity to\ndevelop new types of conversational user interfaces for home appliances. In\nthis paper, we apply state-of-the-art visual reasoning model and demonstrate\nthat it is feasible to ask a smart fridge about its contents and various\nproperties of the food with close-to-natural conversation experience. Our\nvisual reasoning model answers user questions about existence, count, category\nand freshness of each product by analyzing photos made by the image sensor\ninside the smart fridge. Users may chat with their fridge using off-the-shelf\nphone messenger while being away from home, for example, when shopping in the\nsupermarket. We generate a visually realistic synthetic dataset to train\nmachine learning reasoning model that achieves 95% answer accuracy on test\ndata. We present the results of initial user tests and discuss how we modify\ndistribution of generated questions for model training based on\nhuman-in-the-loop guidance. We open source code for the whole system including\ndataset generation, reasoning model and demonstration scripts.", "comment": "NeurIPS 2019 demo track", "links": []}
{"entry_id": "1908.07490", "title": "LXMERT: Learning Cross-Modality Encoder Representations from Transformers", "authors": ["Hao Tan", "Mohit Bansal"], "published": "2019-08-20 17:05:18", "updated": "2019-12-03 19:30:19", "summary": "Vision-and-language reasoning requires an understanding of visual concepts,\nlanguage semantics, and, most importantly, the alignment and relationships\nbetween these two modalities. We thus propose the LXMERT (Learning\nCross-Modality Encoder Representations from Transformers) framework to learn\nthese vision-and-language connections. In LXMERT, we build a large-scale\nTransformer model that consists of three encoders: an object relationship\nencoder, a language encoder, and a cross-modality encoder. Next, to endow our\nmodel with the capability of connecting vision and language semantics, we\npre-train the model with large amounts of image-and-sentence pairs, via five\ndiverse representative pre-training tasks: masked language modeling, masked\nobject prediction (feature regression and label classification), cross-modality\nmatching, and image question answering. These tasks help in learning both\nintra-modality and cross-modality relationships. After fine-tuning from our\npre-trained parameters, our model achieves the state-of-the-art results on two\nvisual question answering datasets (i.e., VQA and GQA). We also show the\ngeneralizability of our pre-trained cross-modality model by adapting it to a\nchallenging visual-reasoning task, NLVR2, and improve the previous best result\nby 22% absolute (54% to 76%). Lastly, we demonstrate detailed ablation studies\nto prove that both our novel model components and pre-training strategies\nsignificantly contribute to our strong results; and also present several\nattention visualizations for the different encoders. Code and pre-trained\nmodels publicly available at: https://github.com/airsplay/lxmert", "comment": "EMNLP 2019 (14 pages; with new attention visualizations)", "links": []}
{"entry_id": "1911.07736", "title": "Modeling Gestalt Visual Reasoning on the Raven's Progressive Matrices Intelligence Test Using Generative Image Inpainting Techniques", "authors": ["Tianyu Hua", "Maithilee Kunda"], "published": "2019-11-18 16:16:55", "updated": "2019-11-26 08:32:20", "summary": "Psychologists recognize Raven's Progressive Matrices as a very effective test\nof general human intelligence. While many computational models have been\ndeveloped by the AI community to investigate different forms of top-down,\ndeliberative reasoning on the test, there has been less research on bottom-up\nperceptual processes, like Gestalt image completion, that are also critical in\nhuman test performance. In this work, we investigate how Gestalt visual\nreasoning on the Raven's test can be modeled using generative image inpainting\ntechniques from computer vision. We demonstrate that a self-supervised\ninpainting model trained only on photorealistic images of objects achieves a\nscore of 27/36 on the Colored Progressive Matrices, which corresponds to\naverage performance for nine-year-old children. We also show that models\ntrained on other datasets (faces, places, and textures) do not perform as well.\nOur results illustrate how learning visual regularities in real-world images\ncan translate into successful reasoning about artificial test stimuli. On the\nflip side, our results also highlight the limitations of such transfer, which\nmay explain why intelligence tests like the Raven's are often sensitive to\npeople's individual sociocultural backgrounds.", "comment": null, "links": []}
{"entry_id": "1907.03950", "title": "Learning by Abstraction: The Neural State Machine", "authors": ["Drew A. Hudson", "Christopher D. Manning"], "published": "2019-07-09 03:08:41", "updated": "2019-11-25 10:02:05", "summary": "We introduce the Neural State Machine, seeking to bridge the gap between the\nneural and symbolic views of AI and integrate their complementary strengths for\nthe task of visual reasoning. Given an image, we first predict a probabilistic\ngraph that represents its underlying semantics and serves as a structured world\nmodel. Then, we perform sequential reasoning over the graph, iteratively\ntraversing its nodes to answer a given question or draw a new inference. In\ncontrast to most neural architectures that are designed to closely interact\nwith the raw sensory data, our model operates instead in an abstract latent\nspace, by transforming both the visual and linguistic modalities into semantic\nconcept-based representations, thereby achieving enhanced transparency and\nmodularity. We evaluate our model on VQA-CP and GQA, two recent VQA datasets\nthat involve compositionality, multi-step inference and diverse reasoning\nskills, achieving state-of-the-art results in both cases. We provide further\nexperiments that illustrate the model's strong generalization capacity across\nmultiple dimensions, including novel compositions of concepts, changes in the\nanswer distribution, and unseen linguistic structures, demonstrating the\nqualities and efficacy of our approach.", "comment": "Published as a conference paper at NeurIPS 2019 (spotlight)", "links": []}
{"entry_id": "1911.09655", "title": "Temporal Reasoning via Audio Question Answering", "authors": ["Haytham M. Fayek", "Justin Johnson"], "published": "2019-11-21 18:26:30", "updated": "2019-11-21 18:26:30", "summary": "Multimodal question answering tasks can be used as proxy tasks to study\nsystems that can perceive and reason about the world. Answering questions about\ndifferent types of input modalities stresses different aspects of reasoning\nsuch as visual reasoning, reading comprehension, story understanding, or\nnavigation. In this paper, we use the task of Audio Question Answering (AQA) to\nstudy the temporal reasoning abilities of machine learning models. To this end,\nwe introduce the Diagnostic Audio Question Answering (DAQA) dataset comprising\naudio sequences of natural sound events and programmatically generated\nquestions and answers that probe various aspects of temporal reasoning. We\nadapt several recent state-of-the-art methods for visual question answering to\nthe AQA task, and use DAQA to demonstrate that they perform poorly on questions\nthat require in-depth temporal reasoning. Finally, we propose a new model,\nMultiple Auxiliary Controllers for Linear Modulation (MALiMo) that extends the\nrecent Feature-wise Linear Modulation (FiLM) model and significantly improves\nits temporal reasoning capabilities. We envisage DAQA to foster research on AQA\nand temporal reasoning and MALiMo a step towards models for AQA.", "comment": null, "links": []}
{"entry_id": "1911.09375", "title": "ChartNet: Visual Reasoning over Statistical Charts using MAC-Networks", "authors": ["Monika Sharma", "Shikha Gupta", "Arindam Chowdhury", "Lovekesh Vig"], "published": "2019-11-21 10:03:25", "updated": "2019-11-21 10:03:25", "summary": "Despite the improvements in perception accuracies brought about via deep\nlearning, developing systems combining accurate visual perception with the\nability to reason over the visual percepts remains extremely challenging. A\nparticular application area of interest from an accessibility perspective is\nthat of reasoning over statistical charts such as bar and pie charts. To this\nend, we formulate the problem of reasoning over statistical charts as a\nclassification task using MAC-Networks to give answers from a predefined\nvocabulary of generic answers. Additionally, we enhance the capabilities of\nMAC-Networks to give chart-specific answers to open-ended questions by\nreplacing the classification layer by a regression layer to localize the\ntextual answers present over the images. We call our network ChartNet, and\ndemonstrate its efficacy on predicting both in vocabulary and out of vocabulary\nanswers. To test our methods, we generated our own dataset of statistical chart\nimages and corresponding question answer pairs. Results show that ChartNet\nconsistently outperform other state-of-the-art methods on reasoning over these\nquestions and may be a viable candidate for applications containing images of\nstatistical charts.", "comment": null, "links": ["http://dx.doi.org/10.1109/IJCNN.2019.8852427"]}
{"entry_id": "1911.07721", "title": "Program synthesis performance constrained by non-linear spatial relations in Synthetic Visual Reasoning Test", "authors": ["Lu Yihe", "Scott C. Lowe", "Penelope A. Lewis", "Mark C. W. van Rossum"], "published": "2019-11-18 15:47:03", "updated": "2019-11-19 12:32:25", "summary": "Despite remarkable advances in automated visual recognition by machines, some\nvisual tasks remain challenging for machines. Fleuret et al. (2011) introduced\nthe Synthetic Visual Reasoning Test (SVRT) to highlight this point, which\nrequired classification of images consisting of randomly generated shapes based\non hidden abstract rules using only a few examples. Ellis et al. (2015)\ndemonstrated that a program synthesis approach could solve some of the SVRT\nproblems with unsupervised, few-shot learning, whereas they remained\nchallenging for several convolutional neural networks trained with thousands of\nexamples. Here we re-considered the human and machine experiments, because they\nfollowed different protocols and yielded different statistics. We thus proposed\na quantitative reintepretation of the data between the protocols, so that we\ncould make fair comparison between human and machine performance. We improved\nthe program synthesis classifier by correcting the image parsings, and compared\nthe results to the performance of other machine agents and human subjects. We\ngrouped the SVRT problems into different types by the two aspects of the core\ncharacteristics for classification: shape specification and location relation.\nWe found that the program synthesis classifier could not solve problems\ninvolving shape distances, because it relied on symbolic computation which\nscales poorly with input dimension and adding distances into such computation\nwould increase the dimension combinatorially with the number of shapes in an\nimage. Therefore, although the program synthesis classifier is capable of\nabstract reasoning, its performance is highly constrained by the accessible\ninformation in image parsings.", "comment": null, "links": []}
{"entry_id": "1911.05990", "title": "Attention on Abstract Visual Reasoning", "authors": ["Lukas Hahne", "Timo Lüddecke", "Florentin Wörgötter", "David Kappel"], "published": "2019-11-14 08:33:40", "updated": "2019-11-14 08:33:40", "summary": "Attention mechanisms have been boosting the performance of deep learning\nmodels on a wide range of applications, ranging from speech understanding to\nprogram induction. However, despite experiments from psychology which suggest\nthat attention plays an essential role in visual reasoning, the full potential\nof attention mechanisms has so far not been explored to solve abstract\ncognitive tasks on image data. In this work, we propose a hybrid network\narchitecture, grounded on self-attention and relational reasoning. We call this\nnew model Attention Relation Network (ARNe). ARNe combines features from the\nrecently introduced Transformer and the Wild Relation Network (WReN). We test\nARNe on the Procedurally Generated Matrices (PGMs) datasets for abstract visual\nreasoning. ARNe excels the WReN model on this task by 11.28 ppt. Relational\nconcepts between objects are efficiently learned demanding only 35% of the\ntraining samples to surpass reported accuracy of the base line model. Our\nproposed hybrid model, represents an alternative on learning abstract relations\nusing self-attention and demonstrates that the Transformer network is also well\nsuited for abstract visual reasoning.", "comment": null, "links": []}
{"entry_id": "1910.03343", "title": "Modulated Self-attention Convolutional Network for VQA", "authors": ["Jean-Benoit Delbrouck", "Antoine Maiorca", "Nathan Hubens", "Stéphane Dupont"], "published": "2019-10-08 11:28:38", "updated": "2019-10-31 16:59:23", "summary": "As new data-sets for real-world visual reasoning and compositional question\nanswering are emerging, it might be needed to use the visual feature extraction\nas a end-to-end process during training. This small contribution aims to\nsuggest new ideas to improve the visual processing of traditional convolutional\nnetwork for visual question answering (VQA). In this paper, we propose to\nmodulate by a linguistic input a CNN augmented with self-attention. We show\nencouraging relative improvements for future research in this direction.", "comment": "Accepted at NeurIPS 2019 workshop: ViGIL", "links": []}
{"entry_id": "1909.09065", "title": "Towards Explainable Neural-Symbolic Visual Reasoning", "authors": ["Adrien Bennetot", "Jean-Luc Laurent", "Raja Chatila", "Natalia Díaz-Rodríguez"], "published": "2019-09-19 16:04:57", "updated": "2019-10-22 15:23:49", "summary": "Many high-performance models suffer from a lack of interpretability. There\nhas been an increasing influx of work on explainable artificial intelligence\n(XAI) in order to disentangle what is meant and expected by XAI. Nevertheless,\nthere is no general consensus on how to produce and judge explanations. In this\npaper, we discuss why techniques integrating connectionist and symbolic\nparadigms are the most efficient solutions to produce explanations for\nnon-technical users and we propose a reasoning model, based on definitions by\nDoran et al. [2017] (arXiv:1710.00794) to explain a neural network's decision.\nWe use this explanation in order to correct bias in the network's decision\nrationale. We accompany this model with an example of its potential use, based\non the image captioning method in Burns et al. [2018] (arXiv:1803.09797).", "comment": "Accepted at IJCAI19 Neural-Symbolic Learning and Reasoning Workshop\n  (https://sites.google.com/view/nesy2019/home)", "links": []}
{"entry_id": "1812.03299", "title": "Learning to Assemble Neural Module Tree Networks for Visual Grounding", "authors": ["Daqing Liu", "Hanwang Zhang", "Feng Wu", "Zheng-Jun Zha"], "published": "2018-12-08 11:04:34", "updated": "2019-10-21 12:31:10", "summary": "Visual grounding, a task to ground (i.e., localize) natural language in\nimages, essentially requires composite visual reasoning. However, existing\nmethods over-simplify the composite nature of language into a monolithic\nsentence embedding or a coarse composition of subject-predicate-object triplet.\nIn this paper, we propose to ground natural language in an intuitive,\nexplainable, and composite fashion as it should be. In particular, we develop a\nnovel modular network called Neural Module Tree network (NMTree) that\nregularizes the visual grounding along the dependency parsing tree of the\nsentence, where each node is a neural module that calculates visual attention\naccording to its linguistic feature, and the grounding score is accumulated in\na bottom-up direction where as needed. NMTree disentangles the visual grounding\nfrom the composite reasoning, allowing the former to only focus on primitive\nand easy-to-generalize patterns. To reduce the impact of parsing errors, we\ntrain the modules and their assembly end-to-end by using the Gumbel-Softmax\napproximation and its straight-through gradient estimator, accounting for the\ndiscrete nature of module assembly. Overall, the proposed NMTree consistently\noutperforms the state-of-the-arts on several benchmarks. Qualitative results\nshow explainable grounding score calculation in great detail.", "comment": "Accepted at ICCV 2019 (Oral); Code available at\n  https://github.com/daqingliu/NMTree", "links": []}
{"entry_id": "1910.01833", "title": "Few-Shot Abstract Visual Reasoning With Spectral Features", "authors": ["Tanner Bohn", "Yining Hu", "Charles X. Ling"], "published": "2019-10-04 08:15:15", "updated": "2019-10-04 08:15:15", "summary": "We present an image preprocessing technique capable of improving the\nperformance of few-shot classifiers on abstract visual reasoning tasks. Many\nvisual reasoning tasks with abstract features are easy for humans to learn with\nfew examples but very difficult for computer vision approaches with the same\nnumber of samples, despite the ability for deep learning models to learn\nabstract features. Same-different (SD) problems represent a type of visual\nreasoning task requiring knowledge of pattern repetition within individual\nimages, and modern computer vision approaches have largely faltered on these\nclassification problems, even when provided with vast amounts of training data.\nWe propose a simple method for solving these problems based on the insight that\nremoving peaks from the amplitude spectrum of an image is capable of\nemphasizing the unique parts of the image. When combined with several\nclassifiers, our method performs well on the SD SVRT tasks with few-shot\nlearning, improving upon the best comparable results on all tasks, with average\nabsolute accuracy increases nearly 40% for some classifiers. In particular, we\nfind that combining Relational Networks with this image preprocessing approach\nimproves their performance from chance-level to over 90% accuracy on several SD\ntasks.", "comment": "11 pages, 3 figures", "links": []}
{"entry_id": "1909.08859", "title": "Procedural Reasoning Networks for Understanding Multimodal Procedures", "authors": ["Mustafa Sercan Amac", "Semih Yagcioglu", "Aykut Erdem", "Erkut Erdem"], "published": "2019-09-19 08:39:00", "updated": "2019-09-19 08:39:00", "summary": "This paper addresses the problem of comprehending procedural commonsense\nknowledge. This is a challenging task as it requires identifying key entities,\nkeeping track of their state changes, and understanding temporal and causal\nrelations. Contrary to most of the previous work, in this study, we do not rely\non strong inductive bias and explore the question of how multimodality can be\nexploited to provide a complementary semantic signal. Towards this end, we\nintroduce a new entity-aware neural comprehension model augmented with external\nrelational memory units. Our model learns to dynamically update entity states\nin relation to each other while reading the text instructions. Our experimental\nanalysis on the visual reasoning tasks in the recently proposed RecipeQA\ndataset reveals that our approach improves the accuracy of the previously\nreported models by a large margin. Moreover, we find that our model learns\neffective dynamic representations of entities even though we do not use any\nsupervision at the level of entity states.", "comment": "Accepted to CoNLL 2019. The project website with code and demo is\n  available at https://hucvl.github.io/prn/", "links": []}
{"entry_id": "1909.08164", "title": "Dynamic Graph Attention for Referring Expression Comprehension", "authors": ["Sibei Yang", "Guanbin Li", "Yizhou Yu"], "published": "2019-09-18 01:47:27", "updated": "2019-09-18 01:47:27", "summary": "Referring expression comprehension aims to locate the object instance\ndescribed by a natural language referring expression in an image. This task is\ncompositional and inherently requires visual reasoning on top of the\nrelationships among the objects in the image. Meanwhile, the visual reasoning\nprocess is guided by the linguistic structure of the referring expression.\nHowever, existing approaches treat the objects in isolation or only explore the\nfirst-order relationships between objects without being aligned with the\npotential complexity of the expression. Thus it is hard for them to adapt to\nthe grounding of complex referring expressions. In this paper, we explore the\nproblem of referring expression comprehension from the perspective of\nlanguage-driven visual reasoning, and propose a dynamic graph attention network\nto perform multi-step reasoning by modeling both the relationships among the\nobjects in the image and the linguistic structure of the expression. In\nparticular, we construct a graph for the image with the nodes and edges\ncorresponding to the objects and their relationships respectively, propose a\ndifferential analyzer to predict a language-guided visual reasoning process,\nand perform stepwise reasoning on top of the graph to update the compound\nobject representation at every node. Experimental results demonstrate that the\nproposed method can not only significantly surpass all existing\nstate-of-the-art algorithms across three common benchmark datasets, but also\ngenerate interpretable visual evidences for stepwisely locating the objects\nreferred to in complex language descriptions.", "comment": "Accepted as an Oral presentation at ICCV2019", "links": []}
{"entry_id": "1907.06794", "title": "2nd Place Solution to the GQA Challenge 2019", "authors": ["Shijie Geng", "Ji Zhang", "Hang Zhang", "Ahmed Elgammal", "Dimitris N. Metaxas"], "published": "2019-07-16 00:09:09", "updated": "2019-08-16 22:04:53", "summary": "We present a simple method that achieves unexpectedly superior performance\nfor Complex Reasoning involved Visual Question Answering. Our solution collects\nstatistical features from high-frequency words of all the questions asked about\nan image and use them as accurate knowledge for answering further questions of\nthe same image. We are fully aware that this setting is not ubiquitously\napplicable, and in a more common setting one should assume the questions are\nasked separately and they cannot be gathered to obtain a knowledge base.\nNonetheless, we use this method as an evidence to demonstrate our observation\nthat the bottleneck effect is more severe on the feature extraction part than\nit is on the knowledge reasoning part. We show significant gaps when using the\nsame reasoning model with 1) ground-truth features; 2) statistical features; 3)\ndetected features from completely learned detectors, and analyze what these\ngaps mean to researches on visual reasoning topics. Our model with the\nstatistical features achieves the 2nd place in the GQA Challenge 2019.", "comment": null, "links": []}
{"entry_id": "1907.12271", "title": "V-PROM: A Benchmark for Visual Reasoning Using Visual Progressive Matrices", "authors": ["Damien Teney", "Peng Wang", "Jiewei Cao", "Lingqiao Liu", "Chunhua Shen", "Anton van den Hengel"], "published": "2019-07-29 08:28:33", "updated": "2019-07-29 08:28:33", "summary": "One of the primary challenges faced by deep learning is the degree to which\ncurrent methods exploit superficial statistics and dataset bias, rather than\nlearning to generalise over the specific representations they have experienced.\nThis is a critical concern because generalisation enables robust reasoning over\nunseen data, whereas leveraging superficial statistics is fragile to even small\nchanges in data distribution. To illuminate the issue and drive progress\ntowards a solution, we propose a test that explicitly evaluates abstract\nreasoning over visual data. We introduce a large-scale benchmark of visual\nquestions that involve operations fundamental to many high-level vision tasks,\nsuch as comparisons of counts and logical operations on complex visual\nproperties. The benchmark directly measures a method's ability to infer\nhigh-level relationships and to generalise them over image-based concepts. It\nincludes multiple training/test splits that require controlled levels of\ngeneralization. We evaluate a range of deep learning architectures, and find\nthat existing models, including those popular for vision-and-language tasks,\nare unable to solve seemingly-simple instances. Models using relational\nnetworks fare better but leave substantial room for improvement.", "comment": null, "links": []}
{"entry_id": "1811.00491", "title": "A Corpus for Reasoning About Natural Language Grounded in Photographs", "authors": ["Alane Suhr", "Stephanie Zhou", "Ally Zhang", "Iris Zhang", "Huajun Bai", "Yoav Artzi"], "published": "2018-11-01 16:47:44", "updated": "2019-07-21 05:26:36", "summary": "We introduce a new dataset for joint reasoning about natural language and\nimages, with a focus on semantic diversity, compositionality, and visual\nreasoning challenges. The data contains 107,292 examples of English sentences\npaired with web photographs. The task is to determine whether a natural\nlanguage caption is true about a pair of photographs. We crowdsource the data\nusing sets of visually rich images and a compare-and-contrast task to elicit\nlinguistically diverse language. Qualitative analysis shows the data requires\ncompositional joint reasoning, including about quantities, comparisons, and\nrelations. Evaluation using state-of-the-art visual reasoning methods shows the\ndata presents a strong challenge.", "comment": "ACL 2019 Long Paper", "links": []}
{"entry_id": "1902.03380", "title": "When Causal Intervention Meets Adversarial Examples and Image Masking for Deep Neural Networks", "authors": ["Chao-Han Huck Yang", "Yi-Chieh Liu", "Pin-Yu Chen", "Xiaoli Ma", "Yi-Chang James Tsai"], "published": "2019-02-09 06:44:13", "updated": "2019-06-25 15:07:42", "summary": "Discovering and exploiting the causality in deep neural networks (DNNs) are\ncrucial challenges for understanding and reasoning causal effects (CE) on an\nexplainable visual model. \"Intervention\" has been widely used for recognizing a\ncausal relation ontologically. In this paper, we propose a causal inference\nframework for visual reasoning via do-calculus. To study the intervention\neffects on pixel-level features for causal reasoning, we introduce pixel-wise\nmasking and adversarial perturbation. In our framework, CE is calculated using\nfeatures in a latent space and perturbed prediction from a DNN-based model. We\nfurther provide the first look into the characteristics of discovered CE of\nadversarially perturbed images generated by gradient-based methods\n\\footnote{~~https://github.com/jjaacckkyy63/Causal-Intervention-AE-wAdvImg}.\nExperimental results show that CE is a competitive and robust index for\nunderstanding DNNs when compared with conventional methods such as\nclass-activation mappings (CAMs) on the Chest X-Ray-14 dataset for\nhuman-interpretable feature(s) (e.g., symptom) reasoning. Moreover, CE holds\npromises for detecting adversarial examples as it possesses distinct\ncharacteristics in the presence of adversarial perturbations.", "comment": "Noted our camera-ready version has changed the title. \"When Causal\n  Intervention Meets Adversarial Examples and Image Masking for Deep Neural\n  Networks\" as the v3 official paper title in IEEE Proceeding. Please use it in\n  your formal reference. Accepted at IEEE ICIP 2019. Pytorch code has released\n  on https://github.com/jjaacckkyy63/Causal-Intervention-AE-wAdvImg", "links": ["http://dx.doi.org/10.1109/ICIP.2019.8803554"]}
{"entry_id": "1905.10226", "title": "Deep Reason: A Strong Baseline for Real-World Visual Reasoning", "authors": ["Chenfei Wu", "Yanzhao Zhou", "Gen Li", "Nan Duan", "Duyu Tang", "Xiaojie Wang"], "published": "2019-05-24 13:34:21", "updated": "2019-06-17 15:26:58", "summary": "This paper presents a strong baseline for real-world visual reasoning (GQA),\nwhich achieves 60.93% in GQA 2019 challenge and won the sixth place. GQA is a\nlarge dataset with 22M questions involving spatial understanding and multi-step\ninference. To help further research in this area, we identified three crucial\nparts that improve the performance, namely: multi-source features, fine-grained\nencoder, and score-weighted ensemble. We provide a series of analysis on their\nimpact on performance.", "comment": "CVPR 2019 Visual Question Answering and Dialog Workshop", "links": []}
{"entry_id": "1906.01784", "title": "Learning to Compose and Reason with Language Tree Structures for Visual Grounding", "authors": ["Richang Hong", "Daqing Liu", "Xiaoyu Mo", "Xiangnan He", "Hanwang Zhang"], "published": "2019-06-05 02:03:55", "updated": "2019-06-05 02:03:55", "summary": "Grounding natural language in images, such as localizing \"the black dog on\nthe left of the tree\", is one of the core problems in artificial intelligence,\nas it needs to comprehend the fine-grained and compositional language space.\nHowever, existing solutions rely on the association between the holistic\nlanguage features and visual features, while neglect the nature of\ncompositional reasoning implied in the language. In this paper, we propose a\nnatural language grounding model that can automatically compose a binary tree\nstructure for parsing the language and then perform visual reasoning along the\ntree in a bottom-up fashion. We call our model RVG-TREE: Recursive Grounding\nTree, which is inspired by the intuition that any language expression can be\nrecursively decomposed into two constituent parts, and the grounding confidence\nscore can be recursively accumulated by calculating their grounding scores\nreturned by sub-trees. RVG-TREE can be trained end-to-end by using the\nStraight-Through Gumbel-Softmax estimator that allows the gradients from the\ncontinuous score functions passing through the discrete tree construction.\nExperiments on several benchmarks show that our model achieves the\nstate-of-the-art performance with more explainable reasoning.", "comment": "Accepted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (T-PAMI)", "links": ["http://dx.doi.org/10.1109/TPAMI.2019.2911066"]}
{"entry_id": "1905.08621", "title": "Shortest-Path-Preserving Rounding", "authors": ["Herman Haverkort", "David Kübel", "Elmar Langetepe"], "published": "2019-05-21 13:30:37", "updated": "2019-05-21 13:30:37", "summary": "Various applications of graphs, in particular applications related to finding\nshortest paths, naturally get inputs with real weights on the edges. However,\nfor algorithmic or visualization reasons, inputs with integer weights would\noften be preferable or even required. This raises the following question: given\nan undirected graph with non-negative real weights on the edges and an error\nthreshold $\\varepsilon$, how efficiently can we decide whether we can round all\nweights such that shortest paths are maintained, and the change of weight of\neach shortest path is less than $\\varepsilon$? So far, only for path-shaped\ngraphs a polynomial-time algorithm was known. In this paper we prove, by\nreduction from 3-SAT, that, in general, the problem is NP-hard. However, if the\ngraph is a tree with $n$ vertices, the problem can be solved in $O(n^2)$ time.", "comment": "20 pages, 5 figures, pre-print of an article presented at IWOCA 2019", "links": []}
{"entry_id": "1902.09506", "title": "GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering", "authors": ["Drew A. Hudson", "Christopher D. Manning"], "published": "2019-02-25 18:37:49", "updated": "2019-05-10 22:24:55", "summary": "We introduce GQA, a new dataset for real-world visual reasoning and\ncompositional question answering, seeking to address key shortcomings of\nprevious VQA datasets. We have developed a strong and robust question engine\nthat leverages scene graph structures to create 22M diverse reasoning\nquestions, all come with functional programs that represent their semantics. We\nuse the programs to gain tight control over the answer distribution and present\na new tunable smoothing technique to mitigate question biases. Accompanying the\ndataset is a suite of new metrics that evaluate essential qualities such as\nconsistency, grounding and plausibility. An extensive analysis is performed for\nbaselines as well as state-of-the-art models, providing fine-grained results\nfor different question types and topologies. Whereas a blind LSTM obtains mere\n42.1%, and strong VQA models achieve 54.1%, human performance tops at 89.3%,\noffering ample opportunity for new research to explore. We strongly hope GQA\nwill provide an enabling resource for the next generation of models with\nenhanced robustness, improved consistency, and deeper semantic understanding\nfor images and language.", "comment": "Published as a conference paper at CVPR 2019 (oral)", "links": []}
{"entry_id": "1904.08608", "title": "Learning to Collocate Neural Modules for Image Captioning", "authors": ["Xu Yang", "Hanwang Zhang", "Jianfei Cai"], "published": "2019-04-18 07:03:19", "updated": "2019-04-18 07:03:19", "summary": "We do not speak word by word from scratch; our brain quickly structures a\npattern like \\textsc{sth do sth at someplace} and then fill in the detailed\ndescriptions. To render existing encoder-decoder image captioners such\nhuman-like reasoning, we propose a novel framework: learning to Collocate\nNeural Modules (CNM), to generate the `inner pattern' connecting visual encoder\nand language decoder. Unlike the widely-used neural module networks in visual\nQ\\&A, where the language (ie, question) is fully observable, CNM for captioning\nis more challenging as the language is being generated and thus is partially\nobservable. To this end, we make the following technical contributions for CNM\ntraining: 1) compact module design --- one for function words and three for\nvisual content words (eg, noun, adjective, and verb), 2) soft module fusion and\nmulti-step module execution, robustifying the visual reasoning in partial\nobservation, 3) a linguistic loss for module controller being faithful to\npart-of-speech collocations (eg, adjective is before noun). Extensive\nexperiments on the challenging MS-COCO image captioning benchmark validate the\neffectiveness of our CNM image captioner. In particular, CNM achieves a new\nstate-of-the-art 127.9 CIDEr-D on Karpathy split and a single-model 126.0 c40\non the official server. CNM is also robust to few training samples, eg, by\ntraining only one sentence per image, CNM can halve the performance loss\ncompared to a strong baseline.", "comment": null, "links": []}
{"entry_id": "1901.00850", "title": "CLEVR-Ref+: Diagnosing Visual Reasoning with Referring Expressions", "authors": ["Runtao Liu", "Chenxi Liu", "Yutong Bai", "Alan Yuille"], "published": "2019-01-03 18:58:06", "updated": "2019-04-06 19:59:25", "summary": "Referring object detection and referring image segmentation are important\ntasks that require joint understanding of visual information and natural\nlanguage. Yet there has been evidence that current benchmark datasets suffer\nfrom bias, and current state-of-the-art models cannot be easily evaluated on\ntheir intermediate reasoning process. To address these issues and complement\nsimilar efforts in visual question answering, we build CLEVR-Ref+, a synthetic\ndiagnostic dataset for referring expression comprehension. The precise\nlocations and attributes of the objects are readily available, and the\nreferring expressions are automatically associated with functional programs.\nThe synthetic nature allows control over dataset bias (through sampling\nstrategy), and the modular programs enable intermediate reasoning ground truth\nwithout human annotators.\n  In addition to evaluating several state-of-the-art models on CLEVR-Ref+, we\nalso propose IEP-Ref, a module network approach that significantly outperforms\nother models on our dataset. In particular, we present two interesting and\nimportant findings using IEP-Ref: (1) the module trained to transform feature\nmaps into segmentation masks can be attached to any intermediate module to\nreveal the entire reasoning process step-by-step; (2) even if all training data\nhas at least one object referred, IEP-Ref can correctly predict no-foreground\nwhen presented with false-premise referring expressions. To the best of our\nknowledge, this is the first direct and quantitative proof that neural modules\nbehave in the way they are intended.", "comment": "To appear in CVPR 2019. All data and code concerning CLEVR-Ref+ and\n  IEP-Ref have been released at https://cs.jhu.edu/~cxliu/2019/clevr-ref+", "links": []}
{"entry_id": "1812.01855", "title": "Explainable and Explicit Visual Reasoning over Scene Graphs", "authors": ["Jiaxin Shi", "Hanwang Zhang", "Juanzi Li"], "published": "2018-12-05 08:35:05", "updated": "2019-03-19 12:55:14", "summary": "We aim to dismantle the prevalent black-box neural architectures used in\ncomplex visual reasoning tasks, into the proposed eXplainable and eXplicit\nNeural Modules (XNMs), which advance beyond existing neural module networks\ntowards using scene graphs --- objects as nodes and the pairwise relationships\nas edges --- for explainable and explicit reasoning with structured knowledge.\nXNMs allow us to pay more attention to teach machines how to \"think\",\nregardless of what they \"look\". As we will show in the paper, by using scene\ngraphs as an inductive bias, 1) we can design XNMs in a concise and flexible\nfashion, i.e., XNMs merely consist of 4 meta-types, which significantly reduce\nthe number of parameters by 10 to 100 times, and 2) we can explicitly trace the\nreasoning-flow in terms of graph attentions. XNMs are so generic that they\nsupport a wide range of scene graph implementations with various qualities. For\nexample, when the graphs are detected perfectly, XNMs achieve 100% accuracy on\nboth CLEVR and CLEVR CoGenT, establishing an empirical performance upper-bound\nfor visual reasoning; when the graphs are noisily detected from real-world\nimages, XNMs are still robust to achieve a competitive 67.5% accuracy on\nVQAv2.0, surpassing the popular bag-of-objects attention models without graph\nstructures.", "comment": "CVPR2019", "links": []}
{"entry_id": "1711.05240", "title": "Weakly-supervised Semantic Parsing with Abstract Examples", "authors": ["Omer Goldman", "Veronica Latcinnik", "Udi Naveh", "Amir Globerson", "Jonathan Berant"], "published": "2017-11-14 18:29:05", "updated": "2019-03-13 09:30:38", "summary": "Training semantic parsers from weak supervision (denotations) rather than\nstrong supervision (programs) complicates training in two ways. First, a large\nsearch space of potential programs needs to be explored at training time to\nfind a correct program. Second, spurious programs that accidentally lead to a\ncorrect denotation add noise to training. In this work we propose that in\nclosed worlds with clear semantic types, one can substantially alleviate these\nproblems by utilizing an abstract representation, where tokens in both the\nlanguage utterance and program are lifted to an abstract form. We show that\nthese abstractions can be defined with a handful of lexical rules and that they\nresult in sharing between different examples that alleviates the difficulties\nin training. To test our approach, we develop the first semantic parser for\nCNLVR, a challenging visual reasoning dataset, where the search space is large\nand overcoming spuriousness is critical, because denotations are either TRUE or\nFALSE, and thus random programs are likely to lead to a correct denotation. Our\nmethod substantially improves performance, and reaches 82.5% accuracy, a 14.7%\nabsolute accuracy improvement compared to the best reported accuracy so far.", "comment": "CNLVR,NLVR. Accepted to ACL 2018", "links": []}
{"entry_id": "1903.02741", "title": "RAVEN: A Dataset for Relational and Analogical Visual rEasoNing", "authors": ["Chi Zhang", "Feng Gao", "Baoxiong Jia", "Yixin Zhu", "Song-Chun Zhu"], "published": "2019-03-07 06:28:44", "updated": "2019-03-07 06:28:44", "summary": "Dramatic progress has been witnessed in basic vision tasks involving\nlow-level perception, such as object recognition, detection, and tracking.\nUnfortunately, there is still an enormous performance gap between artificial\nvision systems and human intelligence in terms of higher-level vision problems,\nespecially ones involving reasoning. Earlier attempts in equipping machines\nwith high-level reasoning have hovered around Visual Question Answering (VQA),\none typical task associating vision and language understanding. In this work,\nwe propose a new dataset, built in the context of Raven's Progressive Matrices\n(RPM) and aimed at lifting machine intelligence by associating vision with\nstructural, relational, and analogical reasoning in a hierarchical\nrepresentation. Unlike previous works in measuring abstract reasoning using\nRPM, we establish a semantic link between vision and reasoning by providing\nstructure representation. This addition enables a new type of abstract\nreasoning by jointly operating on the structure representation. Machine\nreasoning ability using modern computer vision is evaluated in this newly\nproposed dataset. Additionally, we also provide human performance as a\nreference. Finally, we show consistent improvement across all models by\nincorporating a simple neural module that combines visual understanding and\nstructure reasoning.", "comment": "CVPR 2019 paper. Supplementary:\n  http://wellyzhang.github.io/attach/cvpr19zhang_supp.pdf Project:\n  http://wellyzhang.github.io/project/raven.html", "links": []}
{"entry_id": "1902.11280", "title": "From Visual to Acoustic Question Answering", "authors": ["Jerome Abdelnour", "Giampiero Salvi", "Jean Rouat"], "published": "2019-02-28 18:35:45", "updated": "2019-02-28 18:35:45", "summary": "We introduce the new task of Acoustic Question Answering (AQA) to promote\nresearch in acoustic reasoning. The AQA task consists of analyzing an acoustic\nscene composed by a combination of elementary sounds and answering questions\nthat relate the position and properties of these sounds. The kind of relational\nquestions asked, require that the models perform non-trivial reasoning in order\nto answer correctly. Although similar problems have been extensively studied in\nthe domain of visual reasoning, we are not aware of any previous studies\naddressing the problem in the acoustic domain. We propose a method for\ngenerating the acoustic scenes from elementary sounds and a number of relevant\nquestions for each scene using templates. We also present preliminary results\nobtained with two models (FiLM and MAC) that have been shown to work for visual\nreasoning.", "comment": null, "links": []}
{"entry_id": "1902.04955", "title": "Can We Automate Diagrammatic Reasoning?", "authors": ["Sk. Arif Ahmed", "Debi Prosad Dogra", "Samarjit Kar", "Partha Pratim Roy", "Dilip K. Prasad"], "published": "2019-02-13 15:43:11", "updated": "2019-02-13 15:43:11", "summary": "Learning to solve diagrammatic reasoning (DR) can be a challenging but\ninteresting problem to the computer vision research community. It is believed\nthat next generation pattern recognition applications should be able to\nsimulate human brain to understand and analyze reasoning of images. However,\ndue to the lack of benchmarks of diagrammatic reasoning, the present research\nprimarily focuses on visual reasoning that can be applied to real-world\nobjects. In this paper, we present a diagrammatic reasoning dataset that\nprovides a large variety of DR problems. In addition, we also propose a\nKnowledge-based Long Short Term Memory (KLSTM) to solve diagrammatic reasoning\nproblems. Our proposed analysis is arguably the first work in this research\narea. Several state-of-the-art learning frameworks have been used to compare\nwith the proposed KLSTM framework in the present context. Preliminary results\nindicate that the domain is highly related to computer vision and pattern\nrecognition research with several challenging avenues.", "comment": null, "links": []}
{"entry_id": "1901.06706", "title": "Visual Entailment: A Novel Task for Fine-Grained Image Understanding", "authors": ["Ning Xie", "Farley Lai", "Derek Doran", "Asim Kadav"], "published": "2019-01-20 17:55:05", "updated": "2019-01-20 17:55:05", "summary": "Existing visual reasoning datasets such as Visual Question Answering (VQA),\noften suffer from biases conditioned on the question, image or answer\ndistributions. The recently proposed CLEVR dataset addresses these limitations\nand requires fine-grained reasoning but the dataset is synthetic and consists\nof similar objects and sentence structures across the dataset.\n  In this paper, we introduce a new inference task, Visual Entailment (VE) -\nconsisting of image-sentence pairs whereby a premise is defined by an image,\nrather than a natural language sentence as in traditional Textual Entailment\ntasks. The goal of a trained VE model is to predict whether the image\nsemantically entails the text. To realize this task, we build a dataset SNLI-VE\nbased on the Stanford Natural Language Inference corpus and Flickr30k dataset.\nWe evaluate various existing VQA baselines and build a model called Explainable\nVisual Entailment (EVE) system to address the VE task. EVE achieves up to 71%\naccuracy and outperforms several other state-of-the-art VQA based models.\nFinally, we demonstrate the explainability of EVE through cross-modal attention\nvisualizations. The SNLI-VE dataset is publicly available at\nhttps://github.com/ necla-ml/SNLI-VE.", "comment": null, "links": []}
{"entry_id": "1901.05574", "title": "Visual Reasoning of Feature Attribution with Deep Recurrent Neural Networks", "authors": ["Chuan Wang", "Takeshi Onishi", "Keiichi Nemoto", "Kwan-Liu Ma"], "published": "2019-01-17 00:57:33", "updated": "2019-01-17 00:57:33", "summary": "Deep Recurrent Neural Network (RNN) has gained popularity in many sequence\nclassification tasks. Beyond predicting a correct class for each data instance,\ndata scientists also want to understand what differentiating factors in the\ndata have contributed to the classification during the learning process. We\npresent a visual analytics approach to facilitate this task by revealing the\nRNN attention for all data instances, their temporal positions in the\nsequences, and the attribution of variables at each value level. We demonstrate\nwith real-world datasets that our approach can help data scientists to\nunderstand such dynamics in deep RNNs from the training results, hence guiding\ntheir modeling process.", "comment": null, "links": []}
{"entry_id": "1812.03631", "title": "Spatial Knowledge Distillation to aid Visual Reasoning", "authors": ["Somak Aditya", "Rudra Saha", "Yezhou Yang", "Chitta Baral"], "published": "2018-12-10 05:36:23", "updated": "2018-12-11 16:42:29", "summary": "For tasks involving language and vision, the current state-of-the-art methods\ntend not to leverage any additional information that might be present to gather\nrelevant (commonsense) knowledge. A representative task is Visual Question\nAnswering where large diagnostic datasets have been proposed to test a system's\ncapability of answering questions about images. The training data is often\naccompanied by annotations of individual object properties and spatial\nlocations. In this work, we take a step towards integrating this additional\nprivileged information in the form of spatial knowledge to aid in visual\nreasoning. We propose a framework that combines recent advances in knowledge\ndistillation (teacher-student framework), relational reasoning and\nprobabilistic logical languages to incorporate such knowledge in existing\nneural networks for the task of Visual Question Answering. Specifically, for a\nquestion posed against an image, we use a probabilistic logical language to\nencode the spatial knowledge and the spatial understanding about the question\nin the form of a mask that is directly provided to the teacher network. The\nstudent network learns from the ground-truth information as well as the\nteachers prediction via distillation. We also demonstrate the impact of\npredicting such a mask inside the teachers network using attention.\nEmpirically, we show that both the methods improve the test accuracy over a\nstate-of-the-art approach on a publicly available dataset.", "comment": "Equal contribution by first two authors. Accepted in WACV 2019", "links": []}
{"entry_id": "1812.01880", "title": "Learning to Compose Dynamic Tree Structures for Visual Contexts", "authors": ["Kaihua Tang", "Hanwang Zhang", "Baoyuan Wu", "Wenhan Luo", "Wei Liu"], "published": "2018-12-05 09:51:19", "updated": "2018-12-05 09:51:19", "summary": "We propose to compose dynamic tree structures that place the objects in an\nimage into a visual context, helping visual reasoning tasks such as scene graph\ngeneration and visual Q&A. Our visual context tree model, dubbed VCTree, has\ntwo key advantages over existing structured object representations including\nchains and fully-connected graphs: 1) The efficient and expressive binary tree\nencodes the inherent parallel/hierarchical relationships among objects, e.g.,\n\"clothes\" and \"pants\" are usually co-occur and belong to \"person\"; 2) the\ndynamic structure varies from image to image and task to task, allowing more\ncontent-/task-specific message passing among objects. To construct a VCTree, we\ndesign a score function that calculates the task-dependent validity between\neach object pair, and the tree is the binary version of the maximum spanning\ntree from the score matrix. Then, visual contexts are encoded by bidirectional\nTreeLSTM and decoded by task-specific models. We develop a hybrid learning\nprocedure which integrates end-task supervised learning and the tree structure\nreinforcement learning, where the former's evaluation result serves as a\nself-critic for the latter's structure exploration. Experimental results on two\nbenchmarks, which require reasoning over contexts: Visual Genome for scene\ngraph generation and VQA2.0 for visual Q&A, show that VCTree outperforms\nstate-of-the-art results while discovering interpretable visual context\nstructures.", "comment": null, "links": []}
{"entry_id": "1710.09490", "title": "Complete 3D Scene Parsing from an RGBD Image", "authors": ["Chuhang Zou", "Ruiqi Guo", "Zhizhong Li", "Derek Hoiem"], "published": "2017-10-25 23:04:14", "updated": "2018-11-13 18:05:14", "summary": "One major goal of vision is to infer physical models of objects, surfaces,\nand their layout from sensors. In this paper, we aim to interpret indoor scenes\nfrom one RGBD image. Our representation encodes the layout of orthogonal walls\nand the extent of objects, modeled with CAD-like 3D shapes. We parse both the\nvisible and occluded portions of the scene and all observable objects,\nproducing a complete 3D parse. Such a scene interpretation is useful for\nrobotics and visual reasoning, but difficult to produce due to the well-known\nchallenge of segmentation, the high degree of occlusion, and the diversity of\nobjects in indoor scenes. We take a data-driven approach, generating sets of\npotential object regions, matching to regions in training images, and\ntransferring and aligning associated 3D models while encouraging fit to\nobservations and spatial consistency. We use support inference to aid\ninterpretation and propose a retrieval scheme that uses convolutional neural\nnetworks (CNNs) to classify regions and retrieve objects with similar shapes.\nWe demonstrate the performance of our method on our newly annotated NYUd v2\ndataset with detailed 3D shapes.", "comment": "Accepted to International Journal of Computer Vision (IJCV), 2018\n  arXiv admin note: text overlap with arXiv:1504.02437", "links": []}
{"entry_id": "1808.04446", "title": "Visual Reasoning with Multi-hop Feature Modulation", "authors": ["Florian Strub", "Mathieu Seurin", "Ethan Perez", "Harm de Vries", "Jérémie Mary", "Philippe Preux", "Aaron Courville", "Olivier Pietquin"], "published": "2018-08-03 14:32:02", "updated": "2018-10-12 11:36:42", "summary": "Recent breakthroughs in computer vision and natural language processing have\nspurred interest in challenging multi-modal tasks such as visual\nquestion-answering and visual dialogue. For such tasks, one successful approach\nis to condition image-based convolutional network computation on language via\nFeature-wise Linear Modulation (FiLM) layers, i.e., per-channel scaling and\nshifting. We propose to generate the parameters of FiLM layers going up the\nhierarchy of a convolutional network in a multi-hop fashion rather than all at\nonce, as in prior work. By alternating between attending to the language input\nand generating FiLM layer parameters, this approach is better able to scale to\nsettings with longer input sequences such as dialogue. We demonstrate that\nmulti-hop FiLM generation achieves state-of-the-art for the short input\nsequence task ReferIt --- on-par with single-hop FiLM generation --- while also\nsignificantly outperforming prior state-of-the-art and single-hop FiLM\ngeneration on the GuessWhat?! visual dialogue task.", "comment": "In Proc of ECCV 2018", "links": []}
{"entry_id": "1808.09132", "title": "Mapping Natural Language Commands to Web Elements", "authors": ["Panupong Pasupat", "Tian-Shun Jiang", "Evan Zheran Liu", "Kelvin Guu", "Percy Liang"], "published": "2018-08-28 06:09:39", "updated": "2018-10-01 03:22:58", "summary": "The web provides a rich, open-domain environment with textual, structural,\nand spatial properties. We propose a new task for grounding language in this\nenvironment: given a natural language command (e.g., \"click on the second\narticle\"), choose the correct element on the web page (e.g., a hyperlink or\ntext box). We collected a dataset of over 50,000 commands that capture various\nphenomena such as functional references (e.g. \"find who made this site\"),\nrelational reasoning (e.g. \"article by john\"), and visual reasoning (e.g.\n\"top-most article\"). We also implemented and analyzed three baseline models\nthat capture different phenomena present in the dataset.", "comment": "EMNLP 2018", "links": []}
{"entry_id": "1806.02453", "title": "Visual Reasoning by Progressive Module Networks", "authors": ["Seung Wook Kim", "Makarand Tapaswi", "Sanja Fidler"], "published": "2018-06-06 23:02:35", "updated": "2018-09-27 18:09:38", "summary": "Humans learn to solve tasks of increasing complexity by building on top of\npreviously acquired knowledge. Typically, there exists a natural progression in\nthe tasks that we learn - most do not require completely independent solutions,\nbut can be broken down into simpler subtasks. We propose to represent a solver\nfor each task as a neural module that calls existing modules (solvers for\nsimpler tasks) in a functional program-like manner. Lower modules are a black\nbox to the calling module, and communicate only via a query and an output.\nThus, a module for a new task learns to query existing modules and composes\ntheir outputs in order to produce its own output. Our model effectively\ncombines previous skill-sets, does not suffer from forgetting, and is fully\ndifferentiable. We test our model in learning a set of visual reasoning tasks,\nand demonstrate improved performances in all tasks by learning progressively.\nBy evaluating the reasoning process using human judges, we show that our model\nis more interpretable than an attention-based baseline.", "comment": "17 pages, 5 figures", "links": []}
{"entry_id": "1806.06157", "title": "Object Level Visual Reasoning in Videos", "authors": ["Fabien Baradel", "Natalia Neverova", "Christian Wolf", "Julien Mille", "Greg Mori"], "published": "2018-06-16 00:33:50", "updated": "2018-09-20 08:59:32", "summary": "Human activity recognition is typically addressed by detecting key concepts\nlike global and local motion, features related to object classes present in the\nscene, as well as features related to the global context. The next open\nchallenges in activity recognition require a level of understanding that pushes\nbeyond this and call for models with capabilities for fine distinction and\ndetailed comprehension of interactions between actors and objects in a scene.\nWe propose a model capable of learning to reason about semantically meaningful\nspatiotemporal interactions in videos. The key to our approach is a choice of\nperforming this reasoning at the object level through the integration of state\nof the art object detection networks. This allows the model to learn detailed\nspatial interactions that exist at a semantic, object-interaction relevant\nlevel. We evaluate our method on three standard datasets (Twenty-BN\nSomething-Something, VLOG and EPIC Kitchens) and achieve state of the art\nresults on all of them. Finally, we show visualizations of the interactions\nlearned by the model, which illustrate object classes and their interactions\ncorresponding to different activity classes.", "comment": "Accepted at ECCV 2018 - long version (16 pages + ref)", "links": []}
{"entry_id": "1804.06870", "title": "Object Ordering with Bidirectional Matchings for Visual Reasoning", "authors": ["Hao Tan", "Mohit Bansal"], "published": "2018-04-18 18:39:17", "updated": "2018-09-06 16:56:32", "summary": "Visual reasoning with compositional natural language instructions, e.g.,\nbased on the newly-released Cornell Natural Language Visual Reasoning (NLVR)\ndataset, is a challenging task, where the model needs to have the ability to\ncreate an accurate mapping between the diverse phrases and the several objects\nplaced in complex arrangements in the image. Further, this mapping needs to be\nprocessed to answer the question in the statement given the ordering and\nrelationship of the objects across three similar images. In this paper, we\npropose a novel end-to-end neural model for the NLVR task, where we first use\njoint bidirectional attention to build a two-way conditioning between the\nvisual information and the language phrases. Next, we use an RL-based pointer\nnetwork to sort and process the varying number of unordered objects (so as to\nmatch the order of the statement phrases) in each of the three images and then\npool over the three decisions. Our model achieves strong improvements (of 4-6%\nabsolute) over the state-of-the-art on both the structured representation and\nraw image versions of the dataset.", "comment": "NAACL 2018 (8 pages; added pointer-ordering examples)", "links": []}
{"entry_id": "1809.01943", "title": "Cascaded Mutual Modulation for Visual Reasoning", "authors": ["Yiqun Yao", "Jiaming Xu", "Feng Wang", "Bo Xu"], "published": "2018-09-06 12:26:24", "updated": "2018-09-06 12:26:24", "summary": "Visual reasoning is a special visual question answering problem that is\nmulti-step and compositional by nature, and also requires intensive text-vision\ninteractions. We propose CMM: Cascaded Mutual Modulation as a novel end-to-end\nvisual reasoning model. CMM includes a multi-step comprehension process for\nboth question and image. In each step, we use a Feature-wise Linear Modulation\n(FiLM) technique to enable textual/visual pipeline to mutually control each\nother. Experiments show that CMM significantly outperforms most related models,\nand reach state-of-the-arts on two visual reasoning benchmarks: CLEVR and NLVR,\ncollected from both synthetic and natural languages. Ablation studies confirm\nthat both our multistep framework and our visual-guided language modulation are\ncritical to the task. Our code is available at\nhttps://github.com/FlamingHorizon/CMM-VR.", "comment": "to appear in EMNLP 2018", "links": []}
{"entry_id": "1808.09068", "title": "WeSeer: Visual Analysis for Better Information Cascade Prediction of WeChat Articles", "authors": ["Quan Li", "Ziming Wu", "Lingling Yi", "Kristanto Sean N", "Huamin Qu", "Xiaojuan Ma"], "published": "2018-08-28 00:09:20", "updated": "2018-08-28 00:09:20", "summary": "Social media, such as Facebook and WeChat, empowers millions of users to\ncreate, consume, and disseminate online information on an unprecedented scale.\nThe abundant information on social media intensifies the competition of WeChat\nPublic Official Articles (i.e., posts) for gaining user attention due to the\nzero-sum nature of attention. Therefore, only a small portion of information\ntends to become extremely popular while the rest remains unnoticed or quickly\ndisappears. Such a typical `long-tail' phenomenon is very common in social\nmedia. Thus, recent years have witnessed a growing interest in predicting the\nfuture trend in the popularity of social media posts and understanding the\nfactors that influence the popularity of the posts. Nevertheless, existing\npredictive models either rely on cumbersome feature engineering or\nsophisticated parameter tuning, which are difficult to understand and improve.\nIn this paper, we study and enhance a point process-based model by\nincorporating visual reasoning to support communication between the users and\nthe predictive model for a better prediction result. The proposed system\nsupports users to uncover the working mechanism behind the model and improve\nthe prediction accuracy accordingly based on the insights gained. We use\nrealistic WeChat articles to demonstrate the effectiveness of the system and\nverify the improved model on a large scale of WeChat articles. We also elicit\nand summarize the feedback from WeChat domain experts.", "comment": "IEEE Transactions on Visualization and Computer Graphics (TVCG), 2019\n  (To appear)", "links": []}
{"entry_id": "1803.06092", "title": "A Dataset and Architecture for Visual Reasoning with a Working Memory", "authors": ["Guangyu Robert Yang", "Igor Ganichev", "Xiao-Jing Wang", "Jonathon Shlens", "David Sussillo"], "published": "2018-03-16 06:53:45", "updated": "2018-07-20 14:12:49", "summary": "A vexing problem in artificial intelligence is reasoning about events that\noccur in complex, changing visual stimuli such as in video analysis or game\nplay. Inspired by a rich tradition of visual reasoning and memory in cognitive\npsychology and neuroscience, we developed an artificial, configurable visual\nquestion and answer dataset (COG) to parallel experiments in humans and\nanimals. COG is much simpler than the general problem of video analysis, yet it\naddresses many of the problems relating to visual and logical reasoning and\nmemory -- problems that remain challenging for modern deep learning\narchitectures. We additionally propose a deep learning architecture that\nperforms competitively on other diagnostic VQA datasets (i.e. CLEVR) as well as\neasy settings of the COG dataset. However, several settings of COG result in\ndatasets that are progressively more challenging to learn. After training, the\nnetwork can zero-shot generalize to many new tasks. Preliminary analyses of the\nnetwork architectures trained on COG demonstrate that the network accomplishes\nthe task in a manner interpretable to humans.", "comment": null, "links": []}
{"entry_id": "1803.05268", "title": "Transparency by Design: Closing the Gap Between Performance and Interpretability in Visual Reasoning", "authors": ["David Mascharka", "Philip Tran", "Ryan Soklaski", "Arjun Majumdar"], "published": "2018-03-14 13:33:06", "updated": "2018-07-02 18:48:31", "summary": "Visual question answering requires high-order reasoning about an image, which\nis a fundamental capability needed by machine systems to follow complex\ndirectives. Recently, modular networks have been shown to be an effective\nframework for performing visual reasoning tasks. While modular networks were\ninitially designed with a degree of model transparency, their performance on\ncomplex visual reasoning benchmarks was lacking. Current state-of-the-art\napproaches do not provide an effective mechanism for understanding the\nreasoning process. In this paper, we close the performance gap between\ninterpretable models and state-of-the-art visual reasoning methods. We propose\na set of visual-reasoning primitives which, when composed, manifest as a model\ncapable of performing complex reasoning tasks in an explicitly-interpretable\nmanner. The fidelity and interpretability of the primitives' outputs enable an\nunparalleled ability to diagnose the strengths and weaknesses of the resulting\nmodel. Critically, we show that these primitives are highly performant,\nachieving state-of-the-art accuracy of 99.1% on the CLEVR dataset. We also show\nthat our model is able to effectively learn generalized representations when\nprovided a small amount of data containing novel object attributes. Using the\nCoGenT generalization task, we show more than a 20 percentage point improvement\nover the current state of the art.", "comment": "CVPR 2018 pre-print", "links": ["http://dx.doi.org/10.1109/CVPR.2018.00519"]}
{"entry_id": "1806.06765", "title": "Modularity Matters: Learning Invariant Relational Reasoning Tasks", "authors": ["Jason Jo", "Vikas Verma", "Yoshua Bengio"], "published": "2018-06-18 15:19:04", "updated": "2018-06-18 15:19:04", "summary": "We focus on two supervised visual reasoning tasks whose labels encode a\nsemantic relational rule between two or more objects in an image: the MNIST\nParity task and the colorized Pentomino task. The objects in the images undergo\nrandom translation, scaling, rotation and coloring transformations. Thus these\ntasks involve invariant relational reasoning. We report uneven performance of\nvarious deep CNN models on these two tasks. For the MNIST Parity task, we\nreport that the VGG19 model soundly outperforms a family of ResNet models.\nMoreover, the family of ResNet models exhibits a general sensitivity to random\ninitialization for the MNIST Parity task. For the colorized Pentomino task, now\nboth the VGG19 and ResNet models exhibit sluggish optimization and very poor\ntest generalization, hovering around 30% test error. The CNN we tested all\nlearn hierarchies of fully distributed features and thus encode the distributed\nrepresentation prior. We are motivated by a hypothesis from cognitive\nneuroscience which posits that the human visual cortex is modularized, and this\nallows the visual cortex to learn higher order invariances. To this end, we\nconsider a modularized variant of the ResNet model, referred to as a Residual\nMixture Network (ResMixNet) which employs a mixture-of-experts architecture to\ninterleave distributed representations with more specialized, modular\nrepresentations. We show that very shallow ResMixNets are capable of learning\neach of the two tasks well, attaining less than 2% and 1% test error on the\nMNIST Parity and the colorized Pentomino tasks respectively. Most importantly,\nthe ResMixNet models are extremely parameter efficient: generalizing better\nthan various non-modular CNNs that have over 10x the number of parameters.\nThese experimental results support the hypothesis that modularity is a robust\nprior for learning invariant relational reasoning.", "comment": "Modified abstract to fit arXiv character limit", "links": []}
{"entry_id": "1712.07576", "title": "Learning to Act Properly: Predicting and Explaining Affordances from Images", "authors": ["Ching-Yao Chuang", "Jiaman Li", "Antonio Torralba", "Sanja Fidler"], "published": "2017-12-20 16:54:09", "updated": "2018-06-15 05:26:46", "summary": "We address the problem of affordance reasoning in diverse scenes that appear\nin the real world. Affordances relate the agent's actions to their effects when\ntaken on the surrounding objects. In our work, we take the egocentric view of\nthe scene, and aim to reason about action-object affordances that respect both\nthe physical world as well as the social norms imposed by the society. We also\naim to teach artificial agents why some actions should not be taken in certain\nsituations, and what would likely happen if these actions would be taken. We\ncollect a new dataset that builds upon ADE20k, referred to as ADE-Affordance,\nwhich contains annotations enabling such rich visual reasoning. We propose a\nmodel that exploits Graph Neural Networks to propagate contextual information\nfrom the scene in order to perform detailed affordance reasoning about each\nobject. Our model is showcased through various ablation studies, pointing to\nsuccesses and challenges in this complex task.", "comment": null, "links": []}
{"entry_id": "1711.06526", "title": "Multi-Label Zero-Shot Learning with Structured Knowledge Graphs", "authors": ["Chung-Wei Lee", "Wei Fang", "Chih-Kuan Yeh", "Yu-Chiang Frank Wang"], "published": "2017-11-17 13:31:57", "updated": "2018-05-26 12:48:10", "summary": "In this paper, we propose a novel deep learning architecture for multi-label\nzero-shot learning (ML-ZSL), which is able to predict multiple unseen class\nlabels for each input instance. Inspired by the way humans utilize semantic\nknowledge between objects of interests, we propose a framework that\nincorporates knowledge graphs for describing the relationships between multiple\nlabels. Our model learns an information propagation mechanism from the semantic\nlabel space, which can be applied to model the interdependencies between seen\nand unseen class labels. With such investigation of structured knowledge graphs\nfor visual reasoning, we show that our model can be applied for solving\nmulti-label classification and ML-ZSL tasks. Compared to state-of-the-art\napproaches, comparable or improved performances can be achieved by our method.", "comment": "CVPR 2018", "links": []}
{"entry_id": "1802.03390", "title": "Same-different problems strain convolutional neural networks", "authors": ["Matthew Ricci", "Junkyung Kim", "Thomas Serre"], "published": "2018-02-09 18:55:34", "updated": "2018-05-25 17:00:23", "summary": "The robust and efficient recognition of visual relations in images is a\nhallmark of biological vision. We argue that, despite recent progress in visual\nrecognition, modern machine vision algorithms are severely limited in their\nability to learn visual relations. Through controlled experiments, we\ndemonstrate that visual-relation problems strain convolutional neural networks\n(CNNs). The networks eventually break altogether when rote memorization becomes\nimpossible, as when intra-class variability exceeds network capacity. Motivated\nby the comparable success of biological vision, we argue that feedback\nmechanisms including attention and perceptual grouping may be the key\ncomputational components underlying abstract visual reasoning.\\", "comment": "6 Pages, 4 Figures", "links": []}
{"entry_id": "1803.03067", "title": "Compositional Attention Networks for Machine Reasoning", "authors": ["Drew A. Hudson", "Christopher D. Manning"], "published": "2018-03-08 12:37:14", "updated": "2018-04-24 10:25:07", "summary": "We present the MAC network, a novel fully differentiable neural network\narchitecture, designed to facilitate explicit and expressive reasoning. MAC\nmoves away from monolithic black-box neural architectures towards a design that\nencourages both transparency and versatility. The model approaches problems by\ndecomposing them into a series of attention-based reasoning steps, each\nperformed by a novel recurrent Memory, Attention, and Composition (MAC) cell\nthat maintains a separation between control and memory. By stringing the cells\ntogether and imposing structural constraints that regulate their interaction,\nMAC effectively learns to perform iterative reasoning processes that are\ndirectly inferred from the data in an end-to-end approach. We demonstrate the\nmodel's strength, robustness and interpretability on the challenging CLEVR\ndataset for visual reasoning, achieving a new state-of-the-art 98.9% accuracy,\nhalving the error rate of the previous best model. More importantly, we show\nthat the model is computationally-efficient and data-efficient, in particular\nrequiring 5x less data than existing models to achieve strong results.", "comment": "Published as a conference paper at ICLR 2018", "links": []}
{"entry_id": "1803.11189", "title": "Iterative Visual Reasoning Beyond Convolutions", "authors": ["Xinlei Chen", "Li-Jia Li", "Li Fei-Fei", "Abhinav Gupta"], "published": "2018-03-29 17:59:03", "updated": "2018-03-29 17:59:03", "summary": "We present a novel framework for iterative visual reasoning. Our framework\ngoes beyond current recognition systems that lack the capability to reason\nbeyond stack of convolutions. The framework consists of two core modules: a\nlocal module that uses spatial memory to store previous beliefs with parallel\nupdates; and a global graph-reasoning module. Our graph module has three\ncomponents: a) a knowledge graph where we represent classes as nodes and build\nedges to encode different types of semantic relationships between them; b) a\nregion graph of the current image where regions in the image are nodes and\nspatial relationships between these regions are edges; c) an assignment graph\nthat assigns regions to classes. Both the local module and the global module\nroll-out iteratively and cross-feed predictions to each other to refine\nestimates. The final predictions are made by combining the best of both modules\nwith an attention mechanism. We show strong performance over plain ConvNets,\n\\eg achieving an $8.4\\%$ absolute improvement on ADE measured by per-class\naverage precision. Analysis also shows that the framework is resilient to\nmissing regions for reasoning.", "comment": "CVPR 2018", "links": []}
{"entry_id": "1704.04882", "title": "Monoidal computer III: A coalgebraic view of computability and complexity", "authors": ["Dusko Pavlovic", "Muzamil Yahia"], "published": "2017-04-17 06:27:29", "updated": "2018-03-12 01:36:07", "summary": "Monoidal computer is a categorical model of intensional computation, where\nmany different programs correspond to the same input-output behavior. The\nupshot of yet another model of computation is that a categorical formalism\nshould provide a much needed high level language for theory of computation,\nflexible enough to allow abstracting away the low level implementation details\nwhen they are irrelevant, or taking them into account when they are genuinely\nneeded. A salient feature of the approach through monoidal categories is the\nformal graphical language of string diagrams, which supports visual reasoning\nabout programs and computations.\n  In the present paper, we provide a coalgebraic characterization of monoidal\ncomputer. It turns out that the availability of interpreters and specializers,\nthat make a monoidal category into a monoidal computer, is equivalent with the\nexistence of a *universal state space*, that carries a weakly final state\nmachine for any pair of input and output types. Being able to program state\nmachines in monoidal computers allows us to represent Turing machines, to\ncapture their execution, count their steps, as well as, e.g., the memory cells\nthat they use. The coalgebraic view of monoidal computer thus provides a\nconvenient diagrammatic language for studying computability and complexity.", "comment": "34 pages, 24 figures; in this version: added the Appendix", "links": []}
{"entry_id": "1710.07300", "title": "FigureQA: An Annotated Figure Dataset for Visual Reasoning", "authors": ["Samira Ebrahimi Kahou", "Vincent Michalski", "Adam Atkinson", "Akos Kadar", "Adam Trischler", "Yoshua Bengio"], "published": "2017-10-19 18:01:38", "updated": "2018-02-22 22:50:42", "summary": "We introduce FigureQA, a visual reasoning corpus of over one million\nquestion-answer pairs grounded in over 100,000 images. The images are\nsynthetic, scientific-style figures from five classes: line plots, dot-line\nplots, vertical and horizontal bar graphs, and pie charts. We formulate our\nreasoning task by generating questions from 15 templates; questions concern\nvarious relationships between plot elements and examine characteristics like\nthe maximum, the minimum, area-under-the-curve, smoothness, and intersection.\nTo resolve, such questions often require reference to multiple plot elements\nand synthesis of information distributed spatially throughout a figure. To\nfacilitate the training of machine learning systems, the corpus also includes\nside data that can be used to formulate auxiliary objectives. In particular, we\nprovide the numerical data used to generate each figure as well as bounding-box\nannotations for all plot elements. We study the proposed visual reasoning task\nby training several models, including the recently proposed Relation Network as\na strong baseline. Preliminary results indicate that the task poses a\nsignificant machine learning challenge. We envision FigureQA as a first step\ntowards developing models that can intuitively recognize patterns from visual\nrepresentations of data.", "comment": "workshop paper at ICLR 2018", "links": []}
{"entry_id": "1801.05302", "title": "Benchmark Visual Question Answer Models by using Focus Map", "authors": ["Wenda Qiu", "Yueyang Xianzang", "Zhekai Zhang"], "published": "2018-01-13 09:09:33", "updated": "2018-01-13 09:09:33", "summary": "Inferring and Executing Programs for Visual Reasoning proposes a model for\nvisual reasoning that consists of a program generator and an execution engine\nto avoid end-to-end models. To show that the model actually learns which\nobjects to focus on to answer the questions, the authors give a visualization\nof the norm of the gradient of the sum of the predicted answer scores with\nrespect to the final feature map. However, the authors do not evaluate the\nefficiency of focus map. This paper purposed a method for evaluating it. We\ngenerate several kinds of questions to test different keywords. We infer focus\nmaps from the model by asking these questions and evaluate them by comparing\nwith the segmentation graph. Furthermore, this method can be applied to any\nmodel if focus maps can be inferred from it. By evaluating focus map of\ndifferent models on the CLEVR dataset, we will show that CLEVR-iep model has\nlearned where to focus more than end-to-end models.", "comment": "A group project paper for course CS348. arXiv admin note: text\n  overlap with arXiv:1705.03633 by other authors", "links": []}
{"entry_id": "1707.03017", "title": "Learning Visual Reasoning Without Strong Priors", "authors": ["Ethan Perez", "Harm de Vries", "Florian Strub", "Vincent Dumoulin", "Aaron Courville"], "published": "2017-07-10 18:49:28", "updated": "2017-12-18 21:37:16", "summary": "Achieving artificial visual reasoning - the ability to answer image-related\nquestions which require a multi-step, high-level process - is an important step\ntowards artificial general intelligence. This multi-modal task requires\nlearning a question-dependent, structured reasoning process over images from\nlanguage. Standard deep learning approaches tend to exploit biases in the data\nrather than learn this underlying structure, while leading methods learn to\nvisually reason successfully but are hand-crafted for reasoning. We show that a\ngeneral-purpose, Conditional Batch Normalization approach achieves\nstate-of-the-art results on the CLEVR Visual Reasoning benchmark with a 2.4%\nerror rate. We outperform the next best end-to-end method (4.5%) and even\nmethods that use extra supervision (3.1%). We probe our model to shed light on\nhow it reasons, showing it has learned a question-dependent, multi-step\nprocess. Previous work has operated under the assumption that visual reasoning\ncalls for a specialized architecture, but we show that a general architecture\nwith proper conditioning can learn to visually reason effectively.", "comment": "Full AAAI 2018 paper is at arXiv:1709.07871. Presented at ICML 2017's\n  Machine Learning in Speech and Language Processing Workshop. Code is at\n  http://github.com/ethanjperez/film", "links": []}
{"entry_id": "1709.07871", "title": "FiLM: Visual Reasoning with a General Conditioning Layer", "authors": ["Ethan Perez", "Florian Strub", "Harm de Vries", "Vincent Dumoulin", "Aaron Courville"], "published": "2017-09-22 17:54:12", "updated": "2017-12-18 21:25:53", "summary": "We introduce a general-purpose conditioning method for neural networks called\nFiLM: Feature-wise Linear Modulation. FiLM layers influence neural network\ncomputation via a simple, feature-wise affine transformation based on\nconditioning information. We show that FiLM layers are highly effective for\nvisual reasoning - answering image-related questions which require a\nmulti-step, high-level process - a task which has proven difficult for standard\ndeep learning methods that do not explicitly model reasoning. Specifically, we\nshow on visual reasoning tasks that FiLM layers 1) halve state-of-the-art error\nfor the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are\nrobust to ablations and architectural modifications, and 4) generalize well to\nchallenging, new data from few examples or even zero-shot.", "comment": "AAAI 2018. Code available at http://github.com/ethanjperez/film .\n  Extends arXiv:1707.03017", "links": []}
{"entry_id": "1707.01932", "title": "End-to-End Learning of Semantic Grasping", "authors": ["Eric Jang", "Sudheendra Vijayanarasimhan", "Peter Pastor", "Julian Ibarz", "Sergey Levine"], "published": "2017-07-06 18:41:22", "updated": "2017-11-09 08:57:52", "summary": "We consider the task of semantic robotic grasping, in which a robot picks up\nan object of a user-specified class using only monocular images. Inspired by\nthe two-stream hypothesis of visual reasoning, we present a semantic grasping\nframework that learns object detection, classification, and grasp planning in\nan end-to-end fashion. A \"ventral stream\" recognizes object class while a\n\"dorsal stream\" simultaneously interprets the geometric relationships necessary\nto execute successful grasps. We leverage the autonomous data collection\ncapabilities of robots to obtain a large self-supervised dataset for training\nthe dorsal stream, and use semi-supervised label propagation to train the\nventral stream with only a modest amount of human supervision. We\nexperimentally show that our approach improves upon grasping systems whose\ncomponents are not learned end-to-end, including a baseline method that uses\nbounding box detection. Furthermore, we show that jointly training our model\nwith auxiliary data consisting of non-semantic grasping data, as well as\nsemantically labeled images without grasp actions, has the potential to\nsubstantially improve semantic grasping performance.", "comment": "14 pages", "links": []}
{"entry_id": "1710.00453", "title": "Visual Reasoning with Natural Language", "authors": ["Stephanie Zhou", "Alane Suhr", "Yoav Artzi"], "published": "2017-10-02 01:52:05", "updated": "2017-10-02 01:52:05", "summary": "Natural language provides a widely accessible and expressive interface for\nrobotic agents. To understand language in complex environments, agents must\nreason about the full range of language inputs and their correspondence to the\nworld. Such reasoning over language and vision is an open problem that is\nreceiving increasing attention. While existing data sets focus on visual\ndiversity, they do not display the full range of natural language expressions,\nsuch as counting, set reasoning, and comparisons.\n  We propose a simple task for natural language visual reasoning, where images\nare paired with descriptive statements. The task is to predict if a statement\nis true for the given scene. This abstract describes our existing synthetic\nimages corpus and our current work on collecting real vision data.", "comment": "AAAI NCHRC 2017", "links": []}
{"entry_id": "1504.02437", "title": "Predicting Complete 3D Models of Indoor Scenes", "authors": ["Ruiqi Guo", "Chuhang Zou", "Derek Hoiem"], "published": "2015-04-09 19:25:33", "updated": "2017-08-18 01:55:57", "summary": "One major goal of vision is to infer physical models of objects, surfaces,\nand their layout from sensors. In this paper, we aim to interpret indoor scenes\nfrom one RGBD image. Our representation encodes the layout of walls, which must\nconform to a Manhattan structure but is otherwise flexible, and the layout and\nextent of objects, modeled with CAD-like 3D shapes. We represent both the\nvisible and occluded portions of the scene, producing a complete 3D parse. Such\na scene interpretation is useful for robotics and visual reasoning, but\ndifficult to produce due to the well-known challenge of segmentation, the high\ndegree of occlusion, and the diversity of objects in indoor scene. We take a\ndata-driven approach, generating sets of potential object regions, matching to\nregions in training images, and transferring and aligning associated 3D models\nwhile encouraging fit to observations and overall consistency. We demonstrate\nencouraging results on the NYU v2 dataset and highlight a variety of\ninteresting directions for future work.", "comment": null, "links": []}
{"entry_id": "1705.08844", "title": "How a General-Purpose Commonsense Ontology can Improve Performance of Learning-Based Image Retrieval", "authors": ["Rodrigo Toro Icarte", "Jorge A. Baier", "Cristian Ruz", "Alvaro Soto"], "published": "2017-05-24 16:22:53", "updated": "2017-05-24 16:22:53", "summary": "The knowledge representation community has built general-purpose ontologies\nwhich contain large amounts of commonsense knowledge over relevant aspects of\nthe world, including useful visual information, e.g.: \"a ball is used by a\nfootball player\", \"a tennis player is located at a tennis court\". Current\nstate-of-the-art approaches for visual recognition do not exploit these\nrule-based knowledge sources. Instead, they learn recognition models directly\nfrom training examples. In this paper, we study how general-purpose\nontologies---specifically, MIT's ConceptNet ontology---can improve the\nperformance of state-of-the-art vision systems. As a testbed, we tackle the\nproblem of sentence-based image retrieval. Our retrieval approach incorporates\nknowledge from ConceptNet on top of a large pool of object detectors derived\nfrom a deep learning technique. In our experiments, we show that ConceptNet can\nimprove performance on a common benchmark dataset. Key to our performance is\nthe use of the ESPGAME dataset to select visually relevant relations from\nConceptNet. Consequently, a main conclusion of this work is that\ngeneral-purpose commonsense ontologies improve performance on visual reasoning\ntasks when properly filtered to select meaningful visual relations.", "comment": "Accepted in IJCAI-17", "links": []}
{"entry_id": "1705.03633", "title": "Inferring and Executing Programs for Visual Reasoning", "authors": ["Justin Johnson", "Bharath Hariharan", "Laurens van der Maaten", "Judy Hoffman", "Li Fei-Fei", "C. Lawrence Zitnick", "Ross Girshick"], "published": "2017-05-10 07:08:23", "updated": "2017-05-10 07:08:23", "summary": "Existing methods for visual reasoning attempt to directly map inputs to\noutputs using black-box architectures without explicitly modeling the\nunderlying reasoning processes. As a result, these black-box models often learn\nto exploit biases in the data rather than learning to perform visual reasoning.\nInspired by module networks, this paper proposes a model for visual reasoning\nthat consists of a program generator that constructs an explicit representation\nof the reasoning process to be performed, and an execution engine that executes\nthe resulting program to produce an answer. Both the program generator and the\nexecution engine are implemented by neural networks, and are trained using a\ncombination of backpropagation and REINFORCE. Using the CLEVR benchmark for\nvisual reasoning, we show that our model significantly outperforms strong\nbaselines and generalizes better in a variety of settings.", "comment": null, "links": []}
{"entry_id": "1612.08153", "title": "EgoReID: Cross-view Self-Identification and Human Re-identification in Egocentric and Surveillance Videos", "authors": ["Shervin Ardeshir", "Sandesh Sharma", "Ali Broji"], "published": "2016-12-24 09:00:37", "updated": "2016-12-24 09:00:37", "summary": "Human identification remains to be one of the challenging tasks in computer\nvision community due to drastic changes in visual features across different\nviewpoints, lighting conditions, occlusion, etc. Most of the literature has\nbeen focused on exploring human re-identification across viewpoints that are\nnot too drastically different in nature. Cameras usually capture oblique or\nside views of humans, leaving room for a lot of geometric and visual reasoning.\nGiven the recent popularity of egocentric and top-view vision,\nre-identification across these two drastically different views can now be\nexplored. Having an egocentric and a top view video, our goal is to identify\nthe cameraman in the content of the top-view video, and also re-identify the\npeople visible in the egocentric video, by matching them to the identities\npresent in the top-view video. We propose a CRF-based method to address the two\nproblems. Our experimental results demonstrates the efficiency of the proposed\napproach over a variety of video recorded from two views.", "comment": null, "links": []}
{"entry_id": "1612.06890", "title": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning", "authors": ["Justin Johnson", "Bharath Hariharan", "Laurens van der Maaten", "Li Fei-Fei", "C. Lawrence Zitnick", "Ross Girshick"], "published": "2016-12-20 21:40:40", "updated": "2016-12-20 21:40:40", "summary": "When building artificial intelligence systems that can reason and answer\nquestions about visual data, we need diagnostic tests to analyze our progress\nand discover shortcomings. Existing benchmarks for visual question answering\ncan help, but have strong biases that models can exploit to correctly answer\nquestions without reasoning. They also conflate multiple sources of error,\nmaking it hard to pinpoint model weaknesses. We present a diagnostic dataset\nthat tests a range of visual reasoning abilities. It contains minimal biases\nand has detailed annotations describing the kind of reasoning each question\nrequires. We use this dataset to analyze a variety of modern visual reasoning\nsystems, providing novel insights into their abilities and limitations.", "comment": null, "links": []}
{"entry_id": "1605.05462", "title": "Dual Local-Global Contextual Pathways for Recognition in Aerial Imagery", "authors": ["Alina Marcu", "Marius Leordeanu"], "published": "2016-05-18 07:37:22", "updated": "2016-05-18 07:37:22", "summary": "Visual context is important in object recognition and it is still an open\nproblem in computer vision. Along with the advent of deep convolutional neural\nnetworks (CNN), using contextual information with such systems starts to\nreceive attention in the literature. At the same time, aerial imagery is\ngaining momentum. While advances in deep learning make good progress in aerial\nimage analysis, this problem still poses many great challenges. Aerial images\nare often taken under poor lighting conditions and contain low resolution\nobjects, many times occluded by trees or taller buildings. In this domain, in\nparticular, visual context could be of great help, but there are still very few\npapers that consider context in aerial image understanding. Here we introduce\ncontext as a complementary way of recognizing objects. We propose a dual-stream\ndeep neural network model that processes information along two independent\npathways, one for local and another for global visual reasoning. The two are\nlater combined in the final layers of processing. Our model learns to combine\nlocal object appearance as well as information from the larger scene at the\nsame time and in a complementary way, such that together they form a powerful\nclassifier. We test our dual-stream network on the task of segmentation of\nbuildings and roads in aerial images and obtain state-of-the-art results on the\nMassachusetts Buildings Dataset. We also introduce two new datasets, for\nbuildings and road segmentation, respectively, and study the relative\nimportance of local appearance vs. the larger scene, as well as their\nperformance in combination. While our local-global model could also be useful\nin general recognition tasks, we clearly demonstrate the effectiveness of\nvisual context in conjunction with deep nets for aerial image understanding.", "comment": null, "links": []}
{"entry_id": "1604.04125", "title": "Filling in the details: Perceiving from low fidelity images", "authors": ["Farahnaz Ahmed Wick", "Michael L. Wick", "Marc Pomplun"], "published": "2016-04-14 12:10:23", "updated": "2016-04-14 12:10:23", "summary": "Humans perceive their surroundings in great detail even though most of our\nvisual field is reduced to low-fidelity color-deprived (e.g. dichromatic) input\nby the retina. In contrast, most deep learning architectures are\ncomputationally wasteful in that they consider every part of the input when\nperforming an image processing task. Yet, the human visual system is able to\nperform visual reasoning despite having only a small fovea of high visual\nacuity. With this in mind, we wish to understand the extent to which\nconnectionist architectures are able to learn from and reason with low acuity,\ndistorted inputs. Specifically, we train autoencoders to generate full-detail\nimages from low-detail \"foveations\" of those images and then measure their\nability to reconstruct the full-detail images from the foveated versions. By\nvarying the type of foveation, we can study how well the architectures can cope\nwith various types of distortion. We find that the autoencoder compensates for\nlower detail by learning increasingly global feature functions. In many cases,\nthe learnt features are suitable for reconstructing the original full-detail\nimage. For example, we find that the networks accurately perceive color in the\nperiphery, even when 75\\% of the input is achromatic.", "comment": null, "links": []}
{"entry_id": "1602.00753", "title": "Are Elephants Bigger than Butterflies? Reasoning about Sizes of Objects", "authors": ["Hessam Bagherinezhad", "Hannaneh Hajishirzi", "Yejin Choi", "Ali Farhadi"], "published": "2016-02-02 00:16:39", "updated": "2016-02-02 00:16:39", "summary": "Human vision greatly benefits from the information about sizes of objects.\nThe role of size in several visual reasoning tasks has been thoroughly explored\nin human perception and cognition. However, the impact of the information about\nsizes of objects is yet to be determined in AI. We postulate that this is\nmainly attributed to the lack of a comprehensive repository of size\ninformation. In this paper, we introduce a method to automatically infer object\nsizes, leveraging visual and textual information from web. By maximizing the\njoint likelihood of textual and visual observations, our method learns reliable\nrelative size estimates, with no explicit human supervision. We introduce the\nrelative size dataset and show that our method outperforms competitive textual\nand visual baselines in reasoning about size comparisons.", "comment": "To appear in AAAI 2016", "links": []}
{"entry_id": "1503.06813", "title": "Factorization of View-Object Manifolds for Joint Object Recognition and Pose Estimation", "authors": ["Haopeng Zhang", "Tarek El-Gaaly", "Ahmed Elgammal", "Zhiguo Jiang"], "published": "2015-03-23 20:05:36", "updated": "2015-04-13 02:59:41", "summary": "Due to large variations in shape, appearance, and viewing conditions, object\nrecognition is a key precursory challenge in the fields of object manipulation\nand robotic/AI visual reasoning in general. Recognizing object categories,\nparticular instances of objects and viewpoints/poses of objects are three\ncritical subproblems robots must solve in order to accurately grasp/manipulate\nobjects and reason about their environments. Multi-view images of the same\nobject lie on intrinsic low-dimensional manifolds in descriptor spaces (e.g.\nvisual/depth descriptor spaces). These object manifolds share the same topology\ndespite being geometrically different. Each object manifold can be represented\nas a deformed version of a unified manifold. The object manifolds can thus be\nparameterized by its homeomorphic mapping/reconstruction from the unified\nmanifold. In this work, we develop a novel framework to jointly solve the three\nchallenging recognition sub-problems, by explicitly modeling the deformations\nof object manifolds and factorizing it in a view-invariant space for\nrecognition. We perform extensive experiments on several challenging datasets\nand achieve state-of-the-art results.", "comment": null, "links": []}
